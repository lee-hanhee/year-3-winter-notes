\subsubsection{Random Process}
\begin{definition}
    Time-varying random variables $S_0, S_1, S_2, \ldots$.
\end{definition}

\subsubsection{Markov Process}
\begin{definition}
    Random process + depends on previous time step only (memoryless)
    \begin{itemize}
        \item w.l.o.g. states can contain history of previous states.
    \end{itemize}
\end{definition}

\subsection{Markov Chains (MCs)}
\begin{summary}
    In a \textbf{Markov Chain}, we assume that:
    \begin{itemize}
        \item there are no agents
        \item state transitions occur automatically
        \item $S_t$ is the state \textit{after} transition $t$
        \item the state transition process is stochastic and memoryless:
        \[
        S_t \perp S_0, \dots, S_{t-2} \mid S_{t-1}
        \]
        \begin{itemize}
            \item $S_t$ is independent of all previous states given $S_{t-1}$
        \end{itemize}
    \end{itemize}
    \vspace{1em}

    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Name} & \textbf{Function:} \\
            \midrule
            initial state distribution & $p_0(s) := \mathbb{P}[S_0 = s]$ \\
            \midrule
            transition distribution & $p(s'|s) := \mathbb{P}[S_{t+1} = s' | S_t = s]$ \\
            \midrule 
            Prob. that state of the env. after $T$ transitions is $s$ & $p_T(s) := \mathbb{P}[S_T = s]$ \\
            & $\quad \quad \; \; \; \;= \sum_{s'} p_{T-1}(s') p(s|s')$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item $p_{T-1}(s')$: Prob. $s'$ at $T$-$1$ (given) 
                \begin{itemize}
                    \item $p_0(s)$: Base case
                \end{itemize}
                \item $p(s|s')$: Prob. $s$ given $s'$ (from graph)
            \end{itemize}} \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{summary}

\subsubsection{Bayesian Network}
\begin{definition}
    $S_0,S_1,S_2,\ldots$ form a \textbf{Bayesian Network}:
    \customFigure[0.5]{../Images/L8_0.png}{}
\end{definition}
\newpage

\subsection{Markov Reward Processes (MRPs)}
\begin{summary}
    In a \textbf{Markov Reward Process}, we assume that:
    \begin{itemize}
        \item there is one agent
        \item state transitions occur automatically (i.e. agent has no control over actions)
        \item $S_t$ is the state \textit{after} transition $t$
        \item the state transition process is stochastic and memoryless:
        \[
        S_t \perp S_0, \dots, S_{t-2} \mid S_{t-1}
        \]
        \begin{itemize}
            \item $S_t$ is independent of all previous states given $S_{t-1}$
        \end{itemize}
        \item $R_t$ is the reward for transition $t$, i.e., $(S_{t-1}, \varnothing, S_t)$
    \end{itemize}
    \vspace{1em}

    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Name} & \textbf{Function:} \\
            \midrule
            Initial state distribution & $p_0(s) := \mathbb{P}[S_0 = s]$ \\
            \midrule
            Transition distribution & $p(s'|s) := \mathbb{P}[S_{t+1} = s' | S_t = s]$ \\
            \midrule
            Reward function & $r(s, s') := \text{reward for transition } (s, \varnothing, s')$ \\
            \midrule
            Discount factor & $\gamma \in [0,1]$ \\
            \midrule
            Return after $T$ transitions & $U_T = \sum_{t=1}^{T} \gamma^{t-1} R_t$ \\
            & $\quad \; \;= U_{T-1} + \gamma^{T-1} R_T$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item i.e. The (possibly discounted) sum of the rewards after $T$ transitions (sequence of rewards)
                \item \textbf{Why?}
                \begin{itemize}
                    \item Future rewards are less valuable than immediate rewards.
                    \item Won't converge if sum goes to $\infty$ if $\gamma = 1$.
                \end{itemize}
            \end{itemize}} \\
            \midrule
            Expected return after $T$ transitions & $\mathbb{E}[U_T] = \mathbb{E}[U_{T-1}] + \gamma^{T-1} \mathbb{E} [R_t]$ \\
            &  $\quad \quad \; \; \; = \mathbb{E}[U_{T-1}] + \gamma^{T-1} \sum_{s,s'} p_{T-1}(s) p(s'|s) r(s, s')$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item $p_{T-1}(s) p(s'|s)$: Prob. $s \to s'$
                \item $r(s, s')$: rwd $s \to s'$
                \item $\mathbb{E}[U_0] := 0$: Base case
            \end{itemize}} \\
            \midrule
            \bottomrule            
        \end{tabular}
    \end{center}
\end{summary}

\subsubsection{Bayesian Network}
\begin{definition}
    $S_0,R_1,S_1,R_2,S_2,\ldots$ form a \textbf{Bayesian Network}:
    \customFigure[0.5]{../Images/L8_1.png}{}
\end{definition}
\newpage

\subsection{Markov Decision Processes (MDPs)}
\subsubsection{Setup}
\begin{summary}
    In a \textbf{Markov Decision Process (MDP)}, we assume that:
    \begin{itemize}
        \item there is one agent
        \item state transitions occur manually (after each action)
        \item $S_t$ is the state \textit{after} transition $t$
        \item $A_t$ is the action inducing transition $t$
        \item the state transition process is stochastic and memoryless:
        \[
        S_t \perp S_0, A_1, \dots, S_{t-2}, A_{t-1} \mid S_{t-1}, A_t
        \]
        \begin{itemize}
            \item $S_t$ is independent of all previous states and actions given $S_{t-1}$ and $A_t$
        \end{itemize}
        \item $R_t$ is the reward for transition $t$, i.e., $(S_{t-1}, A_t, S_t)$
    \end{itemize}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Name} & \textbf{Function:} \\
            \midrule
            initial state distribution & $p_0(s) := \mathbb{P}[S_0 = s]$ \\
            \midrule
            transition distribution & $p(s'|s, a) := \mathbb{P}[S_t = s' | A_t = a, S_{t-1} = s]$ \\
            \midrule
            reward function & $r(s, a, s') := \text{reward for transition } (s, a, s')$ \\
            \midrule
            a time-invariant policy for choosing actions & $\pi(a|s) := \mathbb{P}[A_t = a | S_t = s]$ \\
            \midrule
            Maximum number of transitions & $T_{\max}$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item A Markov Decision Process can be either:
                \begin{itemize}
                    \item \textbf{Finite}: $T_{\max}$ is finite
                    \item \textbf{Infinite}: $T_{\max}$ is infinite
                    \begin{itemize}
                        \item For infinite MDPs, we must have $\gamma < 1$.
                    \end{itemize}
                \end{itemize}
            \end{itemize}} \\
            \midrule
            Prob. that state of the env. after $T$ transitions is $s$ & $p_T(s) = \sum_{a,s'} p_{T-1}(s) \pi(a|s') p(s|s',a)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item $p_{T-1}(s)$: Prob. $s'$ at $T$-$1$
                \item $\pi(a|s')$: Action $a$ from $s'$
                \item $p(s|s',a)$: Prob. $s$ given $s',a$
            \end{itemize}} \\
            \midrule
            Expected return after $T$ transitions & $\mathbb{E}_{\pi}[U_T] \text{=} \mathbb{E}_{\pi}[U_{T-1}] + \gamma^{T-1} \mathbb{E}_{\pi}[R_t]$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item $\mathbb{E}_\pi [R_t] = \sum_{s,a,s'} p_{T-1}(s) \pi(a \mid s) p(s' \mid s, a) r(s, a, s')$ 
                \item $\mathbb{E}_\pi [U_0] = 0$: Base case.
            \end{itemize}} \\
            \midrule
            Future return after $\tau$ transitions & $G_\tau = \sum_{t = \tau + 1}^T \gamma^{t - (\tau + 1)} R_t$ \\
            & $\quad \; \;= R_{\tau + 1} + \gamma G_{\tau + 1}$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Starting at $\tau + 1$ for the future return. 
            \end{itemize}} \\
            \midrule
            Expected future return after $\tau$ transitions given $S_\tau \text{=} s$ & $\mathbb{E}_{\pi}[G_{\tau} \mid S_{\tau} \text{=} s] \text{=} \mathbb{E}_{\pi}[R_{\tau+1} \mid S_{\tau} \text{=} s] + \gamma \mathbb{E}_{\pi}[G_{\tau+1} \mid S_{\tau} \text{=} s]$ \\
            & $\text{=} \sum_{a, s'} \pi(a \mid s) p(s' \mid s, a) \left( r(s, a, s') + \gamma \mathbb{E}_{\pi}[G_{\tau+1} \mid S_{\tau+1} \text{=} s'] \right)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item $\mathbb{E}_{\pi}[G_{T_{\max}} \mid S_{T_{\max}} = s] = 0$: Base case.
            \end{itemize}} \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Name} & \textbf{Function:} \\
            \midrule
            Value function & $v_{\pi}(s, T) := \mathbb{E}_{\pi}[G_{T_{\max} - T} \mid S_{T_{\max} - T} = s]$ \\
            & $\quad \quad \quad \; \; \; = \sum_{a, s'} \pi(a \mid s) p(s' \mid s, a) \left( r(s, a, s') + \gamma v_{\pi}(s', T-1) \right)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Value of state $s$ under the policy $\pi$ with $T$ transitions remaining.
                \begin{itemize}
                    \item i.e. How good the state is at time $T$ (e.g. If $v(s,T) = 5$, then the expected future return at $T$ is $5$).
                \end{itemize}
                \item $v(s, 0) = 0$ for all $s$: Base case
            \end{itemize}} \\
            \midrule
            Optimal action & $a^*(s,T) = \arg\max_{a \in \mathcal{A}(s)} \sum_{s'} p(s' \mid s, a) \left( r(s, a, s') + \gamma v_{\pi^*}(s', T-1) \right)$ \\
            & $\quad \quad \quad \quad = \arg \max_{a \in \mathcal{A}(s)} q^* (s,a,T)$ \\
            \midrule
            Optimal policy & $\pi^*(a \mid s,T) = \arg\max_{\pi(a \mid s,T)} \mathbb{E}_{\pi} [G_{\tau} \mid S_{\tau} = s] = \begin{cases}
                1 & \text{if } a = a^*(s,T) \\
                0 & \text{otherwise}
            \end{cases}$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Choose $\pi(\cdot \mid s)$ to maximize the expected future return after $T$ transitions given $S_{\tau} = s$.
                \item \textbf{Note:} Policy always depends on transitions remaining so may omit. 
            \end{itemize}} \\
            \midrule 
            Optimal value function & $v^*(s, T) = \max_a \sum_{s'} p(s' \mid a, s) \left( r(s, a, s') + \gamma v^*(s', \tau+1) \right)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Assume we use an optimal policy $\pi^*$.
                \item $v^*(s, 0) = 0$ for all $s$: Base case.
            \end{itemize}} \\
            \midrule 
            Q function (quality) & $q_{\pi}(s,a,T) := \mathbb{E}_{\pi}[G_{T_{\max} - T} \mid S_{T_{\max} - T} = s, A_{T_{\max} - (T-1)} = a]$ \\
            & $\quad \quad \quad \quad \quad = \sum_{s'} p(s' \mid s, a) \left( r(s, a, s') + \gamma \sum_{a'} \pi(a' \mid s') q_\pi (s',a',T-1) \right)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Quality of move $(s,a)$ under policy $\pi$ with $T$ transitions remaining.
                \item $q_\pi (s,a,0) = 0$ for all $s,a$: Base case.
            \end{itemize}} \\
            \midrule 
            Optimal Q function & $q^*(s,a,T) = \sum_{s'} p(s' \mid s, a) \left( r(s, a, s') + \gamma \max_{a'} q^*(s',a',T-1) \right)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item $q^*(s,a,0) = 0$ for all $s,a$: Base case.
            \end{itemize}} \\
            \midrule
            IDK Expected Return & $\mathbb{E}_\pi [U_{T_{\max}}] = \sum_s \mathbb{E}_\pi [G_0 \mid S_0 = s] p_0(s)$ \\
            & $\quad \quad \quad \quad \; \; = \sum_s v_\pi(s, 0) p_0(s)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item $G_0 = U_{T_{\max}}$
            \end{itemize}} \\
            \midrule 
            IDK Optimal Expected Return & $\max_\pi \mathbb{E}[U_{T_{\max}}] = \sum_s v^* (s,0)p_0(s)$ \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsubsection{Bayesian Network}
\begin{definition}
    $S_0,A_1,R_1,S_1,A_2,R_2,S_2,\ldots$ form a \textbf{Bayesian Network}:
    \customFigure[0.5]{../Images/L8_2.png}{}
\end{definition}

\subsubsection{Intuition on Formulae}
\begin{notes}
    \begin{equation*}
        \mathbb{E}_{\pi}[R_{\tau+1} \mid S_{\tau} = s] = \sum_{a, s'} \pi(a \mid s) p(s' \mid a, s) r(s, a, s')
    \end{equation*}
    \begin{itemize}
        \item $\pi(a \mid s) p(s' \mid a, s)$: Prob. of getting to $s'$ from $s$ w/ action $a$
        \item $r(s, a, s')$: Reward of getting to $s'$ from $s$ w/ action $a$
    \end{itemize}
    \vspace{1em}

    \begin{equation*}
        \mathbb{E}_{\pi}[G_{\tau+1} \mid S_{\tau} = s] = \sum_{a, s'} \pi(a \mid s) p(s' \mid a, s) \mathbb{E}_{\pi}[G_{\tau+1} \mid S_{\tau+1} = s']
    \end{equation*}
    \begin{itemize}
        \item $\pi(a \mid s) p(s' \mid a, s)$: Prob. of getting to $s'$ from $s$ w/ action $a$
        \item $\mathbb{E}_{\pi}[G_{\tau+1} \mid S_{\tau+1} = s']$: Expected future return at $\tau + 1$ from $s'$ at $\tau + 1$.
        \item $\sum_{a, s'}$: Sum over all possible future states and current actions to get expected future return at $\tau + 1$ from $s$ at $\tau$.
    \end{itemize}
\end{notes}
\newpage

\subsection{Canonical Examples}
\subsubsection{Markov Chains}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Caveman needs to predict the weather, $W$, which is either sunny or rainy. Suppose the weather tomorrow depends on the weather today:
        \customFigure[0.5]{../Images/L8_4.png}{}
        \item \textbf{Problem:} Caveman wants to predict the weather on a given day.
    \end{enumerate}
\end{example}

\subsubsection{Markov Reward Processes}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Caveman needs to predict the weather, $W$, which is either sunny or rainy. Suppose the weather tomorrow depends on the weather today:
        \customFigure[0.5]{../Images/L8_5.png}{}
        \begin{itemize}
            \item Depending on the transition, caveman may feel happier/sadder. This is quantified w/ the rewards.
        \end{itemize}
        \item \textbf{Problem:} Caveman wants to predict the weather on a given day that maximizes his happiness.
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Markov Decision Processes}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:}
        \customFigure[0.5]{../Images/L8_6.png}{}
        \begin{itemize}
            \item Solid straight line: Outcome of action $a$ from state $s$.
            \item Dotted straight line: Choice of action (policy) from state $s$.
            \begin{itemize}
                \item If policy known, then reduced to MRP.
            \end{itemize}
            \item Squiggly line: Reward for action $a$ from state $s$ to state $s'$.
            \item Assume uniform probability. 
            \begin{itemize}
                \item Since $\sum p= 1$, therefore count \# of arrows going out of $s$ and divide by $1$ to get $p$.
            \end{itemize}
            \item Same states have the same connections (i.e. all can use them just to hard to draw)
        \end{itemize}
        \item \textbf{Problem:} Find the optimal policy for $\gamma=1$ and $T_{\max} = 5$.
        \item \textbf{Soln:}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \begin{center}
        \begin{tabular}{cccc}
            \toprule
            $T$ & $s$ & $a$ & $q^* (s,a,T) = \sum_{s'} p(s' \mid s,a) \left(r(s,a,s') + \gamma \max_{a'} q^* (s',a',T-1)\right)$ \\
            \toprule
            $0$ & - & - & $0$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(s,0) = \text{NA}$ 
            \end{itemize}} \\
            \toprule
            $1$ & seed & wait & $q^*(\text{seed},\text{wait},1)= \underbrace{0.5(-2 + 0)}_{\text{$s'=$seed}} + \underbrace{0.5(0 + 0)}_{\text{$s'=$ga}} = -1$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{seed},1) = \text{wait}$
            \end{itemize}} \\
            \midrule
            $1$ & ga & wait & $q^*(\text{ga},\text{wait},1) = \underbrace{0.25(-2 + 0)}_{\text{$s'=$ga}} + \underbrace{0.5(0 + 0)}_{\text{$s'=$rea}} + \underbrace{0.25(0 + 0)}_{\text{$s'=$roa}} = -0.5$ \\
            $1$ & ga & eat & $q^*(\text{ga},\text{eat},1) = \underbrace{0.1(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.9(6 + 0)}_{\text{$s'=$seed}} = 5.4$ \\
            $1$ & ga & hunt & $q^*(\text{ga},\text{hunt},1) = \underbrace{0.5(24 + 0)}_{\text{$s'=$seed}} + \underbrace{0.5(0 + 0)}_{\text{$s'=$dead}} = 12$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{ga},1) = \text{hunt}$
            \end{itemize}} \\
            \midrule
            $1$ & rea & eat & $q^*(\text{rea},\text{eat},1) = \underbrace{1(12 + 0)}_{\text{$s'=$seed}} = 12$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{rea},1) = \text{eat}$
            \end{itemize}} \\
            \midrule 
            $1$ & roa & eat & $q^*(\text{roa},\text{eat},1) = \underbrace{0.25(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.75(2 + 0)}_{\text{$s'=$seed}} = 1.5$ \\
            $1$ & roa & hunt & $q^*(\text{roa},\text{hunt},1) = \underbrace{0.5(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.5(18 + 0)}_{\text{$s'=$seed}} = 9$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{roa},1) = \text{hunt}$
            \end{itemize}} \\
            \midrule 
            $1$ & dead & - & $q^*(\text{dead},-,1) = \underbrace{1(0 + 0)}_{\text{$s'=$end}} = 0$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(s,1) = -$ 
            \end{itemize}} \\
            \toprule
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Optimal Policy w/ 1 Transition Remaining: $\pi^*(a \mid s,1) = \begin{cases}
                    1 & \text{if } a = a^*(s,1) \\
                    0 & \text{otherwise}
                \end{cases}$
            \end{itemize}} \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{example}
\newpage

\begin{example}
    \begin{center}
        \begin{tabular}{cccc}
            \toprule
            $T$ & $s$ & $a$ & $q^* (s,a,T) = \sum_{s'} p(s' \mid s,a) \left(r(s,a,s') + \gamma \max_{a'} q^* (s',a',T-1)\right)$ \\
            \toprule
            $2$ & seed & wait & $q^*(\text{seed},\text{wait},2) = \underbrace{0.5(-2 - 1)}_{\text{$s'=$seed}} + \underbrace{0.5(0 + 12)}_{\text{$s'=$ga}} = 4.5$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{seed},2) = \text{wait}$
            \end{itemize}} \\
            \midrule
            $2$ & ga & wait & $q^*(\text{ga},\text{wait},2) = \underbrace{0.25(-2 + 12)}_{\text{$s'=$ga}} + \underbrace{0.5(0 + 12)}_{\text{$s'=$rea}} + \underbrace{0.25(0 + 9)}_{\text{$s'=$roa}} = 10.75$ \\
            $2$ & ga & eat & $q^*(\text{ga},\text{eat},2) = \underbrace{0.1(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.9(6 - 1)}_{\text{$s'=$seed}} = 4.5$ \\
            $2$ & ga & hunt & $q^*(\text{ga},\text{hunt},2) = \underbrace{0.5(24 - 1)}_{\text{$s'=$seed}} + \underbrace{0.5(0 + 0)}_{\text{$s'=$dead}} = 11.5$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{ga},2) = \text{hunt}$
            \end{itemize}} \\
            \midrule
            $2$ & rea & eat & $q^*(\text{rea},\text{eat},2) = \underbrace{1(12 - 1)}_{\text{$s'=$seed}} = 11$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{rea},2) = \text{eat}$
            \end{itemize}} \\
            \midrule
            $2$ & roa & eat & $q^*(\text{roa},\text{eat},2) = \underbrace{0.25(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.75(2 - 1)}_{\text{$s'=$seed}} = 0.5$ \\
            $2$ & roa & hunt & $q^*(\text{roa},\text{hunt},2) = \underbrace{0.5(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.5(18 - 1)}_{\text{$s'=$seed}} = 8.5$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{roa},2) = \text{hunt}$
            \end{itemize}} \\
            \midrule
            $2$ & dead & - & $q^*(\text{dead},-,2) = \underbrace{1(0 + 0)}_{\text{$s'=$end}} = 0$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(s,2) = -$
            \end{itemize}} \\
            \toprule
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Optimal Policy w/ 2 Transitions Remaining: $\pi^*(a \mid s,2) = \begin{cases}
                    1 & \text{if } a = a^*(s,2) \\
                    0 & \text{otherwise}
                \end{cases}$
            \end{itemize}} \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{example}
\newpage

\begin{example}
    \begin{center}
        \begin{tabular}{cccc}
            \toprule
            $T$ & $s$ & $a$ & $q^* (s,a,T) = \sum_{s'} p(s' \mid s,a) \left(r(s,a,s') + \gamma \max_{a'} q^* (s',a',T-1)\right)$ \\
            \toprule
            $3$ & seed & wait & $q^*(\text{seed},\text{wait},3) = \underbrace{0.5(-2 + 4.5)}_{\text{$s'=$seed}} + \underbrace{0.5(0 + 11.5)}_{\text{$s'=$ga}} = 7$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{seed},3) = \text{wait}$
            \end{itemize}} \\
            \midrule
            $3$ & ga & wait & $q^*(\text{ga},\text{wait},3) = \underbrace{0.25(-2 + 11.5)}_{\text{$s'=$ga}} + \underbrace{0.5(0 + 11)}_{\text{$s'=$rea}} + \underbrace{0.25(0 + 8.5)}_{\text{$s'=$roa}} = 10$ \\
            $3$ & ga & eat & $q^*(\text{ga},\text{eat},3) = \underbrace{0.1(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.9(6 + 4.5)}_{\text{$s'=$seed}} = 9.45$ \\
            $3$ & ga & hunt & $q^*(\text{ga},\text{hunt},3) = \underbrace{0.5(24 + 4.5)}_{\text{$s'=$seed}} + \underbrace{0.5(0 + 0)}_{\text{$s'=$dead}} = 14.25$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{ga},3) = \text{hunt}$
            \end{itemize}} \\
            \midrule
            $3$ & rea & eat & $q^*(\text{rea},\text{eat},3) = \underbrace{1(12 + 4.5)}_{\text{$s'=$seed}} = 16.5$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{rea},3) = \text{eat}$
            \end{itemize}} \\
            \midrule
            $3$ & roa & eat & $q^*(\text{roa},\text{eat},3) = \underbrace{0.25(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.75(2 + 4.5)}_{\text{$s'=$seed}} = 4.875$ \\
            $3$ & roa & hunt & $q^*(\text{roa},\text{hunt},3) = \underbrace{0.5(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.5(18 + 4.5)}_{\text{$s'=$seed}} = 11.25$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{roa},3) = \text{hunt}$
            \end{itemize}} \\
            \midrule
            $3$ & dead & - & $q^*(\text{dead},-,3) = \underbrace{1(0 + 0)}_{\text{$s'=$end}} = 0$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(s,3) = -$
            \end{itemize}} \\
            \toprule
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Optimal Policy w/ 3 Transitions Remaining: $\pi^*(a \mid s,3) = \begin{cases}
                    1 & \text{if } a = a^*(s,3) \\
                    0 & \text{otherwise}
                \end{cases}$
            \end{itemize}} \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{example}
\newpage

\begin{example}
    \begin{center}
        \begin{tabular}{cccc}
            \toprule
            $T$ & $s$ & $a$ & $q^* (s,a,T) = \sum_{s'} p(s' \mid s,a) \left(r(s,a,s') + \gamma \max_{a'} q^* (s',a',T-1)\right)$ \\
            \toprule
            $4$ & seed & wait & $q^*(\text{seed},\text{wait},4) = \underbrace{0.5(-2 + 7)}_{\text{$s'=$seed}} + \underbrace{0.5(0 + 14.25)}_{\text{$s'=$ga}} = 9.625$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{seed},4) = \text{wait}$
            \end{itemize}} \\
            \midrule
            $4$ & ga & wait & $q^*(\text{ga},\text{wait},4) = \underbrace{0.25(-2 + 14.25)}_{\text{$s'=$ga}} + \underbrace{0.5(0 + 16.5)}_{\text{$s'=$rea}} + \underbrace{0.25(0 + 11.25)}_{\text{$s'=$roa}} = 14.125$ \\
            $4$ & ga & eat & $q^*(\text{ga},\text{eat},4) = \underbrace{0.1(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.9(6 + 7)}_{\text{$s'=$seed}} = 11.7$ \\
            $4$ & ga & hunt & $q^*(\text{ga},\text{hunt},4) = \underbrace{0.5(24 + 7)}_{\text{$s'=$seed}} + \underbrace{0.5(0 + 0)}_{\text{$s'=$dead}} = 15.5$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{ga},4) = \text{hunt}$
            \end{itemize}} \\
            \midrule
            $4$ & rea & eat & $q^*(\text{rea},\text{eat},4) = \underbrace{1(12 + 7)}_{\text{$s'=$seed}} = 19$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{rea},4) = \text{eat}$
            \end{itemize}} \\
            \midrule
            $4$ & roa & eat & $q^*(\text{roa},\text{eat},4) = \underbrace{0.25(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.75(2 + 7)}_{\text{$s'=$seed}} = 6.75$ \\
            $4$ & roa & hunt & $q^*(\text{roa},\text{hunt},4) = \underbrace{0.5(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.5(18 + 7)}_{\text{$s'=$seed}} = 12.5$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{roa},4) = \text{hunt}$
            \end{itemize}} \\
            \midrule
            $4$ & dead & - & $q^*(\text{dead},-,4) = \underbrace{1(0 + 0)}_{\text{$s'=$end}} = 0$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(s,4) = -$
            \end{itemize}} \\
            \toprule
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Optimal Policy w/ 4 Transitions Remaining: $\pi^*(a \mid s,4) = \begin{cases}
                    1 & \text{if } a = a^*(s,4) \\
                    0 & \text{otherwise}
                \end{cases}$
            \end{itemize}} \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{example}
\newpage

\begin{example}
    \begin{center}
        \begin{tabular}{cccc}
            \toprule
            $T$ & $s$ & $a$ & $q^* (s,a,T) = \sum_{s'} p(s' \mid s,a) \left(r(s,a,s') + \gamma \max_{a'} q^* (s',a',T-1)\right)$ \\
            \toprule
            $5$ & seed & wait & $q^*(\text{seed},\text{wait},5) = \underbrace{0.5(-2 + 9.625)}_{\text{$s'=$seed}} + \underbrace{0.5(0 + 15.5)}_{\text{$s'=$ga}} = 11.5625$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{seed},5) = \text{wait}$
            \end{itemize}} \\
            \midrule
            $5$ & ga & wait & $q^*(\text{ga},\text{wait},5) = \underbrace{0.25(-2 + 15.5)}_{\text{$s'=$ga}} + \underbrace{0.5(0 + 19)}_{\text{$s'=$rea}} + \underbrace{0.25(0 + 12.5)}_{\text{$s'=$roa}} = 16$ \\
            $5$ & ga & eat & $q^*(\text{ga},\text{eat},5) = \underbrace{0.1(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.9(6 + 9.625)}_{\text{$s'=$seed}} = 14.0625$ \\
            $5$ & ga & hunt & $q^*(\text{ga},\text{hunt},5) = \underbrace{0.5(24 + 9.625)}_{\text{$s'=$seed}} + \underbrace{0.5(0 + 0)}_{\text{$s'=$dead}} = 16.8125$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{ga},5) = \text{hunt}$
            \end{itemize}} \\
            \midrule
            $5$ & rea & eat & $q^*(\text{rea},\text{eat},5) = \underbrace{1(12 + 9.625)}_{\text{$s'=$seed}} = 21.625$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{rea},5) = \text{eat}$
            \end{itemize}} \\
            \midrule
            $5$ & roa & eat & $q^*(\text{roa},\text{eat},5) = \underbrace{0.25(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.75(2 + 9.625)}_{\text{$s'=$seed}} = 8.71875$ \\
            $5$ & roa & hunt & $q^*(\text{roa},\text{hunt},5) = \underbrace{0.5(0 + 0)}_{\text{$s'=$dead}} + \underbrace{0.5(18 + 9.625)}_{\text{$s'=$seed}} = 13.8125$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(\text{roa},5) = \text{hunt}$
            \end{itemize}} \\
            \midrule
            $5$ & dead & - & $q^*(\text{dead},-,5) = \underbrace{1(0 + 0)}_{\text{$s'=$end}} = 0$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Best Action: $a^*(s,5) = -$
            \end{itemize}} \\
            \toprule
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Optimal Policy w/ 5 Transitions Remaining: $\pi^*(a \mid s,5) = \begin{cases}
                    1 & \text{if } a = a^*(s,5) \\
                    0 & \text{otherwise}
                \end{cases}$
            \end{itemize}} \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{example}

