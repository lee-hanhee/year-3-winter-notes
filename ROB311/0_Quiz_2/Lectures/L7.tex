\subsection{Problem Setup}
\begin{definition}
    Given a Bayesian network, $\mathcal{B} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V} = \{X_1, \dots, X_{|\mathcal{V}|}\}$, we want to find the value of:
    \[
    \operatorname{pr}(\mathbf{Q} \mid \mathbf{E}) := \operatorname{pr}(Q_1, \dots, Q_{|\mathbf{Q}|} \mid E_1, \dots, E_{|\mathbf{E}|}) = \frac{\sum_{\mathcal{V} \setminus (\mathbf{Q} \cup \mathbf{E})} p(X_1, \dots, X_{|\mathcal{V}|})}
    {\sum_{\mathcal{V} \setminus \mathbf{E}} p(X_1, \dots, X_{|\mathcal{V}|})}
    \]
    \[
    \operatorname{pr}(\mathbf{Q} \mid \mathbf{E}) \propto 
    \sum_{\mathcal{V} \setminus (\mathbf{Q} \cup \mathbf{E})} 
    \left( p(X_1) \prod_{i \neq 1} p(X_i \mid \operatorname{pts}(X_i)) \right)
    \]


    \begin{itemize}
        \item $\mathbf{Q} = \{Q_1, \dots, Q_{|\mathbf{Q}|}\}$: Query variables
        \item $\mathbf{E} = \{E_1, \dots, E_{|\mathbf{E}|}\} \subseteq \mathcal{V}$: Evidence variables
        \item $\mathbf{Q} \cap \mathbf{E} = \emptyset$.
    \end{itemize}
\end{definition}

\begin{warning}
    \begin{itemize}
        \item Denominator: Normalization constant (assuming $\mathbf{E}$ is fixed)
        \item Therefore, only need to compute numerator (w/o specifying $\mathbf{Q}$), which we can then normalize w.r.t. $\mathbf{Q}$
    \end{itemize}
\end{warning}

\subsubsection{Joint Distribution in a Bayesian Network}
\begin{derivation}
    For any joint distribution, the following factorization holds:
    \begin{equation*}
        p(X_1,\ldots,X_{|p|})  = p(X_1) \prod_{i \neq 1} p(X_i \mid X_1,\ldots, X_{i-1})
    \end{equation*}

    \textbf{Bayesian Network Conditions:} If 
    \begin{itemize}
        \item at least 1 variable will be an orphan (i.e. no parents)
        \item no variable is both ancestor and descendant of another. 
    \end{itemize}
    \vspace{1em}

    then this allows us to order $X_1,\ldots,X_{|\mathcal{V}|}$,  so that if $X_j$ is a descendent of $X_i$, then for any $j > i$, 
    \begin{equation*}
        \text{pts}(X_i) \subseteq \{X_1,\ldots,X_{i-1}\} \text{ and } X_1,\ldots,X_{i-1} \notin \text{des}(X_i)
    \end{equation*}

    Therefore, using the consequence of dependence separation, then 
    \begin{equation*}
        p(X_1,\ldots,X_{|\mathcal{V}|}) = p(X_1) \prod_{i \neq 1} p(X_i \mid \text{pts}(X_i))
    \end{equation*}
\end{derivation}

\subsection{Method 1: Bayesian Network Inference}

\subsubsection{Markov Blanket}
\begin{definition} 
    The \textbf{Markov blanket} of a variable $X$, denoted $\operatorname{mbk}(X)$, consists of the following variables:
    \begin{itemize}
        \item $X$'s children
        \item $X$'s parents
        \item The other parents of $X$'s children, excluding $X$ itself.
    \end{itemize}
    which is when a variable, $X$, is "eliminated", the resulting factor's scope is the Markov blanket of $X$.
\end{definition}

\subsubsection{Graphical Interpretation}
\begin{notes}
    Pictorially, eliminating $X$ is equivalent to replacing all hyper-edges that include $X$ with their union minus $X$, and then removing $X$.
\end{notes}

\subsubsection{Elimination Ordering}
\begin{definition}
    The order that the variables are eliminated.
    \begin{itemize}
        \item This creates a sequence of hyper-graphs that depend on the elimination ordering.
    \end{itemize}
\end{definition}

\subsubsection{Elimination Width}
\begin{definition}
    The \textbf{elimination width} of a sequence of hyper-graphs is the \# of variables in the hyper-edge within the sequence with the most variables.
\end{definition}

\subsubsection{Heuristics for Elimination Ordering}
\begin{definition}
    Choose the elimination ordering to minimize the elimination width using the following heuristics:
    \begin{enumerate}
        \item Eliminate variable with the fewest parents.
        \item Eliminate variable with the smallest domain for its parents, where
        \[
        |\operatorname{dom}(\operatorname{pts}(X))| = \prod_{Z \in \operatorname{pnt}(X)} |\operatorname{dom}(Z)|.
        \]
        \item Eliminate variable with the smallest Markov blanket.
        \item Eliminate variable with the smallest domain for its Markov blanket, where
        \[
        |\operatorname{dom}(\operatorname{mbk}(X))| = \prod_{Z \in \operatorname{embk}(X)} |\operatorname{dom}(Z)|.
        \]
    \end{enumerate}
\end{definition}

\begin{warning}
    Choosing the variable with the smallest domain for its Markov blanket is the most effective heuristic.
\end{warning}
\newpage

\subsection{Method 2: Inference via Sampling}
\begin{definition}
    Generate a large \# of samples and then approximate as:
    \[
    p(\mathbf{Q} \mid \mathbf{E}) \approx \frac{\# \text{ of samples w/ } \mathbf{Q} \text{ and } \mathbf{E}}{\# \text{ of samples w/ } \mathbf{E}}.
    \]
    \begin{itemize}
        \item As $\# \text{ of samples} \to \infty$, the approximation becomes exact.
    \end{itemize}
\end{definition}

\subsubsection{Inference via Sampling with Likelihood Weighting}
\begin{motivation}
    Most of the samples are wasted since they are not consistent with the evidence.
\end{motivation}

\begin{definition}
    Generate a large \# of samples and then approximate as:
    \[
    p(\mathbf{Q} \mid \mathbf{E}) \approx \frac{\text{weight of samples w/ } \mathbf{Q} \text{ and } \mathbf{E}}{\text{weight of samples w/ } \mathbf{E}}.
    \]
    \begin{itemize}
        \item Weight for each sample: Probability of forcing the evidence, i.e. probability of the evidence given the sample.
    \end{itemize}
\end{definition}
\newpage

\subsection{Canonical Problems:}
% \begin{example}
%     \begin{enumerate}
%         \item \textbf{Given:} Caveman is deciding whether to go hunt for meat. He must take into account several factors:
%         \begin{itemize}
%             \item Weather
%             \item Possibility of over-exertion
%             \item Possibility encountering lion
%         \end{itemize}

%         These factors can result in Cavemen's death. His decision will ultimately depend on the \textbf{chances} of his death.
%         \item \textbf{Binary Variables:}
%         \begin{itemize}
%             \item $W = \{\text{Sun}, \text{Rainy}\}$: Weather
%             \item $H$: Whether the Cavemen goes hunting or not.
%             \item $L$: Whether the Cavemen encounters a lion or not.
%             \item $T$: Whether the Cavement is tired or not.
%             \item $D$: Whether the Cavemen dies or not
%         \end{itemize}
%         \item \textbf{Problem:} Cavemen must decide whether to go hunting or not. 
%         \begin{itemize}
%             \item He must consider the conditional probabilities (i.e. dependence) of each event.
%         \end{itemize}
%     \end{enumerate}
% \end{example}

% \begin{warning}
%     Have to be discrete. 
% \end{warning}
% \newpage

\subsubsection{Bayesian Inference via Variable Elimination}
\begin{process}
    \begin{enumerate}
        \item Given Bayesian network w/ variables and their conditional probabilities.
        \item Find the probability of the query variable given the evidence variable, $p(\mathbf{Q} \mid \mathbf{E})$.
        \item Use $p(\mathbf{Q} \mid \mathbf{E}) = \frac{\sum_{\mathcal{V} \setminus (\mathbf{Q} \cup \mathbf{E})} p(X_1, \dots, X_{|\mathcal{V}|})}{\sum_{\mathcal{V} \setminus \mathbf{E}} p(X_1, \dots, X_{|\mathcal{V}|})}$.
        \item Determine $p(X_1) \prod_{i \neq 1} p(X_i \mid \operatorname{pts}(X_i))$ using the Bayesian network. 
        \item Write out the summation of the numerator in an order using heuristics to determine elimination ordering. 
        \item Start with inner summation and work outwards.
        \item Calculate the probability of the query variable(s) given the evidence variable(s).
    \end{enumerate}
\end{process}

\begin{warning}
    \begin{itemize}
        \item Write the complement probability to make life easier. (HIGHLY RECOMMENDED)
        \item To determine the conditional probability summation of a variable, look at its parents (inward arrows)
        \item Inner sum must have all probabilities with that variable in it that you are summing over. 
        \item TO determine $g_?$, look at which variable you aren't summing over and also aren't query/evidence variables, then it will be a fn of the remaining variables.
    \end{itemize}
\end{warning}
\newpage

\begin{example} 
    \begin{enumerate}
        \item \textbf{Given:}
        \customFigure[0.5]{../Images/L6_9.png}{}
        \begin{center}
            \begin{tabular}{ll}
                \toprule
                \textbf{Variables} & \textbf{Values} \\
                \midrule
                $W$ & $P(\text{Sunny}) = 0.5 \mid P(\text{Rainy}) = 0.5$ \\
                \midrule
                $H$ & $P(h) = 0.5 \mid P(\lnot h) = 0.5$ \\
                \midrule
                $T$ & $P(t \mid \text{Sunny}, h) = 1 \mid P(t \mid \text{Sunny}, \lnot h) = 0.5 \mid P(t \mid \text{Rainy}, h) = 0.25 \mid P(t \mid \text{Rainy}, \lnot h) = 0$ \\
                & $P(\lnot t \mid \text{Sunny}, h) = 0 \mid P(\lnot t \mid \text{Sunny}, \lnot h) = 0.5 \mid P(\lnot t \mid \text{Rainy}, h) = 0.75 \mid P(\lnot t \mid \text{Rainy}, \lnot h) = 1$ \\
                \midrule
                $L$ & $P(l \mid h) = 1 \mid P(l \mid \lnot h) = 0.75$ \\
                & $P(\lnot l \mid h) = 0 \mid P(\lnot l \mid \lnot h) = 0.25$ \\
                \midrule
                $D$ & $P(d \mid t,l) = 0.75 \mid P(d \mid t,\lnot l) = 1 \mid P(d \mid \lnot t,l) = 0 \mid P(d \mid \lnot t,\lnot l) = 0$ \\
                & $P(\lnot d \mid t,l) = 0.25 \mid P(\lnot d \mid t,\lnot l) = 0 \mid P(\lnot d \mid \lnot t,l) = 1 \mid P(\lnot d \mid \lnot t,\lnot l) = 1$ \\
                \bottomrule
            \end{tabular}
        \end{center}
        \item \textbf{Problem:} $p(d \mid h)$? 
        \item \textbf{Soln:}
        \begin{enumerate}
            \item $p(d \mid h) = \frac{p(d,h)}{p(h)} = \frac{\sum_{W,T,L} p(W,h,T,L,d)}{\sum_{W,T,L,D} p(W,h,T,L,d)}$ by definition of query and evidence equations.
            \item $p(W,h,T,L,D) = p(h) p(W) p(L \mid h) p(t \mid W,h) p(D \mid T,L)$ by Bayesian network and $p(X_1, \dots, X_{|\mathcal{V}|}) = p(X_1) \prod_{i \neq 1} p(X_i \mid \operatorname{pts}(X_i))$.
        \end{enumerate}
        \begin{center}
            \begin{tabular}{l}
                \toprule
                \textbf{Summation} \\
                \toprule
                \multicolumn{1}{p{\linewidth}}{
                \begin{center}
                    $\text{Numerator}: \underbrace{p(h) \sum_L p(L \mid h) \underbrace{\sum_T p(D \mid T,L) \underbrace{\sum_W p(W) p(T \mid W,h)}_{g_1(T)}}_{g_2(L,D)}}_{g_3(D)}$
                \end{center}} \\
                \toprule
                \multicolumn{1}{p{\linewidth}}{
                \begin{center}
                    $g_1(T) = p(\text{Sunny}) p(T \mid \text{Sunny},h) + p(\text{Rainy}) p(T \mid \text{Rainy},h)$
                \end{center}} \\
                $g_1(t) = p(\text{Sunny}) p(t \mid \text{Sunny},h) + p(\text{Rainy}) p(t \mid \text{Rainy},h) = 0.5 \cdot 1 + 0.5 \cdot 0.25 = 0.625$ \\
                $g_1(\lnot t) =  p(\text{Sunny}) p(\lnot t \mid \text{Sunny},h) + p(\text{Rainy}) p(\not t \mid \text{Rainy},h) = 0.5 \cdot 0 + 0.5 \cdot 0.75 = 0.375$ \\
                \midrule
                \multicolumn{1}{p{\linewidth}}{
                \begin{center}
                    $g_2(L,D) = p(D \mid t,L) g_1(t) + p(D \mid \lnot t, L) g_1(\lnot t)$ 
                \end{center}} \\
                $g_2(l,d) = p(d \mid t,l) g_1(t) + p(d \mid \lnot t, l) g_1(\lnot t) = 0.75 \cdot 0.625 + 0 \cdot 0.375 = 0.46875$ \\
                $g_2(l,\lnot d) = p(\lnot d \mid t,l) g_1(t) + p(\lnot d \mid \lnot t, l) g_1(\lnot t) = 0.25 \cdot 0.625 + 1 \cdot 0.375 = 0.53125$ \\
                $g_2(\lnot l,d) = p(d \mid t,\lnot l) g_1(t) + p(d \mid \lnot t, \lnot l) g_1(\lnot t) = 1 \cdot 0.625 + 0 \cdot 0.375 = 0.625$ \\
                $g_2(\lnot l,\lnot d) = p(\lnot d \mid t,\lnot l) g_1(t) + p(\lnot d \mid \lnot t, \lnot l) g_1(\lnot t) = 0 \cdot 0.625 + 1 \cdot 0.375 = 0.375$ \\
                \midrule
                \multicolumn{1}{p{\linewidth}}{
                \begin{center}
                    $g_3(D) = p(h) p(l \mid h) g_2(l,D) + p(h) p(\lnot l \mid h) g_2(\lnot l, D)$ 
                \end{center}} \\
                $g_3(d) = p(h) p(l \mid h) g_2(l,d) + p(h) p(\lnot l \mid h) g_2(\lnot l,d) = (0.5)(1)(0.46875) + (0.5)(0)(0.625) = 0.234375$ \\
                $g_3(\lnot d) = p(h) p(l \mid h) g_2(l,\lnot d) + p(h) p(\lnot l \mid h) g_2(\lnot l,\lnot d) = (0.5)(1)(0.53125) + (0.5)(0)(0.375) = 0.265625$ \\
                \bottomrule
            \end{tabular}
        \end{center}
        \vspace{1em}

        \begin{equation*}
            p(d \mid h) = \frac{g_3(d)}{g_3(d) + g_3(\lnot d)} = \frac{0.234375}{0.234375 + 0.265625} = \frac{0.234375}{0.5} = 0.46875
        \end{equation*}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \begin{center}
        \begin{tabular}{l}
            \toprule
            \textbf{Summation} \\
            \toprule
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $\text{Numerator}: \underbrace{p(h) \sum_L p(L \mid h) \underbrace{\sum_W p(W) \underbrace{\sum_T p(T \mid W,h) p(D \mid T,L)}_{g_1(W,D,L)}}_{g_2(D,L)}}_{g_3(D)}$
            \end{center}} \\
            \toprule 
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $\text{Numerator}: \underbrace{p(h) \sum_W p(W) \underbrace{\sum_T p(T \mid W,h) \underbrace{\sum_L p(L \mid h) p(D \mid T,L)}_{g_1(D,T)}}_{g_2(D,W)}}_{g_3(D)}$
            \end{center}} \\
            \toprule
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $\text{Numerator}: \underbrace{p(h) \sum_W p(W) \underbrace{\sum_L p(L \mid h) \underbrace{\sum_T p(T \mid W,h) p(D \mid T,L)}_{g_1(W,D,L)}}_{g_2(W,D)}}_{g_3(D)}$
            \end{center}} \\
            \toprule
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $\text{Numerator}: \underbrace{p(h) \sum_T p(T \mid W,h) \underbrace{\sum_W p(W) \underbrace{\sum_L p(L \mid h) p(D \mid T,L)}_{g_1(D,T)}}_{g_2(D,T)}}_{g_3(D)}$
            \end{center}} \\
            \toprule
            \multicolumn{1}{p{\linewidth}}{
            \begin{center}
                $\ldots$
            \end{center}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{example}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:}
        \customFigure[0.5]{../Images/L7_1.png}{}
        \item \textbf{Problem:} Compute $\Pr(s = 1 \mid t = 1)$ if $\Pr(G = 1) = 0.3$, $\Pr(E = 1) = 0.4$, and the conditional probability tables for $S, Y,$ and $T$ are given below.
        \item \textbf{Solution:}
        \begin{enumerate}
            \item Derivation of $p(s=1 \mid t=1)$: 
            \begin{align*}
                p(s=1 \mid t=1) &= \frac{p(s=1,t=1)}{p(t=1)} \\
                &= \frac{P(s=1,t=1)}{\sum_S P(S,t=1)} \\
                &= \frac{\sum_{E,G,Y} P(E,G,Y,s=1,t=1)}{\sum_S \sum_{E,G,Y} P(E,G,Y,S,t=1)}
            \end{align*}
            \item Summation Term: 
            \begin{align*}
                & \sum_{E,G,Y} p(E) p(G) p(S \mid G,E) p(Y \mid S) p(t=1 \mid S) \\
                & \underbrace{p(t=1 \mid S) \sum_E p(E) \underbrace{\sum_G p(G) p(S \mid G,E) \underbrace{\sum_Y p(Y \mid S)}_{g_1(S)}}_{g_2(E,S)}}_{g_3(S)} \quad \text{one possible ordering}
            \end{align*}
            \begin{itemize}
                \item Conditional probability and individual probabilities come from Bayesian network, and set $t,s=1$ due to the query and evidence variables.
            \end{itemize}
            \item Choose: 
            \[
            \underbrace{p(t=1 \mid S) \sum_E p(E) \underbrace{\sum_G p(G) p(S \mid G,E) \underbrace{\sum_Y p(Y \mid S)}_{g_1(S)}}_{g_2(E,S)}}_{g_3(S)}
            \]
            \item $g_1(S)$: 
            \begin{align*}
                g_1(S) &= p(Y=1 \mid S) + p(Y=0 \mid S) \\
                &= \begin{cases}
                    0.1 + 0.9 = 1 \quad \text{if } S=0 \\
                    0.8 + 0.2 = 1 \quad \text{if } S=1 \\
                \end{cases} 
            \end{align*}
            \item $g_2(E,S)$: 
            \begin{align*}
                g_2(E,S) &= p(G=1) p(S \mid G=1,E) g_1(S) + p(G=0) p(S \mid G=0,E) g_1(S) \\
                g_2(E,S) &= p(G=1) p(S \mid G=1,E) + p(G=0) p(S \mid G=0,E) \\
                &= \begin{cases}
                    0.3(0.5) + 0.7(0.9) \quad \text{if } E=0, S=0 \\
                    0.3(0.5) + 0.7(0.1) \quad \text{if } E=0, S=1 \\
                    0.3(0.3) + 0.7(0.6) \quad \text{if } E=1, S=0 \\
                    0.3(0.7) + 0.7(0.4) \quad \text{if } E=1, S=1 \\
                \end{cases} \\
                &= \begin{cases}
                    0.78 \quad \text{if } E=0, S=0 \\
                    0.22 \quad \text{if } E=0, S=1 \\
                    0.51 \quad \text{if } E=1, S=0 \\
                    0.49 \quad \text{if } E=1, S=1 \\
                \end{cases}
            \end{align*}
            \item $g_3(S)$: 
            \begin{align*}
                g_3(S) &= p(t=1 \mid S) p(E=1) g_2(E=1,S) + p(t=1 \mid S) p(E=0) g_2(E=0,S) \\
                &= \begin{cases}
                    0.2(0.4)(0.51) + 0.2(0.6)(0.78) \quad \text{if } S=0 \\
                    0.2(0.4)(0.49) + 0.2(0.6)(0.22) \quad \text{if } S=1 \\
                \end{cases} \\
                &= \begin{cases}
                    0.1344 \quad \text{if } S=0 \\
                    0.2952 \quad \text{if } S=1 \\
                \end{cases} 
            \end{align*}
            \item $p(s=1 \mid t=1)$: 
            \begin{align*}
                p(s=1 \mid t=1) &= \frac{g_3(1)}{g_3(0) + g_3(1)} \\
                &= \frac{0.2952}{0.2952 + 0.1344} \\
                &= 0.6875
            \end{align*}
        \end{enumerate}
    \end{enumerate}

    % \begin{center}
    %     \begin{tabular}{l}
    %         \toprule
    %         \textbf{Summation} \\
    %         \toprule
    %         \multicolumn{1}{p{\linewidth}}{
    %         \begin{center}
    %             $\text{Numerator}: \underbrace{p(t=1 \mid s=1) \sum_E p(E) \underbrace{\sum_G p(G) p(s=1 \mid G,E) \underbrace{\sum_Y p(Y \mid s=1)}_{g_1}}_{g_2}}_{g_3}$
    %         \end{center}} \\
    %         \toprule
    %         \multicolumn{1}{p{\linewidth}}{
    %         \begin{center}
    %             $g_1 = p(Y=1 \mid S=1) + p(Y=0 \mid S=1) = 0.9 + 0.1 = 1$
    %         \end{center}} \\
    %         \midrule
    %         \multicolumn{1}{p{\linewidth}}{
    %         \begin{center}
    %             $g_2(E) = (p(g=1) p(s=1 \mid g=1, E) + p(g=0) p(s=1 \mid g=0, E)) g_1$
    %         \end{center}} \\
    %         $g_2(e=1,s=1) = 0.3(0.7) + 0.7(0.4) = 0.49$ \\
    %         $g_2(e=0,s=1) = 0.3(0.5) + 0.7(0.1) = 0.22$ \\
    %         $g_2(e=1,s=0) = 0.3(0.3) + 0.7(0.6) = 0.51$ \\
    %         $g_2(e=0,s=0) = 0.3(0.5) + 0.7(0.9) = 0.78$ \\
    %         \midrule
    %         \multicolumn{1}{p{\linewidth}}{
    %         \begin{itemize}
    %             \item $g_3(t=1 \mid s=1) = 0.9 p(e=1) g_2(e=1) + 0.9 p(e=0) g_2(e=0) = 0.9(0.4)(0.49) + 0.9(0.6)(0.22) = 0.2952$ 
    %             \item $g_3(t=1 \mid s=0) = 0.2 p(e=1) g_2(e=1) + 0.2 p(e=0) g_2(e=0) = 0.2(0.4)(0.51) + 0.2(0.6)(0.78) = 0.1344$
    %         \end{itemize}} \\
    %         \bottomrule
    %     \end{tabular}
    % \end{center}

    % \begin{equation*}
    %     p(s=1 \mid t=1) = \frac{g_3}{\sum_S p(t=1,S)} = \frac{0.2952}{0.2952 + 0.1344} = \frac{0.2952}{0.4296} = 0.6875
    % \end{equation*}
\end{example}
\newpage

\begin{example} 
    \begin{enumerate}
        \item \textbf{Given:} Consider the following Bayesian network, where $A,B,C,D$ are binary R.V. over $\{0,1\}$
        \customFigure[0.5]{../Images/FE23_3.png}{}
        \customFigure[0.5]{../Images/FE23_2.png}{}
        \item \textbf{Problem:} Find $P(A=0 \mid C=0)$ and $P(D=1 \mid C=0)$.
        \item \textbf{Solution:}
        \begin{enumerate}
            \item Derivation of \( P(D=1 \mid C=0) \): 
            \begin{align*}
                P(D=1 \mid C=0) &= \frac{P(D=1,C=0)}{P(C=0)} \quad \text{by definition}\\
                &= \frac{P(D=1,C=0)}{\sum_d P(D=d,C=0)} \quad \text{marginalize over $D$}\\
                &= \frac{\sum_{A,B} P(A,B,C=0,D=1)}{\sum_d \sum_{A,B} P(A,B,C=0,D=d)} \quad \text{equation in problem setup} 
            \end{align*}
            \begin{itemize}
                \item Summing over the variables that are not in the query and evidence variables.
            \end{itemize}
            
            \item Summation Term: 
            \begin{align*}
                \sum_{A,B} & P(A) P(B \mid A) P(C=0 \mid A) P(D=d \mid B,C=0) \quad \text{Bayesian network} \\
                \sum_A & P(A) P(C=0 \mid A) \sum_B P(B \mid A) P(D=d \mid B,C=0) \quad \text{(1st ordering)} \\
                \sum_B & P(D=d \mid B,C=0) \sum_A P(A) P(B \mid A) P(C=0 \mid A) \quad \text{(2nd ordering)}
            \end{align*}
            
            \item Choose: 
            \[
            \underbrace{\sum_B P(D=d \mid B,C=0) \underbrace{\sum_A P(A) P(B \mid A) P(C=0 \mid A)}_{g_1(B)}}_{g_2(d)}
            \]
            
            \item \( g_1(B) \):
            \begin{align*}
                g_1(B) &= P(A=0)P(B \mid A=0) P(C=0 \mid A=0) + P(A=1)P(B \mid A=1) P(C=0 \mid A=1) \\
                &= \begin{cases}
                    0.9(0.6)(0.8) + 0.1(0.5)(0.6) \quad \text{if } B=0 \\
                    0.9(0.4)(0.8) + 0.1(0.5)(0.6) \quad \text{if } B=1
                \end{cases} \\
                &= \begin{cases}
                    0.462 \quad \text{if } B=0 \\
                    0.318 \quad \text{if } B=1
                \end{cases} 
            \end{align*}
            
            \item \( g_2(d) \): 
            \begin{align*}
                g_2(d) &= P(D=d \mid B=0, C=0) g_1(B=0) + P(D=d \mid B=1, C=0)g_1(B=1) \\
                &= \begin{cases}
                    0.5(0.462) + 0.5(0.318) \quad \text{if } d=0 \\
                    0.5(0.462) + 0.5(0.318) \quad \text{if } d=1
                \end{cases} \\
                &= \begin{cases}
                    0.39 \quad \text{if } d=0 \\
                    0.39 \quad \text{if } d=1
                \end{cases} 
            \end{align*}
            
            \item \( P(D=1 \mid C=0) = \frac{g_2(1)}{g_2(0) + g_2(1)} = \frac{0.39}{0.39 + 0.39} = 0.5 \)
        \end{enumerate}   
        \item \textbf{Solution 2:}
        \begin{enumerate}
            \item Derivation of \( P(A=0 \mid C=0) \):
            \begin{align*}
                P(A=0 \mid C=0) &= \frac{P(A=0,C=0)}{P(C=0)} \\
                &= \frac{P(A=0,C=0)}{\sum_a P(A=a,C=0)} \\
                &= \frac{\sum_{B,D} P(A=0,B,C=0,D)}{\sum_a \sum_{B,D} P(A=a,B,C=0,D)}
            \end{align*}
            \item Summation Term:
            \begin{align*}
                \sum_{B,D} & P(A=a) P(B \mid A=a) P(C=0 \mid A=a) P(D \mid B,C=0) \quad \text{Bayesian network} \\
                P(C=0 \mid A=a) \sum_B & P(B \mid A=a) P(A=a \mid B,C=0) \sum_D P(D \mid B,C=0) \quad \text{(1st ordering)} \\
                P(C=0 \mid A=a) \sum_D & P(D \mid B,C=0) \sum_B P(B \mid A=a) P(A=a \mid B,C=0) \quad \text{(2nd ordering)}
            \end{align*}
            \item Choose:
            \[
            P(C=0 \mid A=a) \underbrace{\sum_B P(B \mid A=a) P(A=a \mid B,C=0) \underbrace{\sum_D P(D \mid B,C=0)}_{g_1(B)}}_{g_2(A)}
            \]
            \item Same as before.
        \end{enumerate}     
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Hypergraph}
\begin{process} Process of eliminating a variable. 
    \begin{enumerate}
        \item Create a Hyper-graph by creating a node for each variable. 
        \item Create hyper-edges (factors) by circling the nodes based on of its parents (i.e. arrows pointing into a variable). If no parents, circle itself.
        \item Select a variable $v$ that we are summing over. 
        \begin{enumerate}
            \item Circle all the variables that have $v$ in their hyperedge into one big hyperedge (i.e. union of hyper-edges).
            \item Eliminate $v$ by removing the node. 
            \item Calculate the factor by multiplying the support of the variables in the union of hyperedges.
        \end{enumerate}
        \item Repeat the process for all other $v$. 
        \item Select the smallest factor to eliminate first.
        \item Repeat until all variables are eliminated to determine the best ordering of elimination. 
        \begin{itemize}
            \item The first eliminated variable will be the inner sum. 
        \end{itemize}
    \end{enumerate}
\end{process}
\newpage

\begin{example}
    \customFigure[0.5]{../Images/L6_11.png}{}
    \begin{itemize}
        \item Since these are all binary variables, we are selecting the factor with the least number of variables to eliminate first.
    \end{itemize}
    \customFigure[0.75]{../Images/L7_0.png}{}
\end{example}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Bayesian network 
        \customFigure[0.5]{../Images/FE23_4.png}{}
        with cardinality of the support of each variable (i.e. number of values each variable can take on) as follows:
        \begin{itemize}
            \item $A$: $2^4$
            \item $B$: $2^2$
            \item $C$: $2^{12}$
            \item $D$: $2^2$
            \item $E$: $2^3$ 
            \item $F$: $2^6$
        \end{itemize}
        Suppose elimination ordering is chosen so that the next variable eliminated is the one that results in the smallest factor (breaking ties alphabetically).
        \item \textbf{Problem 1:} How many variables must be eliminated to compute $P(A,F \mid C)$?
        \item \textbf{Solution 1:}
        \begin{enumerate}
            \item Since $A$, $F$ are query, and $C$ is evidence, we must eliminate $B$, $D$, and $E$, so 3 variables must be eliminated.
        \end{enumerate}
        \item \textbf{Problem 2:} What is the first variable to be eliminated to compute $P(F \mid A)$? 
        \item \textbf{Solution 2:}
        \begin{enumerate}
            \item Try eliminating all variables that aren't query or evidence and count \# of variables in union of hyperedges.
            \customFigure[0.5]{../Images/FE23_5.png}{}
            \begin{enumerate}
                \item Eliminate $B$: Hyperunion is ACDF $\rightarrow$ $2^4 \cdot 2^{12} \cdot 2^2 \cdot 2^6 = 2^{24}$
                \item Eliminate $C$: Hyperunion is ABDEF $\rightarrow$ $2^4 \cdot 2^2 \cdot 2^2 \cdot 2^3 \cdot 2^6 = 2^{17}$
                \item Eliminate $D$: Hyperunion is BCF $\rightarrow$ $2^2 \cdot 2^{12} \cdot 2^6 = 2^{20}$
                \item Eliminate $E$: Hyperunion is AC $\rightarrow$ $\boxed{2^4 \cdot 2^{12} = 2^{16}}$
            \end{enumerate}
            \item Choose $E$ as the first variable to be eliminated because it has the lowest support in its hyperunion.
        \end{enumerate}
        \item \textbf{Problem 3:} What is the second variable to be eliminated to compute $P(F \mid A)$?
        \item \textbf{Solution 3:}
        \begin{enumerate}
            \item Try eliminating all variable except $F,A,E$.
            \customFigure[0.5]{../Images/FE23_6.png}{}
            \begin{enumerate}
                \item Eliminate $B$: Hyperunion is ACDF $\rightarrow$ $2^4 \cdot 2^{12} \cdot 2^2 \cdot 2^6 = 2^{24}$
                \item Eliminate $C$: Hyperunion is ABDF $\rightarrow$ $\boxed{2^4 \cdot 2^2 \cdot 2^2 \cdot 2^6 = 2^{14}}$
                \item Eliminate $D$: Hyperunion is BCF $\rightarrow$ $2^2 \cdot 2^{12} \cdot 2^6 = 2^{20}$
            \end{enumerate}
            \item Choose $C$ as the second variable to be eliminated because it has the lowest support in its hyperunion.
        \end{enumerate}
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Inference via Sampling}
\begin{process}
    \begin{enumerate}
        \item Given samples 
        \item Calculate number of samples w/ the query and evidence variables.
        \item Calculate number of samples w/ the evidence variables.
        \item Approximate the probability of the query variable given the evidence variable by dividing the \# of samples w/ the query and evidence variables by the \# of samples w/ the evidence variables.
    \end{enumerate}
\end{process}

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Samples 
        \customFigure[0.5]{../Images/L6_12.png}{}
        \item \textbf{Problem:} Find the probability of $p(d \mid h)$.
        \item \textbf{Soln:} $p(d \mid h) \approx \frac{\# \text{ of samples w/ } d \text{ and } h}{\# \text{ of samples w/ } h} = \frac{3}{5} = 0.6$.
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Probability Review}
\begin{example}
    \begin{enumerate}
        \item[(a)] [1 pts] Assume \( A, B, C \) are random variables where \( A \perp B \), \( B \perp C \), and \( C \perp A \). Which of the following expressions are equivalent to \( P(A, B) \)?
        
        \begin{itemize}
            \item[ ] \( \boxed{\sum_c P(A, B, C = c)} \): Marginalizing over $C$
            \item[ ] \( \sum_c P(A|C = c) P(B|C = c) \)
            \item[ ] \(\boxed{\sum_c P(A) P(B|A) P(C = c | A, B)} \): Writing out in terms of $\sum_c P(A, B, C = c)$
            \item[ ] \( \boxed{P(A) P(B)} \): $A$ and $B$ are independent.
            \item[ ] None of the above
        \end{itemize}

        \item[(b)] [1 pts] Let \( A, B, C \) be random variables with \( A \not\perp B \). Is it possible for \( A \perp B | C \)? 
        
        \begin{itemize}
            \item[ ] $\boxed{\text{Yes}}$: Not being indepedent doesn't imply conditional independence.
            \item[ ] $\text{No}$
        \end{itemize}

        \item[(c)] [1 pts] Let \( \mathcal{V} \) denote the set of variables in a Bayesian network. Suppose \( X \in \mathcal{V} \), and let \texttt{pts}(\( X \)), \texttt{chl}(\( X \)), \texttt{ans}(\( X \)), and \texttt{des}(\( X \)) represent the parents, children, ancestors, and descendants of \( X \). Provide a general independence rule of the form: 
        \[
        X \perp \mathcal{V} \setminus \text{des}(X) \; \mid \; \texttt{pts}(X)
        \]
        You may use the set operations, \( \cap, \cup \) and/or \( \setminus \) if necessary.

        \item[(d)] [1 pts] Assume \( A, B, C, \) and \( D \) are binary random variables, and \( E \) is a trinary random variable over \( \{1,2,3\} \). What is \( \sum_{A,B} P(A|B, E = 1) \)?

        \begin{itemize}
            \item[ ] $P(A=0 \mid B=0, E=1) + P(A=1 \mid B=0, E=1) = 1$ 
            \item[ ] $P(A=0 \mid B=1, E=1) + P(A=1 \mid B=1, E=1) = 1$
            \item[] Therefore, $2$.
        \end{itemize}
    \end{enumerate}
\end{example}
