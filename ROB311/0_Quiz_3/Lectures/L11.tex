\begin{summary}
    In a \textbf{Multi-Agent problem}, we assume that:
    \begin{itemize}
        \item Set of states for environment is $\mathcal{S}$
        \item $P$ agents within environment. 
        \item For each state $s \in \mathcal{S}$: 
        \begin{itemize}
            \item possible actions for agent $i$ is $\mathcal{A}_i(s)$
            \item set of action profiles is $\mathcal{A}(s) = \prod_{i=1}^P \mathcal{A}_i(s)$
        \end{itemize}
        \item possible state-action pairs are $\mathcal{T} = \{(s,a) \text{ s.t. } s \in \mathcal{S}, a \in \mathcal{A}(s)\}$
        \item environment in some origin state, $s_0$ 
        \item environment destroyed after $N$ transitions 
        \item agent $j$ wants to find policy $\pi_j (a_j \mid s)$ so that $\mathbb{E}[r_j(p)]$ is maximized
        \item agents act independently given the environment's state: $\pi (a \mid s) = \prod_{j\in [P]} \pi_j (a_j \mid s)$
    \end{itemize}

    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Name} & \textbf{Function:} \\
            \midrule
            State transition given state-action pair defined by $\text{tr}: \mathcal{T} \to \mathcal{S}$ & $\text{tr}(s,a) = \text{state transition from $s$ under $a$}$ \\ 
            \midrule
            Reward to each agent, $i$ defined by $r_i$: $\mathcal{Q} \times \mathcal{S} \rightarrow \mathbb{R}_+$ & $r_i(s,a,\text{tr}(s,a)) = \text{rwd to agent $i$ for $(s,a,tr(s,a))$}$ \\
            \midrule
            State evolution of environment after $N$ transitions & $p = \langle (s_0,a^{(1)},s_{1}),\ldots,(s_{N-1},a^{(N)},s_{N})\rangle$ \\ 
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Given sequence of actions: $p.a = \langle a^{(1)},\ldots,a^{(n)}\rangle$
                \item $s_N = \tau (s_{n-1},a^{(n)})$
            \end{itemize}} \\
            \midrule
            reward to agent $i$ & $r_i(p) = \sum_{n=1}^N r_i (s_{n-1},a^{(n)}, s_n)$ \\
            \midrule
            expected-reward (value) of playing $a$ from $s$ for agent $j$ & $q_j (s,a) = r_j(s,a,\tau(s,a)) +$ \\
            & $\sum_{a' \in \mathcal{A}(\tau(s,a))} \pi(a' \mid \tau(s,a)) q_j(\tau(s,a),a')$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item $\mathcal{A}(s) = \emptyset$ if $s \in \mathcal{G}$
                \end{itemize}} \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{summary}

\subsection{Policy Equilibria}
\begin{notes}
    \begin{itemize}
        \item \textbf{No Regret:} $\pi$ is no-regret if $\pi_j$ maximizes $q_j$ when $\pi_{-j}$ is fixed. 
        \item If all agents play perfectly, then we expect
        \begin{equation*}
            \pi (a \mid s) = \begin{cases}
                1 & \text{if } a= a^*(s) \\
                0 & \text{otherwise}
            \end{cases}
        \end{equation*}
        \begin{itemize}
            \item $a_j^*(s) = \arg \max_{a_j \in \mathcal{A}_j(s)} q_j(s,a_j,a_{-j}^*)$ is the best action for agent $j$ given the other agents' policies.
        \end{itemize}
    \end{itemize}
\end{notes}
\newpage

\subsection{Single Action Games}
\begin{summary}
    In a \textbf{Single Action Game}, we assume that:
    \begin{itemize}
        \item $N=1$ (one-shot game)
        \item Initial state is $s_0 \in \mathcal{S}$
        \item Agent $j$ wants to find policy, $\pi_i (a_i \mid s_0)$ so $\mathbb{E}[r_i(p)]$ is maximized
    \end{itemize}
\end{summary}

\subsection{Actions (Deterministic)}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Name} & \textbf{Function:} \\
            \midrule
            Action $j$ for agent $i$ & $[0 \cdots 0 \; 1 \; 0 \cdots 0]$ \\
            \midrule
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item One-hot vector of $M_i$ components, $\mathbf{e}_{i,j}$
                \end{itemize}} \\
            \midrule
            Agent $i$'s set of possible actions & $\mathcal{A}_i = \{a_i \in \{0,1\}^{M_i} \mid \sum_{j \in [M_i]} a_{i,j} = 1\}$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item Agent $i$'s chosen action with $a_i \in \mathcal{A}_i$
                \end{itemize}} \\
            \midrule
            Action profile is a tuple of actions & $a=(a_1,\ldots,a_P)$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item \textbf{Notational Convenience:} $a_{-i} = (a_1,\ldots,a_{i-1},a_{i+1},\ldots,a_P)$ so that $a=(a_i,a_{-i})$. 
                \end{itemize}} \\
            \midrule 
            Optimal action profile & $a^+ \text{ s.t. } \forall a \exists i \text{ s.t. } r_i(a) < r_i(a^+)$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item An action profile w.r.t. which any other action profile leaves at least one player worse off.
                \end{itemize}} \\
            \midrule 
            Set of optimal action profiles & $\text{aOpt} = \{a^+ \mid \forall a \exists i: r_i(a) < r_i(a^+)\}$ \\
            \midrule
            Best-action mapping, $\text{ba}_i$: $\mathcal{A}_{-i} \to \mathcal{A}_i$ & $\text{ba}_i (a_{-i}) = \arg \max_{a_i \in \mathcal{A}_i} r_i (a_i,a_{-i})$ \\
            & $ = \{a_i \in \mathcal{A}_i \mid r_i (a_i,a_{-i}) = \max_{a_i' \in \mathcal{A}_i} r_i (a_i',a_{-i})\}$ \\
            \midrule 
            Agent $i$ will \textbf{not regret} playing $a_i^*$ when others play $a_{-i}^*$ if & $r_i (a_i^*,a_{-i}^*) \geq r_i (a_i,a_{-i}^*) \; \forall a_i \in \mathcal{A}_i$ \\
            & or $a_i^* \in \text{ba}_i (a_{-i}^*)$ \\
            \midrule
            Action equilibria is any action, $a^*$ in which no agent regrets & $a_i^* \in \text{ba}_i (a_{-i}^*) \; \forall i \in [P]$ \\
            \midrule
            Existence of action equilibria & May not always exist, i.e., it may be that $\text{aEq} = \emptyset$ \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{summary}

\subsection{Strategies (Probabilistic)}
\begin{summary}

\end{summary}

\subsubsection{Simplifying Games}
\begin{notes}

\end{notes}
\newpage

\subsection{Examples}
\subsubsection{Finding Action Equilibria}
\begin{process} To find action equilibria:
    \begin{enumerate}
        \item For each $i$, compute $\text{ba}_i (a_{-i})$ for all $a_{-i}$
        \item Define $\text{bap}_i$ so that $\text{bap}_i = \{(a_i',a_{-i}), \; \forall a_i' \in \text{ba}_i (a_{-i}), \; \forall a_{-i} \in \mathcal{A}_{-i}\}$
        \item Action equilibria are then $\text{aEq} = \bigcap_{i\in [P]} \text{bap}_i$.
    \end{enumerate}
\end{process}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Suppose lion and cavemen both want meat. Each must decide whether to fight for the food or share it. 
        \customFigure[0.5]{../Images/L11_3.png}{}
        \item \textbf{Problem:} Find the action equilibria 
        \item \textbf{Solution:}
        \begin{enumerate}
            \item \textbf{Best Action Profiles:}
            \begin{itemize}
                \item Cavemen: $\text{bap}_{\text{cavemen}} = \{(\text{Fight, Fight}), (\text{Fight, Share})\}$. Cavemen fights no matter what. 
                \begin{itemize}
                    \item If lion fights, then cavemen fights to get maximum reward in this scenario of $+1$. 
                    \item If lion shares, then caveman fights to get maximum reward in this scenario of $+4$.
                \end{itemize}
                \item Lion: $\text{bap}_{\text{lion}} = \{(\text{Fight, Fight}), (\text{Share, Fight})\}$. Lion fights no matter what.
                \begin{itemize}
                    \item If caveman fights, then lion fights to get maximum reward in this scenario of $+1$.
                    \item If caveman shares, then lion fights to get maximum reward in this scenario of $+4$.
                \end{itemize}
            \end{itemize}
            \item \textbf{Best Action Equilibria:} Intersection of the best action profiles.
            \begin{itemize}
                \item $\text{aEq} = \text{bap}_{\text{cavemen}} \cap \text{bap}_{\text{lion}} = \{(\text{Fight, Fight})\}$
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Suppose lion and cavemen both want meat. Each must decide whether to fight for the food or share it. 
        \customFigure[0.5]{../Images/L11_4.png}{}
        \item \textbf{Problem:} Find the action equilibria 
        \item \textbf{Solution:}
        \begin{enumerate}
            \item \textbf{Best Action Profiles:}
            \begin{itemize}
                \item Cavemen: $\text{bap}_{\text{cavemen}} = \{(\text{Fight, Fight}), (\text{Share, Share})\}$. Cavemen fights no matter what. 
                \begin{itemize}
                    \item If lion fights, then cavemen fights to get maximum reward in this scenario of $+3$. 
                    \item If lion shares, then caveman shares to get maximum reward in this scenario of $+1$.
                \end{itemize}
                \item Lion: $\text{bap}_{\text{lion}} = \{(\text{Fight, Share}), (\text{Share, Fight})\}$. Lion fights no matter what.
                \begin{itemize}
                    \item If caveman fights, then lion shares to get maximum reward in this scenario of $+3$.
                    \item If caveman shares, then lion fights to get maximum reward in this scenario of $+1$.
                \end{itemize}
            \end{itemize}
            \item \textbf{Best Action Equilibria:} Intersection of the best action profiles.
            \begin{itemize}
                \item $\text{aEq} = \text{bap}_{\text{cavemen}} \cap \text{bap}_{\text{lion}} = \emptyset$
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Optimal Action Profiles}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Suppose lion and cavemen both want meat. Each must decide whether to fight for the food or share it. 
        \customFigure[0.5]{../Images/L11_3.png}{}
        \item \textbf{Problem:} Find the optimal action profiles: 
        \item \textbf{Solution:}
        \begin{enumerate}
            \item $\text{aOpt} = \{(\text{Share, Share}), (\text{Fight, Share}), (\text{Share, Fight})\}$
            \begin{itemize}
                \item FINISH
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Finding Strategy Equilibria}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given/Problem:} Find all equilibria of the following one-shot game or state that none exist.
        \vspace{1em}
            \begin{center}
            \begin{tabular}{ccc}
            \toprule
            & \textbf{B1 (y)} & \textbf{B2 (1-y)} \\
            \midrule
            \textbf{A1 (x)} & (5, 3) & (1, 0) \\
            \textbf{A2 (1-x)} & (0, 1) & (2, 4) \\
            \bottomrule
            \end{tabular}
            \end{center}
        \vspace{1em}
        \begin{itemize}
            \item (\#,\#) is the payoff to P1 and P2 respectively for a given action profile.
        \end{itemize}
        \item \textbf{Solution:}
        \begin{enumerate}
            \item \textbf{Define Probabilities:}
            \begin{itemize}
                \item Let $y$ be the probability that B1 plays action B1 so $1-y$ is the probability that B1 plays action B2.
                \item Let $x$ be the probability that A1 plays action A1 so $1-x$ is the probability that A1 plays action A2.
            \end{itemize}
            \item \textbf{Expected Rewards:} 
            \begin{itemize}
                \item P1: 
                \begin{align*}
                    E[x] = 5xy + 1x(1-y) + 0(1-x)y + 2(1-x)(1-y) &= 5xy + x - xy + 2 - 2x - 2y + 2xy \\
                    &= 5xy - xy + 2xy + x - 2x - 2y + 2 \\
                    &= 6xy - x - 2y + 2 \quad \text{simplify} \\
                    &= \underbrace{(6y - 1)}_{c}x + 2 - 2y \quad \text{linear in $x$} 
                \end{align*}
                \item P2:
                \begin{align*}
                    E[y] = 3xy + 0x(1-y) + 1(1-x)y + 4(1-x)(1-y) &= 3xy + 0 + y - xy + 4 - 4x - 4y + 4xy \\
                    &= 3xy - xy + 4xy + y - 4x - 4y + 4 \\
                    &= 6xy - 4x - 3y + 4 \quad \text{simplify} \\
                    &= \underbrace{(6x - 3)}_{c}y + 4 - 4x \quad \text{linear in $y$}
                \end{align*}
                \item \textbf{Note:} $E[x]$ is linear in $x$ and $E[y]$ is linear in $y$.
            \end{itemize}
            \item \textbf{Constrained Argmax Expected Rewards w.r.t $x \in [0,1]$ (since P1):} If it was cost, then minimize. Also don't care about constant term in $y$ since we are derivating w.r.t $x$.
            \begin{itemize}
                \item P1: 
                \begin{equation*}
                    x = \begin{cases}
                        1 & \text{if } y > \frac{1}{6} \text{ i.e. } c > 0 \text{ since positive want maximum positive}\\
                        & \\
                        [0,1] & \text{if } y=\frac{1}{6} \text{ i.e. }c = 0 \text{ doesn't matter since 0} \\
                        & \\
                        0 & \text{if } y < \frac{1}{6} \text{ i.e. } c < 0 \text{ since negative want maximum negative}
                    \end{cases}
                \end{equation*}
                \item P2:
                \begin{equation*}
                    y = \begin{cases}
                        1 & \text{if } x > \frac{3}{6} \text{ i.e. } c > 0 \text{ since positive want maximum positive}\\
                        & \\
                        [0,1] & \text{if } x=\frac{3}{6} \text{ i.e. }c = 0 \text{ doesn't matter since 0} \\
                        & \\
                        0 & \text{if } x < \frac{3}{6} \text{ i.e. } c < 0 \text{ since negative want maximum negative}
                    \end{cases}
                \end{equation*}
            \end{itemize}
            \item \textbf{Finding all equilibrium:} Lines on the graph represents where your reward is maximized. 
            \customFigure[0.5]{../Images/L11_9.png}{}
            \begin{itemize}
                \item \textbf{Case 1:} $x=0$ and $y=0$
                \begin{itemize}
                    \item $P(\text{P1 chooses A1}) = 0$ 
                    \item $P(\text{P1 chooses A2}) = 1$
                    \item $P(\text{P2 chooses B1}) = 0$
                    \item $P(\text{P2 chooses B2}) = 1$
                \end{itemize}
                \item \textbf{Case 2:} $x=1/2$ and $y=1/6$
                \begin{itemize}
                    \item $P(\text{P1 chooses A1}) = 1/2$
                    \item $P(\text{P1 chooses A2}) = 1/2$
                    \item $P(\text{P2 chooses B1}) = 1/6$
                    \item $P(\text{P2 chooses B2}) = 5/6$
                \end{itemize}
                \item \textbf{Case 3:} $x=1$ and $y=1$
                \begin{itemize}
                    \item $P(\text{P1 chooses A1}) = 1$
                    \item $P(\text{P1 chooses A2}) = 0$
                    \item $P(\text{P2 chooses B1}) = 1$
                    \item $P(\text{P2 chooses B2}) = 0$
                \end{itemize}
            \end{itemize}
            \item \textbf{Unstable Equilibrium:} P1 moves left and right b/c $x$ is associated with $x$-axis. P2 moves up and down b/c $y$ is associated with $y$-axis.
            \customFigure[0.5]{../Images/L11_10.png}{}
            \begin{itemize}
                \item Stability means that in a radius disc around the equilibrium, if you move a little bit, you will still be in the equilibrium (have to check all relevant quadrants)
                \begin{itemize}
                    \item If one quadrant is unstable, then don't need to check the other quadrants as the equilibrium point is unstable. 
                    \item Simulatenous (both players move at the same time) and sequential (one player moves first and the other player moves second) 
                \end{itemize}
                \item \textbf{Case 1:} $x=0$ and $y=0$ is stable
                \begin{itemize}
                    \item Q1: Always converges to $(0,0)$ since P1 moves left to red and P2 moves down to turquoise.
                \end{itemize}
                \item \textbf{Case 2:} $x=1/2$ and $y=1/6$ is unstable
                \begin{itemize}
                    \item Q1 (Top Left): P1 moves right to red and P2 moves up to turquoise $\implies (1,1)$
                    \item Q2 (Top Right): P1 moves right to red and P2 moves up to turquoise $\implies (1,1)$
                    \item Q3 (Bottom Left): P1 moves left to red and P2 moves down to turquoise $\implies (0,0)$
                    \item Q4 (Bottom Right): P1 moves left to red and P2 moves down to turquoise $\implies (0,0)$
                \end{itemize}
                \item \textbf{Case 3:} $x=1$ and $y=1$ is stable
                \begin{itemize}
                    \item Q1: Always converges to $(1,1)$ since P1 moves left to red and P2 moves down to turquoise.
                \end{itemize}
            \end{itemize}
        \end{enumerate}
    \end{enumerate}

\end{example}

