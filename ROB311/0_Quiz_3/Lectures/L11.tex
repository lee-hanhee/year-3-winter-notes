\begin{summary}
    In a \textbf{Multi-Agent problem}, we assume that:
    \begin{itemize}
        \item Set of states for environment is $\mathcal{S}$
        \item $P$ agents within environment. 
        \item For each state $s \in \mathcal{S}$: 
        \begin{itemize}
            \item possible actions for agent $i$ is $\mathcal{A}_i(s)$
            \item set of action profiles is $\mathcal{A}(s) = \prod_{i=1}^P \mathcal{A}_i(s)$
        \end{itemize}
        \item possible state-action pairs are $\mathcal{T} = \{(s,a) \text{ s.t. } s \in \mathcal{S}, a \in \mathcal{A}(s)\}$
        \item environment in some origin state, $s_0$ 
        \item environment destroyed after $N$ transitions 
        \item agent $j$ wants to find policy $\pi_j (a_j \mid s)$ so that $\mathbb{E}[r_j(p)]$ is maximized
        \item agents act independently given the environment's state: $\pi (a \mid s) = \prod_{j\in [P]} \pi_j (a_j \mid s)$
    \end{itemize}

    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Name} & \textbf{Function:} \\
            \midrule
            State transition given state-action pair defined by $\text{tr}: \mathcal{T} \to \mathcal{S}$ & $\text{tr}(s,a) = \text{state transition from $s$ under $a$}$ \\ 
            \midrule
            Reward to each agent, $i$ defined by $r_i$: $\mathcal{Q} \times \mathcal{S} \rightarrow \mathbb{R}_+$ & $r_i(s,a,\text{tr}(s,a)) = \text{rwd to agent $i$ for $(s,a,tr(s,a))$}$ \\
            \midrule
            State evolution of environment after $N$ transitions & $p = \langle (s_0,a^{(1)},s_{1}),\ldots,(s_{N-1},a^{(N)},s_{N})\rangle$ \\ 
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Given sequence of actions: $p.a = \langle a^{(1)},\ldots,a^{(n)}\rangle$
                \item $s_N = \tau (s_{n-1},a^{(n)})$
            \end{itemize}} \\
            \midrule
            reward to agent $i$ & $r_i(p) = \sum_{n=1}^N r_i (s_{n-1},a^{(n)}, s_n)$ \\
            \midrule
            expected-reward (value) of playing $a$ from $s$ for agent $j$ & $q_j (s,a) = r_j(s,a,\tau(s,a)) +$ \\
            & $\sum_{a' \in \mathcal{A}(\tau(s,a))} \pi(a' \mid \tau(s,a)) q_j(\tau(s,a),a')$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item $\mathcal{A}(s) = \emptyset$ if $s \in \mathcal{G}$
                \end{itemize}} \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{summary}

\subsection{Policy Equilibria}
\begin{notes}
    \begin{itemize}
        \item \textbf{No Regret:} $\pi$ is no-regret if $\pi_j$ maximizes $q_j$ when $\pi_{-j}$ is fixed. 
        \item If all agents play perfectly, then we expect
        \begin{equation*}
            \pi (a \mid s) = \begin{cases}
                1 & \text{if } a= a^*(s) \\
                0 & \text{otherwise}
            \end{cases}
        \end{equation*}
        \begin{itemize}
            \item $a_j^*(s) = \arg \max_{a_j \in \mathcal{A}_j(s)} q_j(s,a_j,a_{-j}^*)$ is the best action for agent $j$ given the other agents' policies.
        \end{itemize}
    \end{itemize}
\end{notes}

\begin{warning}
    No regret if it got the highest reward given the other players' action. 
\end{warning}
\newpage

\subsection{Single Action Games}
\begin{summary}
    In a \textbf{Single Action Game}, we assume that:
    \begin{itemize}
        \item $N=1$ (one-shot game)
        \item Initial state is $s_0 \in \mathcal{S}$
        \item Agent $j$ wants to find policy, $\pi_i (a_i \mid s_0)$ so $\mathbb{E}[r_i(p)]$ is maximized
    \end{itemize}
\end{summary}

\subsection{Actions (Deterministic)}
\begin{summary} Allow each agent to choose action deterministically.
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Name} & \textbf{Function:} \\
            \midrule
            Action $j$ for agent $i$ & $[0 \cdots 0 \; 1 \; 0 \cdots 0]^T$ \\
            \midrule
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item One-hot vector of $M_i$ components, $\mathbf{e}_{i,j}$
                \end{itemize}} \\
            \midrule
            Agent $i$'s set of possible actions & $\mathcal{A}_i = \left\{a_i \in \{0,1\}^{M_i} \mid \sum_{j \in [M_i]} a_{i,j} = 1\right\}$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item Agent $i$'s chosen action with $a_i \in \mathcal{A}_i$
                \end{itemize}} \\
            \midrule
            Action profile is a tuple of actions & $a=(a_1,\ldots,a_P)$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item \textbf{Notational Convenience:} $a_{-i} = (a_1,\ldots,a_{i-1},a_{i+1},\ldots,a_P)$ so that $a=(a_i,a_{-i})$. 
                \end{itemize}} \\
            \midrule 
            Optimal action profile & $a^+ \text{ s.t. } \forall a \exists i \text{ s.t. } r_i(a) < r_i(a^+)$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item An action profile w.r.t. which any other action profile leaves at least one player worse off.
                \end{itemize}} \\
            \midrule 
            Set of optimal action profiles & $\text{aOpt} = \{a^+ \mid \forall a \exists i: r_i(a) < r_i(a^+)\}$ \\
            \midrule
            Best-action mapping, $\text{ba}_i$: $\mathcal{A}_{-i} \to \mathcal{A}_i$ & $\text{ba}_i (a_{-i}) = \arg \max_{a_i \in \mathcal{A}_i} r_i (a_i,a_{-i})$ \\
            & $ = \{a_i \in \mathcal{A}_i \mid r_i (a_i,a_{-i}) = \max_{a_i' \in \mathcal{A}_i} r_i (a_i',a_{-i})\}$ \\
            \midrule 
            Agent $i$ will \textbf{not regret} playing $a_i^*$ when others play $a_{-i}^*$ if & $r_i (a_i^*,a_{-i}^*) \geq r_i (a_i,a_{-i}^*) \; \forall a_i \in \mathcal{A}_i$ \\
            & or $a_i^* \in \text{ba}_i (a_{-i}^*)$ \\
            \midrule
            Action equilibria is any action, $a^*$ in which no agent regrets & $a_i^* \in \text{ba}_i (a_{-i}^*) \; \forall i \in [P]$ \\
            \midrule
            Existence of action equilibria & May not always exist, i.e., it may be that $\text{aEq} = \emptyset$ \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Strategies (Probabilistic)}
\begin{summary} Allow each agent to choose action based on a distribution/strategy. 
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Name} & \textbf{Function:} \\
            \midrule
            Stategy for agent $i$ & $[0.05 \cdots 0.2 \; 0.7 \; 0 \cdots 0.05]^T$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item Vector of $M_i$ components, that are non-negative and sum to 1
                \end{itemize}} \\
            \midrule
            Agent $i$'s set of possible strategies & $\Delta_i = \Delta^{M_i} = \left\{x_i \in [0,1]^{M_i}, \sum_{j \in [M_i]} x_{i,j} = 1 \right\}$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item Agent $i$'s chosen strategy with $x_i \in \Delta_i$
                \end{itemize}} \\
            \midrule
            Expected reward & $\bar{r}_i(x_1,\ldots,x_P) = \mathbb{E}[r_i(a)] = \sum_{a_i \in \mathcal{A}_i} \pi(a) r_i(a)$ \\
            \midrule
            Stategy profile is a tuple of strategies & $x=(x_1,\ldots,x_P)$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item \textbf{Notational Convenience:} $x_{-i} = (x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_P)$ so that $x=(x_i,x_{-i})$. 
                \end{itemize}} \\
            \midrule
            No-regret strategies & $\bar{r}_i(x_i^*,x_{-i}^*) \geq \bar{r}_i(x_i,x_{-i}^*) \; \forall x_i \in \Delta_i$ \\
            & $x_i^* \in \text{bs}_i (x_{-i}^*)$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item Agent $i$ will not regret using $x_i^*$ when others use $x_{-i}^*$.
                \end{itemize}} \\
            \midrule
            Best strategy mapping $\text{bs}_i$: $\Delta_{-i} \to \Delta_i$ & $\text{bs}_i (x_{-i}) = \arg \max_{x_i \in \Delta_i} \bar{r}_i (x_i,x_{-i})$ \\
            & $ = \{x_i \in \Delta_i \mid \bar{r}_i (x_i,x_{-i}) = \max_{x_i' \in \Delta_i} \bar{r}_i (x_i',x_{-i})\}$ \\
            \midrule
            Strategy equilibria is any strategy, $x^*$ in which no agent regrets & $x_i^* \in \text{bs}_i (x_{-i}^*) \; \forall i \in [P]$ \\
            \midrule
            Joint best-strategy mapping $\text{bs}: \Delta \to \Delta$ & $\text{bs}(x) = (\text{bs}_1(x_{-1}),\ldots,\text{bs}_P(x_{-P}))$ \\
            \midrule
            Existence of strategy equilibria & Any strategy equilibrium, $x^*$ is a fixed pt. \\
            & of $x^* = \text{bs}(x^*)$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item Fixed pt. always exists. 
                \end{itemize}} \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{summary}

\subsubsection{Simplifying Games}
\begin{notes} May be able to reduce $M_i$ by eliminating useless actions/strategies: 
    \begin{itemize}
        \item \textbf{Equivalent Stategies:} $x_i^{(1)} \equiv x_i^{(2)}$
        \begin{equation*}
            \bar{r}_i(x_i^{(1)},x_{-i}) = \bar{r}_i(x_i^{(2)},x_{-i}) \; \forall x_{-i} 
        \end{equation*}
        \item \textbf{Dominated Strategies:} $x_i$ 
        \begin{equation*}
            \exists x_i' \text{ s.t. } \bar{r}_i(x_i',x_{-i}) \leq \bar{r}_i(x_i,x_{-i}) \; \forall x_{-i}
        \end{equation*}
    \end{itemize}
    Can remove dominated and equilvalent strategies w/o changing the game. 
\end{notes}
\newpage

\subsection{Examples}
\subsubsection{Finding Action Equilibria}
\begin{process} To find action equilibria:
    \begin{enumerate}
        \item For each $i$, compute $\text{ba}_i (a_{-i})$ for all $a_{-i}$
        \item Define $\text{bap}_i$ so that $\text{bap}_i = \{(a_i',a_{-i}), \; \forall a_i' \in \text{ba}_i (a_{-i}), \; \forall a_{-i} \in \mathcal{A}_{-i}\}$
        \item Action equilibria are then $\text{aEq} = \bigcap_{i\in [P]} \text{bap}_i$.
    \end{enumerate}
\end{process}

\begin{process}
    \begin{enumerate}
        \item Fix strategy (i.e. prob. 1) for other player, then find best move for current player by getting max reward. 
        \item See if there's intersection between best responses.
    \end{enumerate}
\end{process}

\begin{warning}
    \begin{itemize}
        \item Action equilibrium is a pure equilibrium (i.e. prob. 1 or 0)
        \begin{itemize}
            \item aEq doesn't mean that the actual outcome is fight,fight. 
            \item aEq doesn't mean it is socially optimal (i.e. equilibria may not be optimal). 
        \end{itemize}
        \item Mixed equilibrium is a probabilistic equilibrium (i.e. prob. $p$)
        \begin{itemize}
            \item Every action equilibrium is a mixed equilibrium, but not vice versa.
        \end{itemize}
    \end{itemize}
\end{warning}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Consider a 2-player single-action game, in which each player has 2 actions. Let $a_1$ and $a_2$ be their chosen actions and $r_1(a_1, a_2), r_2(a_1, a_2)$ be the resulting rewards. 
            
            \[
            \begin{array}{c c|c|c}
            a_1 & a_2 & r_1(a_1, a_2) & r_2(a_1, a_2) \\
            \hline
            1 & 1 & 2 & 2 \\
            1 & 2 & 4 & 1 \\
            2 & 1 & 1 & 4 \\
            2 & 2 & 3 & 3 \\
            \end{array}
            \]
        \item \textbf{Solution:} 
        \begin{itemize}
            \item Convert:
            \[
            \begin{array}{c|c|c}
            & a_2 = 1 & a_2 = 2 \\
            \hline
            a_1 = 1 & (2,2) & (4,1) \\
            a_1 = 2 & (1,4) & (3,3) \\
            \end{array}
            \]
            \item \textbf{Fix} $a_2$, \textbf{choose }$a_1$:
            \begin{align*}
            \text{If } a_2 = 1 &\Rightarrow a_1 = 1 \Rightarrow (1,1) \\
            \text{If } a_2 = 2 &\Rightarrow a_1 = 1 \Rightarrow (1,2) \\
            &\Rightarrow \text{bap}_1 = \{(1,1), (1,2)\}
            \end{align*}

            \item \textbf{Fix} $a_1$, \textbf{choose }$a_2$:
            \begin{align*}
            \text{If } a_1 = 1 &\Rightarrow a_2 = 1 \Rightarrow (1,1) \\
            \text{If } a_1 = 2 &\Rightarrow a_2 = 1 \Rightarrow (2,1) \\
            &\Rightarrow \text{bap}_2 = \{(1,1), (2,2)\}
            \end{align*}

            \noindent
            \textbf{Intersection:}
            \[
                \text{aEq} = \text{bap}_1 \cap \text{bap}_2 = \{(1,1)\}
            \]
        \end{itemize}
        \item Therefore, the action equilibria is $(1,1)$.
    \end{enumerate}

\end{example}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Suppose lion and cavemen both want meat. Each must decide whether to fight for the food or share it. 
        \customFigure[0.5]{../Images/L11_3.png}{}
        \vspace{-1em}
        \item \textbf{Problem:} Find the action equilibria 
        \item \textbf{Solution:}
        \begin{enumerate}
            \item \textbf{Best Action Profiles:}
            \begin{itemize}
                \item Cavemen: $\text{bap}_{\text{cavemen}} = \{(\text{Fight, Fight}), (\text{Fight, Share})\}$. Cavemen fights no matter what. 
                \begin{itemize}
                    \item If lion fights, then cavemen fights to get maximum reward in this scenario of $+1$. 
                    \item If lion shares, then caveman fights to get maximum reward in this scenario of $+4$.
                \end{itemize}
                \item Lion: $\text{bap}_{\text{lion}} = \{(\text{Fight, Fight}), (\text{Share, Fight})\}$. Lion fights no matter what.
                \begin{itemize}
                    \item If caveman fights, then lion fights to get maximum reward in this scenario of $+1$.
                    \item If caveman shares, then lion fights to get maximum reward in this scenario of $+4$.
                \end{itemize}
            \end{itemize}
            \item \textbf{Best Action Equilibria:} Intersection of the best action profiles.
            \begin{itemize}
                \item $\text{aEq} = \text{bap}_{\text{cavemen}} \cap \text{bap}_{\text{lion}} = \{(\text{Fight, Fight})\}$
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{example}

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Suppose lion and cavemen both want meat. Each must decide whether to fight for the food or share it. 
        \customFigure[0.5]{../Images/L11_4.png}{}
        \vspace{-1em}
        \item \textbf{Problem:} Find the action equilibria 
        \item \textbf{Solution:}
        \begin{enumerate}
            \item \textbf{Best Action Profiles:}
            \begin{itemize}
                \item Cavemen: $\text{bap}_{\text{cavemen}} = \{(\text{Fight, Fight}), (\text{Share, Share})\}$. Cavemen fights no matter what. 
                \begin{itemize}
                    \item If lion fights, then cavemen fights to get maximum reward in this scenario of $+3$. 
                    \item If lion shares, then caveman shares to get maximum reward in this scenario of $+1$.
                \end{itemize}
                \item Lion: $\text{bap}_{\text{lion}} = \{(\text{Fight, Share}), (\text{Share, Fight})\}$. Lion fights no matter what.
                \begin{itemize}
                    \item If caveman fights, then lion shares to get maximum reward in this scenario of $+3$.
                    \item If caveman shares, then lion fights to get maximum reward in this scenario of $+1$.
                \end{itemize}
            \end{itemize}
            \item \textbf{Best Action Equilibria:} Intersection of the best action profiles.
            \begin{itemize}
                \item $\text{aEq} = \text{bap}_{\text{cavemen}} \cap \text{bap}_{\text{lion}} = \emptyset$
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Optimal Action Profiles}
\begin{process}
    \begin{enumerate}
        \item Switching action profiles will leave at least one player worse off.
        \begin{itemize}
            \item That is, no unilateral deviation by any player should result in both players being better off.
        \end{itemize}
    \end{enumerate}
\end{process} 

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:}
         \[
        \begin{array}{c c|c|c}
        a_1 & a_2 & r_1(a_1, a_2) & r_2(a_1, a_2) \\
        \hline
        1 & 1 & 2 & 2 \\
        1 & 2 & 4 & 1 \\
        2 & 1 & 1 & 4 \\
        2 & 2 & 3 & 3 \\
        \end{array}
        \]
        
        \item \textbf{Solution:}
        \begin{itemize}
            \item \textbf{At action profile }$(1,1)$:
            \[
            \text{Switch to }(2,2): \text{ both players are better off} \Rightarrow \text{not optimal}
            \]
        
            \item \textbf{At action profile }$(1,2)$:
            \[
            \text{Switch to }(1,1), (2,1), (2,2): \text{ Player 1 will be worse off for all switches} \Rightarrow \text{optimal}
            \]
        
            \item \textbf{At action profile }$(2,1)$:
            \[
            \text{Switch to }(1,1), (1,2), (2,2): \text{ Player 2 will be worse off for all switches} \Rightarrow \text{optimal}
            \]
        
            \item \textbf{At action profile }$(2,2)$:
            \begin{align*}
                \text{Switch to }(1,1): &\text{ Both Player 1 and Player 2 will be worse off} \\
                \text{Switch to }(2,1): &\text{ Player 1 will be worse off} \\
                \text{Switch to }(1,2): &\text{ Player 2 will be worse off} \Rightarrow \text{optimal} 
            \end{align*}
            \item Therefore, $(1,2)$, $(2,1)$, and $(2,2)$ are optimal action profiles.
        \end{itemize}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Suppose lion and cavemen both want meat. Each must decide whether to fight for the food or share it. 
        \customFigure[0.5]{../Images/L11_3.png}{}
        \item \textbf{Problem:} Find the optimal action profiles: 
        \item \textbf{Solution:}
        \begin{enumerate}
            \item $\text{aOpt} = \{(\text{Share, Share}), (\text{Fight, Share}), (\text{Share, Fight})\}$
            \begin{itemize}
                \item \textbf{(Fight, Fight) $\rightarrow$ (1, 1):} \\
                Both players could be better off by choosing (Share, Share), which gives (2, 2). \\
                Since there exists another outcome where no one is worse off and at least one player is better off, this is not an optimal action profile. \\
                $\Rightarrow$ \textbf{Not optimal}.
                
                \item \textbf{(Fight, Share) $\rightarrow$ (4, 0):} \\
                The caveman gets the highest possible payoff (4), but the lion gets 0. \\
                Any other outcome that gives the lion more than 0 will reduce the caveman’s payoff. \\
                So, every other option makes at least one player worse off. \\
                $\Rightarrow$ \textbf{Optimal}.
                
                \item \textbf{(Share, Fight) $\rightarrow$ (0, 4):} \\
                The lion gets the highest possible payoff (4), but the caveman gets 0. \\
                Any other outcome that gives the caveman more than 0 will reduce the lion’s payoff. \\
                So, every other option makes at least one player worse off. \\
                $\Rightarrow$ \textbf{Optimal}.
                
                \item \textbf{(Share, Share) $\rightarrow$ (2, 2):} \\
                Both players receive a fair and equal payoff. No other outcome makes both players better off at the same time. \\
                So, changing this would hurt at least one of them. \\
                $\Rightarrow$ \textbf{Optimal}.
            \end{itemize}                                 
        \end{enumerate}
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Finding/Convergence Strategy Equilibria}
\begin{process} \textbf{Finding}
    \begin{enumerate}
        \item For each $i$, compute $\text{bs}_i (x_{-i})$ for all $x_{-i}$
        \item Define $\text{bsp}_i$ so that $\text{bsp}_i = \{(x_i',x_{-i}), \; \forall x_i' \in \text{bs}_i (x_{-i}), \; \forall x_{-i} \in \Delta_{-i}\}$
        \item Strategy equilibria are then $\text{sEq} = \bigcap_{i\in [P]} \text{bsp}_i$.
    \end{enumerate}
    \begin{itemize}
        \item Requires each agent, $j$, to know $\bar{r}_1,\ldots,\bar{r}_P$
    \end{itemize}
\end{process}

\begin{warning}
    \begin{itemize}
        \item If on the line, then don't move for the player, then it's optimal, so don't move it. 
    \end{itemize}
\end{warning}
\newpage

\begin{example}
    \begin{enumerate}
        \item 4-1, pg. 29-35
    \end{enumerate}
\end{example}

\begin{example}
    \begin{enumerate}
        \item 4-1, pg. 46-47
    \end{enumerate}
\end{example}

\begin{example}
    \begin{enumerate}
        \item \textbf{Given/Problem:} Find all equilibria of the following one-shot game or state that none exist.
        \vspace{1em}
            \begin{center}
            \begin{tabular}{ccc}
            \toprule
            & \textbf{B1 (y)} & \textbf{B2 (1-y)} \\
            \midrule
            \textbf{A1 (x)} & (5, 3) & (1, 0) \\
            \textbf{A2 (1-x)} & (0, 1) & (2, 4) \\
            \bottomrule
            \end{tabular}
            \end{center}
        \vspace{1em}
        \begin{itemize}
            \item (\#,\#) is the payoff to P1 and P2 respectively for a given action profile.
        \end{itemize}
        \item \textbf{Solution:}
        \begin{enumerate}
            \item \textbf{Define Probabilities:}
            \begin{itemize}
                \item Let $y$ be the probability that B1 plays action B1 so $1-y$ is the probability that B1 plays action B2.
                \item Let $x$ be the probability that A1 plays action A1 so $1-x$ is the probability that A1 plays action A2.
            \end{itemize}
            \item \textbf{Expected Rewards:} 
            \begin{itemize}
                \item P1: 
                \begin{align*}
                    E[x] = 5xy + 1x(1-y) + 0(1-x)y + 2(1-x)(1-y) &= 5xy + x - xy + 2 - 2x - 2y + 2xy \\
                    &= 5xy - xy + 2xy + x - 2x - 2y + 2 \\
                    &= 6xy - x - 2y + 2 \quad \text{simplify} \\
                    &= \underbrace{(6y - 1)}_{c}x + 2 - 2y \quad \text{linear in $x$} 
                \end{align*}
                \item P2:
                \begin{align*}
                    E[y] = 3xy + 0x(1-y) + 1(1-x)y + 4(1-x)(1-y) &= 3xy + 0 + y - xy + 4 - 4x - 4y + 4xy \\
                    &= 3xy - xy + 4xy + y - 4x - 4y + 4 \\
                    &= 6xy - 4x - 3y + 4 \quad \text{simplify} \\
                    &= \underbrace{(6x - 3)}_{c}y + 4 - 4x \quad \text{linear in $y$}
                \end{align*}
                \item \textbf{Note:} $E[x]$ is linear in $x$ and $E[y]$ is linear in $y$.
            \end{itemize}
            \item \textbf{Constrained Argmax Expected Rewards w.r.t $x \in [0,1]$ (since P1):} If it was cost, then minimize. Also don't care about constant term in $y$ since we are derivating w.r.t $x$.
            \begin{itemize}
                \item P1: 
                \begin{equation*}
                    \text{bs}_{\text{A}}(x) = \begin{cases}
                        1 & \text{if } y > \frac{1}{6} \text{ i.e. } c > 0 \text{ since positive want maximum positive}\\
                        & \\
                        [0,1] & \text{if } y=\frac{1}{6} \text{ i.e. }c = 0 \text{ doesn't matter since 0} \\
                        & \\
                        0 & \text{if } y < \frac{1}{6} \text{ i.e. } c < 0 \text{ since negative want maximum negative}
                    \end{cases}
                \end{equation*}
                \item P2:
                \begin{equation*}
                    \text{bs}_{\text{B}}(y) = \begin{cases}
                        1 & \text{if } x > \frac{3}{6} \text{ i.e. } c > 0 \text{ since positive want maximum positive}\\
                        & \\
                        [0,1] & \text{if } x=\frac{3}{6} \text{ i.e. }c = 0 \text{ doesn't matter since 0} \\
                        & \\
                        0 & \text{if } x < \frac{3}{6} \text{ i.e. } c < 0 \text{ since negative want maximum negative}
                    \end{cases}
                \end{equation*}
            \end{itemize}
            \item \textbf{Finding all equilibrium:} Lines on the graph represents where your reward is maximized. 
            \customFigure[0.5]{../Images/L11_9.png}{}
            \begin{itemize}
                \item \textbf{Case 1:} $x=0$ and $y=0$
                \begin{itemize}
                    \item $P(\text{P1 chooses A1}) = 0$ 
                    \item $P(\text{P1 chooses A2}) = 1$
                    \item $P(\text{P2 chooses B1}) = 0$
                    \item $P(\text{P2 chooses B2}) = 1$
                \end{itemize}
                \item \textbf{Case 2:} $x=1/2$ and $y=1/6$
                \begin{itemize}
                    \item $P(\text{P1 chooses A1}) = 1/2$
                    \item $P(\text{P1 chooses A2}) = 1/2$
                    \item $P(\text{P2 chooses B1}) = 1/6$
                    \item $P(\text{P2 chooses B2}) = 5/6$
                \end{itemize}
                \item \textbf{Case 3:} $x=1$ and $y=1$
                \begin{itemize}
                    \item $P(\text{P1 chooses A1}) = 1$
                    \item $P(\text{P1 chooses A2}) = 0$
                    \item $P(\text{P2 chooses B1}) = 1$
                    \item $P(\text{P2 chooses B2}) = 0$
                \end{itemize}
            \end{itemize}
            \item \textbf{Unstable Equilibrium:} P1 moves left and right b/c $x$ is associated with $x$-axis. P2 moves up and down b/c $y$ is associated with $y$-axis.
            \customFigure[0.5]{../Images/L11_10.png}{}
            \begin{itemize}
                \item Stability means that in a radius disc around the equilibrium, if you move a little bit, you will still be in the equilibrium (have to check all relevant quadrants)
                \begin{itemize}
                    \item If one quadrant is unstable, then don't need to check the other quadrants as the equilibrium point is unstable. 
                    \item Simulatenous (both players move at the same time) and sequential (one player moves first and the other player moves second) 
                \end{itemize}
                \item \textbf{Case 1:} $x=0$ and $y=0$ is stable
                \begin{itemize}
                    \item Q1: Always converges to $(0,0)$ since P1 moves left to red and P2 moves down to turquoise.
                \end{itemize}
                \item \textbf{Case 2:} $x=1/2$ and $y=1/6$ is unstable
                \begin{itemize}
                    \item Q1 (Top Left): P1 moves right to red and P2 moves up to turquoise $\implies (1,1)$
                    \item Q2 (Top Right): P1 moves right to red and P2 moves up to turquoise $\implies (1,1)$
                    \item Q3 (Bottom Left): P1 moves left to red and P2 moves down to turquoise $\implies (0,0)$
                    \item Q4 (Bottom Right): P1 moves left to red and P2 moves down to turquoise $\implies (0,0)$
                \end{itemize}
                \item \textbf{Case 3:} $x=1$ and $y=1$ is stable
                \begin{itemize}
                    \item Q1: Always converges to $(1,1)$ since P1 moves left to red and P2 moves down to turquoise.
                \end{itemize}
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Simplifying Games}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} 
        \customFigure[0.5]{../Images/L11_8.png}{}
        \item \textbf{Problem:} Simplify the game 
        \item \textbf{Solution:}
        \begin{enumerate}
            \item 'Leave' action for Cavemen is dominated so it will never choose this action b/c it can get more reward by choosing Fight or Share. 
            \item As aresult, we can remove it from the game and have a 2 action, 2 player game, rather than a 2-3 action, 2 player game.
        \end{enumerate}
    \end{enumerate}
\end{example}

