\begin{summary}
    In a \textbf{Multi-Agent problem}, we assume that:
    \begin{itemize}
        \item Set of states for environment is $\mathcal{S}$
        \item $P$ agents within environment. 
        \item For each state $s \in \mathcal{S}$: 
        \begin{itemize}
            \item possible actions for agent $i$ is $\mathcal{A}_i(s)$
            \item set of action profiles is $\mathcal{A}(s) = \prod_{i=1}^P \mathcal{A}_i(s)$
        \end{itemize}
        \item possible state-action pairs are $\mathcal{T} = \{(s,a) \text{ s.t. } s \in \mathcal{S}, a \in \mathcal{A}(s)\}$
        \item environment in some origin state, $s_0$ 
        \item environment destroyed after $N$ transitions 
        \item agent $j$ wants to find policy $\pi_j (a_j \mid s)$ so that $\mathbb{E}[r_j(p)]$ is maximized
        \item agents act independently given the environment's state: $\pi (a \mid s) = \prod_{j\in [P]} \pi_j (a_j \mid s)$
    \end{itemize}

    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Name} & \textbf{Function:} \\
            \midrule
            State transition given state-action pair defined by $\text{tr}: \mathcal{T} \to \mathcal{S}$ & $\text{tr}(s,a) = \text{state transition from $s$ under $a$}$ \\ 
            \midrule
            Reward to each agent, $i$ defined by $r_i$: $\mathcal{Q} \times \mathcal{S} \rightarrow \mathbb{R}_+$ & $r_i(s,a,\text{tr}(s,a)) = \text{rwd to agent $i$ for $(s,a,tr(s,a))$}$ \\
            \midrule
            State evolution of environment after $N$ transitions & $p = \langle (s_0,a^{(1)},s_{1}),\ldots,(s_{N-1},a^{(N)},s_{N})\rangle$ \\ 
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Given sequence of actions: $p.a = \langle a^{(1)},\ldots,a^{(n)}\rangle$
                \item $s_N = \tau (s_{n-1},a^{(n)})$
            \end{itemize}} \\
            \midrule
            reward to agent $i$ & $r_i(p) = \sum_{n=1}^N r_i (s_{n-1},a^{(n)}, s_n)$ \\
            \midrule
            expected-reward (value) of playing $a$ from $s$ for agent $j$ & $q_j (s,a) = r_j(s,a,\tau(s,a)) +$ \\
            & $\sum_{a' \in \mathcal{A}(\tau(s,a))} \pi(a' \mid \tau(s,a)) q_j(\tau(s,a),a')$ \\
            \multicolumn{2}{p{\linewidth}}{
                \begin{itemize}
                    \item $\mathcal{A}(s) = \emptyset$ if $s \in \mathcal{G}$
                \end{itemize}} \\
            \bottomrule            
        \end{tabular}
    \end{center}
\end{summary}

\subsection{Action Equilibria}

\subsubsection{Finding Action Equilibria}

\subsection{Strategy Equilibria}

\subsubsection{Finding Strategy Equilibria}

\subsubsection{Existence of Stategy Equilibria}

\subsubsection{Convergence of Stategy Equilibria}

\subsection{Examples}
\subsubsection{Finding Action Equilibria}

\subsubsection{Optimal Action Profiles}

