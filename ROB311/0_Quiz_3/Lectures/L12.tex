\begin{summary}
    In a \textbf{zero-sum turn-based games}, we assume that 
    \begin{itemize}
        \item \textbf{Agents and Environment:} 
        \begin{itemize}
            \item there are two agents, called the \textcolor{red}{\textbf{maximizer}} and \textcolor{blue}{\textbf{minimizer}}
            \item the environment is always in one of a discrete set of states, $\mathcal{S}$
            \item a subset of the states, $\mathcal{T} \subseteq \mathcal{S}$, are terminal states
            \item there is only one decision maker for each non-terminal state, $s \in \mathcal{S} \setminus \mathcal{T}$
            \item For each non-terminal state, $s \in \mathcal{S} \setminus \mathcal{T}$, the decision-maker has a discrete set of actions, $\mathcal{A}(s)$
        \end{itemize}
        \item \textbf{Decision Process:} At time-step $t$, the decision-maker will: 
        \begin{itemize}
            \item \textbf{Observe:} Observe the state $s_t$ 
            \item \textbf{Select:} Select an action $a_t \in \mathcal{A}(s_t)$
            \item \textbf{Move:} Make the move $(s_t,a_t)$
        \end{itemize}
        \item \textbf{State Transitions:} 
        \begin{itemize}
            \item Environment transitions to a deterministic state, $s_{t+1}$, based on a stationary fn, 
            \begin{equation*}
                s_{t+1} = \text{tr}(s_t,a_t)
            \end{equation*}
            \item Once a terminal state is reached (if $s_{t+1} \in \mathcal{T}$), the maximizer obtains a reward for the final transition based on a reward fn, $r(\cdot,\cdot,\cdot)$:
            \begin{equation*}
                r(s_t,a_t,s_{t+1}) = \text{maximizer's reward for reaching state $s_{t+1}$}
            \end{equation*}
            \begin{equation*}
                - r(s_t,a_t,s_{t+1}) = \text{minimizer's reward for reaching state $s_{t+1}$}
            \end{equation*}
        \end{itemize}
    \end{itemize}
\end{summary}

\begin{warning}
    \begin{itemize}
        \item Maximizer is trying to maximize the reward of agent 1
        \item Minimizer is trying to minimize the reward of agent 1 (i.e. maximize the reward of agent 2)
    \end{itemize}
\end{warning}

\subsection{$\alpha$/$\beta$ Pruning}
\begin{motivation}
    Don't explore the entire game tree by pruning branches that are unreachable under perfect play.
\end{motivation}
\begin{definition}
    For each state $s$: 
    \begin{itemize}
        \item \textcolor{red}{$\alpha_s$}: Maximum value at $s$ thus far (initially $-\infty$)
        \item \textcolor{blue}{$\beta_s$}: Minimum value at $s$ thus far (initially $+\infty$)
    \end{itemize}
\end{definition}

\subsubsection{$\alpha$ Cuts}
\begin{definition}
    If the \textcolor{red}{\textbf{maximizer}} is the turn-taker at $s$, then \textcolor{red}{$\alpha_s$} increases to the maximum value of $s$'s successors as they are explored, and \textcolor{blue}{$\beta_s$} $=$ \textcolor{blue}{$\beta_{\text{parent}(s)}$}.
    \begin{itemize}
        \item If \textcolor{red}{$\alpha_s$} increases beyond \textcolor{blue}{$\beta_s$}, then $s$ unreachable under perfect play.
    \end{itemize}
\end{definition}

\subsubsection{$\beta$ Cuts}
\begin{definition}
    If the \textcolor{blue}{\textbf{minimizer}} is the turn-taker at $s$, then \textcolor{blue}{$\beta_s$} decreases to the minimum value of $s$'s successors as they are explored, and \textcolor{red}{$\alpha_s$} $=$ \textcolor{red}{$\alpha_{\text{parent}(s)}$}.
    \begin{itemize}
        \item If \textcolor{blue}{$\beta_s$} decreases beyond \textcolor{red}{$\alpha_s$}, then $s$ unreachable under perfect play. 
    \end{itemize}
\end{definition}
\newpage

\subsection{Examples}
\subsubsection{Min-Max Algorithm}
\begin{example}
    \begin{itemize}
        \item \textbf{Given:} Cavemen is injured from his hunt. He has extra food, but needs medicine.
        \begin{itemize}
            \item He meets another caveman who is willing to trade. 
        \end{itemize} 
        \customFigure[0.5]{../Images/L12_1.png}{States}
        \vspace{-1.5em}
        \customFigure[0.5]{../Images/L12_2.png}{Actions}
        \vspace{-1.5em}
        \customFigure[0.5]{../Images/L12_3.png}{Decision Process}
        \customFigure[0.5]{../Images/L12_0.png}{Game Tree}
        \begin{itemize}
            \item States
            \begin{itemize}
                \item Red triangle: Maximizing agent
                \item Blue triangle: Minimizing agent
                \item White circles with \#s: terminal states
                \item Rewards: In red b/c it's for the maximizer. The minimizer's reward is the negative of the maximizer's reward.
            \end{itemize}
            \item Actions: Square boxes are actions
        \end{itemize}
        \item \textbf{Solution:} Backtracking through the game tree, we can find the optimal path for the maximizer and minimizer.
        \begin{itemize}
            \item \textbf{Maximizer Turn:} 
            \begin{itemize}
                \item \textbf{Left Branch:}
                \begin{itemize}
                    \item Far Left: Accept to get reward of 8, 
                    \item Mid Left: Accept to get reward of 16, 
                \end{itemize}
                \item \textbf{Right Branch:}
                \begin{itemize}
                    \item Mid Left: Accept to get reward of 2, 
                    \item Far Left: Accept to get reward of 4
                \end{itemize}
            \end{itemize}
            \item \textbf{Minimizer Turn:} 
            \begin{itemize}
                \item \textbf{Left Branch:}
                \begin{itemize}
                    \item L: 1 medicine to make maximizer get reward of 8, 
                \end{itemize}
                \item \textbf{Right Branch:}
                \begin{itemize}
                    \item L: 1 medicine to make maximizer get reward of 2
                \end{itemize}
            \end{itemize}
            \item \textbf{Maximizer Turn:} 1 food to make maximizer get reward of 8 b/c going right will make maximizer get reward of 2
            \item \textbf{Optimal Path:} Therefore, the optimal path will be LLL b/c the maximizer will get a reward of 8, while the minimizer will reduce the reward from 16 to 8.
            \begin{itemize}
                \item Assume boths agents play optimally, this will be the path taken. 
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{example}
\newpage

\subsubsection{$\alpha$ Cuts}
\begin{example}
    \begin{itemize}
        \item Explored 14, 12 and now $\beta_{\text{parent}(s)} = \beta_s = 5$, so this will be compared for $\alpha_s$ until $\alpha_s > \beta_s$ b/c then $s$ unreachable under perfect play.
        \item Iterate: 
        \begin{itemize}
            \item $\alpha_s = -\infty < \alpha_s' = 2 \rightarrow \alpha_s = 2$, but $\alpha_s = 2 < \beta_s = 5$ 
            \item $\alpha_s = 2 < \alpha_s' = 4 \rightarrow \alpha_s = 4$, but $\alpha_s = 4 < \beta_s = 5$
            \item $\alpha_s = 4 < \alpha_s' = 9 \rightarrow \alpha_s = 9$, and $\alpha_s = 9 > \beta_s = 5$, therefore, prune all the other branches that haven't been explored yet in the children of $s$ paths
        \end{itemize}
    \end{itemize}
    \customFigure[0.5]{../Images/L12_4.png}{}
\end{example}

\subsubsection{$\beta$ Cuts}
\begin{example}
    \begin{itemize}
        \item Explored $4$,$6$, and now $\alpha_{\text{parent}(s)} = \alpha_s = 7$, so this will be compared for $\beta_s$ until $\beta_s < \alpha_s$ b/c then $s$ unreachable under perfect play.
        \item Iterate:
        \begin{itemize}
            \item $\beta_s = +\infty > \beta_s' = 9 \rightarrow \beta_s = 9$, but $\beta_s = 9 > \alpha_s = 7$
            \item $\beta_s = 9 > \beta_s' = 8 \rightarrow \beta_s = 5$, but $\beta_s = 8 > \alpha_s = 7$
            \item $\beta_s = 8 > \beta_s' = 3 \rightarrow \beta_s = 3$, and $\beta_s = 3 < \alpha_s = 7$, therefore, prune all the other branches that haven't been explored yet in the children of $s$ paths
        \end{itemize}
    \end{itemize}
    \customFigure[0.5]{../Images/L12_5.png}{}
\end{example}
\newpage

\subsubsection{Alpha Beta Pruning}
\begin{process}
    \begin{enumerate}
        \item Initialize $\alpha = -\infty$ and $\beta = +\infty$
        \item Iterate through the game tree:
        \begin{itemize}
            \item If the maximizer is the turn-taker, then update $\alpha$ to the maximum value of $s$'s successors as they are explored.
            \item If the minimizer is the turn-taker, then update $\beta$ to the minimum value of $s$'s successors as they are explored.
        \end{itemize}
        \item Nodes are pruned if $\alpha \geq \beta$
    \end{enumerate}
\end{process}

\begin{example} \href{https://pascscha.ch/info2/abTreePractice/}{Alpha-Beta Pruning Practice}
\end{example}