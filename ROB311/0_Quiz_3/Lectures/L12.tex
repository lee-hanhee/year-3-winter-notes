\begin{summary}
    In a \textbf{zero-sum turn-based games}, we assume that 
    \begin{itemize}
        \item \textbf{Agents and Environment:} 
        \begin{itemize}
            \item there are two agents, called the \textcolor{red}{\textbf{maximizer}} and \textcolor{blue}{\textbf{minimizer}}
            \item the environment is always in one of a discrete set of states, $\mathcal{S}$
            \item a subset of the states, $\mathcal{T} \subseteq \mathcal{S}$, are terminal states
            \item there is only one decision maker for each non-terminal state, $s \in \mathcal{S} \setminus \mathcal{T}$
            \item For each non-terminal state, $s \in \mathcal{S} \setminus \mathcal{T}$, the decision-maker has a discrete set of actions, $\mathcal{A}(s)$
        \end{itemize}
        \item \textbf{Decision Process:} At time-step $t$, the decision-maker will: 
        \begin{itemize}
            \item \textbf{Observe:} Observe the state $s_t$ 
            \item \textbf{Select:} Select an action $a_t \in \mathcal{A}(s_t)$
            \item \textbf{Move:} Make the move $(s_t,a_t)$
        \end{itemize}
        \item \textbf{State Transitions:} 
        \begin{itemize}
            \item Environment transitions to a deterministic state, $s_{t+1}$, based on a stationary fn, 
            \begin{equation*}
                s_{t+1} = \text{tr}(s_t,a_t)
            \end{equation*}
            \item Once a terminal state is reached (if $s_{t+1} \in \mathcal{T}$), the maximizer obtains a reward for the final transition based on a reward fn, $r(\cdot,\cdot,\cdot)$:
            \begin{equation*}
                r(s_t,a_t,s_{t+1}) = \text{maximizer's reward for reaching state $s_{t+1}$}
            \end{equation*}
            \begin{equation*}
                - r(s_t,a_t,s_{t+1}) = \text{minimizer's reward for reaching state $s_{t+1}$}
            \end{equation*}
        \end{itemize}
    \end{itemize}
\end{summary}

\begin{warning}
    \begin{itemize}
        \item Maximizer is trying to maximize the reward of agent 1
        \item Minimizer is trying to minimize the reward of agent 1 (i.e. maximize the reward of agent 2)
    \end{itemize}
\end{warning}

\subsection{$\alpha$/$\beta$ Pruning}
\begin{motivation}
    Don't explore the entire game tree by pruning branches that are unreachable under perfect play.
\end{motivation}
\begin{definition}
    For each state $s$: 
    \begin{itemize}
        \item \textcolor{red}{$\alpha_s$}: Maximum value at $s$ thus far (initially $-\infty$)
        \item \textcolor{blue}{$\beta_s$}: Minimum value at $s$ thus far (initially $+\infty$)
    \end{itemize}
\end{definition}

\subsubsection{$\alpha$ Cuts}
\begin{definition}
    If the \textcolor{red}{\textbf{maximizer}} is the turn-taker at $s$, then \textcolor{red}{$\alpha_s$} increases to the maximum value of $s$'s successors as they are explored, and \textcolor{blue}{$\beta_s$} $=$ \textcolor{blue}{$\beta_{\text{parent}(s)}$}.
    \begin{itemize}
        \item If \textcolor{red}{$\alpha_s$} increases beyond \textcolor{blue}{$\beta_s$}, then $s$ unreachable under perfect play.
    \end{itemize}
\end{definition}

\subsubsection{$\beta$ Cuts}
\begin{definition}
    If the \textcolor{blue}{\textbf{minimizer}} is the turn-taker at $s$, then \textcolor{blue}{$\beta_s$} decreases to the minimum value of $s$'s successors as they are explored, and \textcolor{red}{$\alpha_s$} $=$ \textcolor{red}{$\alpha_{\text{parent}(s)}$}.
    \begin{itemize}
        \item If \textcolor{blue}{$\beta_s$} decreases beyond \textcolor{red}{$\alpha_s$}, then $s$ unreachable under perfect play. 
    \end{itemize}
\end{definition}
\newpage

\subsection{Monte-Carlo Tree Search (MCTS) Algorithm}
\begin{algo}
    \begin{enumerate}
        \item Selection: Traverse using an alternate policy until a node has unexplored children. 
        \customFigure[0.4]{../Images/L12_6.png}{}
        \begin{itemize}
            \item Our Agent (Upper Triangle): Uses UCB to choose the next node to explore
            \item Other Agent (Down Triangle): Can't control their actions, so this agent picks w/ their own heuristic.
            \item Square Boxes: Estimated values (i.e. $n$ and $\hat{q}$)
            \item Ends when there is at least one action that hasn't been explored yet. In this case, two actions ahven't been explored. 
            \item Can skip expansion and simulation if the most recently expanded node is a terminal state.
        \end{itemize}
        \item Expansion: Expand an unexplored child; initialize $n(a)$ and $\hat{q}(s,a)$.
        \customFigure[0.4]{../Images/L12_7.png}{}
        \begin{itemize}
            \item $\hat{q}(s,a)$ is initialized to 0 and $n(a)$ is initialized to 1 b/c we've visited this node once.
            \item Randomly pick an unexplored action unless there is only one action left.
            \item Can skip similuation if the most recently expanded node is a terminal state. 
        \end{itemize}
        \item Simulation: Traverse using the random policy until a terminal node is reached. 
        \customFigure[0.4]{../Images/L12_8.png}{}
        \begin{itemize}
            \item Using random policy to simulate the game until a terminal state is reached (i.e. reward is obtained)
        \end{itemize}
        \item Back-propogation: Get the reward and reverse; update $n(a)$ and $\hat{q}(s,a)$.
        \customFigure[0.4]{../Images/L12_9.png}{}
        \begin{itemize}
            \item Go up the path in yellow and update the values of $n(a)$ and $\hat{q}(s,a)$ for OUR agent only (i.e. the upper triangle)
        \end{itemize}
    \end{enumerate}
\end{algo}

\begin{warning}
    \begin{itemize}
        \item Works for more than 2 agents. 
        \item Don't need to know anyone else's reward function. 
        \item Has to be turn taking but can be not alternating (i.e. immediate switch between agents)
        \item Can augment simultaneous actions 
        \item Communication 
        \item Works fo rnon-zero sum games. 
    \end{itemize}
\end{warning}
\newpage



\subsection{Examples}
\subsubsection{Zero Sum Turn-Based Games}
\begin{example}
    \begin{itemize}
        \item \textbf{Given:} Cavemen is injured from his hunt. He has extra food, but needs medicine.
        \begin{itemize}
            \item He meets another caveman who is willing to trade. 
        \end{itemize} 
        \customFigure[0.5]{../Images/L12_1.png}{States}
        \vspace{-1.5em}
        \customFigure[0.5]{../Images/L12_2.png}{Actions}
        \vspace{-1.5em}
        \customFigure[0.5]{../Images/L12_3.png}{Decision Process}
        \customFigure[0.5]{../Images/L12_0.png}{Game Tree}
        \begin{itemize}
            \item States
            \begin{itemize}
                \item Red triangle: Maximizing agent
                \item Blue triangle: Minimizing agent
                \item White circles with \#s: terminal states
                \item Rewards: In red b/c it's for the maximizer. The minimizer's reward is the negative of the maximizer's reward.
            \end{itemize}
            \item Actions: Square boxes are actions
        \end{itemize}
        \item \textbf{Solution:} Backtracking through the game tree, we can find the optimal path for the maximizer and minimizer.
        \begin{itemize}
            \item \textbf{Maximizer Turn:} LL: Accept to get reward of 8, L: Accept to get reward of 16, R: Accept to get reward of 2, RR: Accept to get reward of 4
            \item \textbf{Minimizer Turn:} LL: 1 medicine to make maximizer get reward of 8, R: 1 medicine to make maximizer get reward of 2
            \item \textbf{Maximizer Turn:} 1 food to make maximizer get reward of 8 b/c going right will make maximizer get reward of 2
            \item \textbf{Optimal Path:} Therefore, the optimal path will be LLL b/c the maximizer will get a reward of 8, while the minimizer will reduce the reward from 16 to 8.
            \begin{itemize}
                \item Assume boths agents play optimally, this will be the path taken. 
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{example}

\subsubsection{$\alpha$ Cuts}
\begin{example}
    \begin{itemize}
        \item Explored 14, 12 and now $\beta_{\text{parent}(s)} = \beta_s = 5$, so this will be compared for $\alpha_s$ until $\alpha_s > \beta_s$ b/c then $s$ unreachable under perfect play.
        \item Iterate: 
        \begin{itemize}
            \item $\alpha_s = -\infty < \alpha_s' = 2 \rightarrow \alpha_s = 2$, but $\alpha_s = 2 < \beta_s = 5$ 
            \item $\alpha_s = 2 < \alpha_s' = 4 \rightarrow \alpha_s = 4$, but $\alpha_s = 4 < \beta_s = 5$
            \item $\alpha_s = 4 < \alpha_s' = 9 \rightarrow \alpha_s = 9$, and $\alpha_s = 9 > \beta_s = 5$, therefore, prune all the other branches that haven't been explored yet in the children of $s$ paths
        \end{itemize}
    \end{itemize}
    \customFigure[0.5]{../Images/L12_4.png}{}
\end{example}

\subsubsection{$\beta$ Cuts}
\begin{example}
    \begin{itemize}
        \item Explored $4$,$6$, and now $\alpha_{\text{parent}(s)} = \alpha_s = 7$, so this will be compared for $\beta_s$ until $\beta_s < \alpha_s$ b/c then $s$ unreachable under perfect play.
        \item Iterate:
        \begin{itemize}
            \item $\beta_s = +\infty > \beta_s' = 9 \rightarrow \beta_s = 9$, but $\beta_s = 9 > \alpha_s = 7$
            \item $\beta_s = 9 > \beta_s' = 8 \rightarrow \beta_s = 5$, but $\beta_s = 8 > \alpha_s = 7$
            \item $\beta_s = 8 > \beta_s' = 3 \rightarrow \beta_s = 3$, and $\beta_s = 3 < \alpha_s = 7$, therefore, prune all the other branches that haven't been explored yet in the children of $s$ paths
        \end{itemize}
    \end{itemize}
    \customFigure[0.5]{../Images/L12_5.png}{}
\end{example}

\subsubsection{Alpha Beta Pruning}
\begin{process}
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{process}

\begin{example} \href{https://pascscha.ch/info2/abTreePractice/}{Alpha-Beta Pruning Practice}
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{example}

\subsubsection{Monte-Carlo Tree Search (MCTS) Algorithm}
\begin{example}
    
\end{example}