\begin{summary}
    In a \textbf{zero-sum turn-based games}, we assume that 
    \begin{itemize}
        \item \textbf{Agents and Environment:} 
        \begin{itemize}
            \item there are two agents, called the \textcolor{red}{\textbf{maximizer}} and \textcolor{blue}{\textbf{minimizer}}
            \item the environment is always in one of a discrete set of states, $\mathcal{S}$
            \item a subset of the states, $\mathcal{T} \subseteq \mathcal{S}$, are terminal states
            \item there is only one decision maker for each non-terminal state, $s \in \mathcal{S} \setminus \mathcal{T}$
            \item For each non-terminal state, $s \in \mathcal{S} \setminus \mathcal{T}$, the decision-maker has a discrete set of actions, $\mathcal{A}(s)$
        \end{itemize}
        \item \textbf{Decision Process:} At time-step $t$, the decision-maker will: 
        \begin{itemize}
            \item \textbf{Observe:} Observe the state $s_t$ 
            \item \textbf{Select:} Select an action $a_t \in \mathcal{A}(s_t)$
            \item \textbf{Move:} Make the move $(s_t,a_t)$
        \end{itemize}
        \item \textbf{State Transitions:} 
        \begin{itemize}
            \item Environment transitions to a deterministic state, $s_{t+1}$, based on a stationary fn, 
            \begin{equation*}
                s_{t+1} = \text{tr}(s_t,a_t)
            \end{equation*}
            \item Once a terminal state is reached (if $s_{t+1} \in \mathcal{T}$), the maximizer obtains a reward for the final transition based on a reward fn, $r(\cdot,\cdot,\cdot)$:
            \begin{equation*}
                r(s_t,a_t,s_{t+1}) = \text{maximizer's reward for reaching state $s_{t+1}$}
            \end{equation*}
            \begin{equation*}
                - r(s_t,a_t,s_{t+1}) = \text{minimizer's reward for reaching state $s_{t+1}$}
            \end{equation*}
        \end{itemize}
    \end{itemize}
\end{summary}

\begin{warning}
    \begin{itemize}
        \item Maximizer is trying to maximize the reward of agent 1
        \item Minimizer is trying to minimize the reward of agent 1 (i.e. maximize the reward of agent 2)
    \end{itemize}
\end{warning}

\subsection{$\alpha$/$\beta$ Pruning}
\begin{motivation}
    Don't explore the entire game tree by pruning branches that are unreachable under perfect play.
\end{motivation}
\begin{definition}
    For each state $s$: 
    \begin{itemize}
        \item \textcolor{red}{$\alpha_s$}: Maximum value at $s$ thus far (initially $-\infty$)
        \item \textcolor{blue}{$\beta_s$}: Minimum value at $s$ thus far (initially $+\infty$)
    \end{itemize}
\end{definition}

\subsubsection{$\alpha$ Cuts}
\begin{definition}
    If the \textcolor{red}{\textbf{maximizer}} is the turn-taker at $s$, then \textcolor{red}{$\alpha_s$} increases to the maximum value of $s$'s successors as they are explored, and \textcolor{blue}{$\beta_s$} $=$ \textcolor{blue}{$\beta_{\text{parent}(s)}$}.
    \begin{itemize}
        \item If \textcolor{red}{$\alpha_s$} increases beyond \textcolor{blue}{$\beta_s$}, then $s$ unreachable under perfect play.
    \end{itemize}
\end{definition}

\subsubsection{$\beta$ Cuts}
\begin{definition}
    If the \textcolor{blue}{\textbf{minimizer}} is the turn-taker at $s$, then \textcolor{blue}{$\beta_s$} decreases to the minimum value of $s$'s successors as they are explored, and \textcolor{red}{$\alpha_s$} $=$ \textcolor{red}{$\alpha_{\text{parent}(s)}$}.
    \begin{itemize}
        \item If \textcolor{blue}{$\beta_s$} decreases beyond \textcolor{red}{$\alpha_s$}, then $s$ unreachable under perfect play. 
    \end{itemize}
\end{definition}
\newpage

\subsection{Monte-Carlo Tree Search (MCTS) Algorithm}
\begin{algo}
    \begin{enumerate}
        \item Selection: Traverse using an alternate policy until a node has unexplored children. 
        \customFigure[0.4]{../Images/L12_6.png}{}
        \begin{itemize}
            \item Our Agent (Upper Triangle): Uses UCB to choose the next node to explore
            \item Other Agent (Down Triangle): Can't control their actions, so this agent picks w/ their own heuristic.
            \item Square Boxes: Estimated values (i.e. $n$ and $\hat{q}$)
            \item Ends when there is at least one action that hasn't been explored yet. In this case, two actions ahven't been explored. 
            \item Can skip expansion and simulation if the most recently expanded node is a terminal state.
        \end{itemize}
        \item Expansion: Expand an unexplored child; initialize $n(a)$ and $\hat{q}(s,a)$.
        \customFigure[0.4]{../Images/L12_7.png}{}
        \begin{itemize}
            \item $\hat{q}(s,a)$ is initialized to 0 and $n(a)$ is initialized to 1 b/c we've visited this node once.
            \item Randomly pick an unexplored action unless there is only one action left.
            \item Can skip similuation if the most recently expanded node is a terminal state. 
        \end{itemize}
        \item Simulation: Traverse using the random policy until a terminal node is reached. 
        \customFigure[0.4]{../Images/L12_8.png}{}
        \begin{itemize}
            \item Using random policy to simulate the game until a terminal state is reached (i.e. reward is obtained)
        \end{itemize}
        \item Back-propogation: Get the reward and reverse; update $n(a)$ and $\hat{q}(s,a)$.
        \customFigure[0.4]{../Images/L12_9.png}{}
        \begin{itemize}
            \item Go up the path in yellow and update the values of $n(a)$ and $\hat{q}(s,a)$ for OUR agent only (i.e. the upper triangle)
        \end{itemize}
    \end{enumerate}
\end{algo}

\begin{warning}
    \begin{itemize}
        \item Works for more than 2 agents. 
        \item Don't need to know anyone else's reward function. 
        \item Has to be turn taking but can be not alternating (i.e. immediate switch between agents)
        \item Can augment simultaneous actions 
        \item Communication 
        \item Works fo rnon-zero sum games. 
    \end{itemize}
\end{warning}
\newpage



\subsection{Examples}
\subsubsection{Zero Sum Turn-Based Games}
\begin{example}
    \begin{itemize}
        \item \textbf{Given:} Cavemen is injured from his hunt. He has extra food, but needs medicine.
        \begin{itemize}
            \item He meets another caveman who is willing to trade. 
        \end{itemize} 
        \customFigure[0.5]{../Images/L12_1.png}{States}
        \vspace{-1.5em}
        \customFigure[0.5]{../Images/L12_2.png}{Actions}
        \vspace{-1.5em}
        \customFigure[0.5]{../Images/L12_3.png}{Decision Process}
        \customFigure[0.5]{../Images/L12_0.png}{Game Tree}
        \begin{itemize}
            \item States
            \begin{itemize}
                \item Red triangle: Maximizing agent
                \item Blue triangle: Minimizing agent
                \item White circles with \#s: terminal states
                \item Rewards: In red b/c it's for the maximizer. The minimizer's reward is the negative of the maximizer's reward.
            \end{itemize}
            \item Actions: Square boxes are actions
        \end{itemize}
        \item \textbf{Solution:} Backtracking through the game tree, we can find the optimal path for the maximizer and minimizer.
        \begin{itemize}
            \item \textbf{Maximizer Turn:} LL: Accept to get reward of 8, L: Accept to get reward of 16, R: Accept to get reward of 2, RR: Accept to get reward of 4
            \item \textbf{Minimizer Turn:} LL: 1 medicine to make maximizer get reward of 8, R: 1 medicine to make maximizer get reward of 2
            \item \textbf{Maximizer Turn:} 1 food to make maximizer get reward of 8 b/c going right will make maximizer get reward of 2
            \item \textbf{Optimal Path:} Therefore, the optimal path will be LLL b/c the maximizer will get a reward of 8, while the minimizer will reduce the reward from 16 to 8.
            \begin{itemize}
                \item Assume boths agents play optimally, this will be the path taken. 
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{example}
\newpage

\subsubsection{$\alpha$ Cuts}
\begin{example}
    \begin{itemize}
        \item Explored 14, 12 and now $\beta_{\text{parent}(s)} = \beta_s = 5$, so this will be compared for $\alpha_s$ until $\alpha_s > \beta_s$ b/c then $s$ unreachable under perfect play.
        \item Iterate: 
        \begin{itemize}
            \item $\alpha_s = -\infty < \alpha_s' = 2 \rightarrow \alpha_s = 2$, but $\alpha_s = 2 < \beta_s = 5$ 
            \item $\alpha_s = 2 < \alpha_s' = 4 \rightarrow \alpha_s = 4$, but $\alpha_s = 4 < \beta_s = 5$
            \item $\alpha_s = 4 < \alpha_s' = 9 \rightarrow \alpha_s = 9$, and $\alpha_s = 9 > \beta_s = 5$, therefore, prune all the other branches that haven't been explored yet in the children of $s$ paths
        \end{itemize}
    \end{itemize}
    \customFigure[0.5]{../Images/L12_4.png}{}
\end{example}

\subsubsection{$\beta$ Cuts}
\begin{example}
    \begin{itemize}
        \item Explored $4$,$6$, and now $\alpha_{\text{parent}(s)} = \alpha_s = 7$, so this will be compared for $\beta_s$ until $\beta_s < \alpha_s$ b/c then $s$ unreachable under perfect play.
        \item Iterate:
        \begin{itemize}
            \item $\beta_s = +\infty > \beta_s' = 9 \rightarrow \beta_s = 9$, but $\beta_s = 9 > \alpha_s = 7$
            \item $\beta_s = 9 > \beta_s' = 8 \rightarrow \beta_s = 5$, but $\beta_s = 8 > \alpha_s = 7$
            \item $\beta_s = 8 > \beta_s' = 3 \rightarrow \beta_s = 3$, and $\beta_s = 3 < \alpha_s = 7$, therefore, prune all the other branches that haven't been explored yet in the children of $s$ paths
        \end{itemize}
    \end{itemize}
    \customFigure[0.5]{../Images/L12_5.png}{}
\end{example}
\newpage

\subsubsection{Alpha Beta Pruning}
\begin{process}
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{process}

\begin{example} \href{https://pascscha.ch/info2/abTreePractice/}{Alpha-Beta Pruning Practice}
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Monte-Carlo Tree Search (MCTS) Algorithm}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Consider a simplified two-player turn-based game tree. You are currently at the root node $S_0$, which has three possible actions $a_1, a_2, a_3$. The current statistics of its children are as follows:
        \vspace{1em}
        \begin{center}
        \begin{tabular}{ccc}
        \toprule
        \textbf{Action} & $N(s_0, a)$ & $\bar{X}(s_0, a)$ \\
        \midrule
        $a_1$ & 10 & 0.6 \\
        $a_2$ & 5 & 0.8 \\
        $a_3$ & 0 & -- \\
        \bottomrule
        \end{tabular}
        \end{center}
        \vspace{1em}
        \begin{itemize}
            \item $N(s_0, a)$: Number of times action $a$ has been selected at state $s_0$
            \item $\bar{X}(s_0, a)$: Average reward obtained from action $a$ at state $s_0$
            \item $\text{UCB} = \bar{X}(s_0, a) + \sqrt{\frac{\ln(t)}{N(s_0, a)}}$ 
            \begin{itemize}
                \item $t$: Total number of actions taken at $s_0$
            \end{itemize}
        \end{itemize}
        \item \textbf{Problems:} 
        \begin{itemize}
            \item If we were to use the UCB algorithm, which nodes get selected during the selection phase? Which node gets expanded during the expansion phase?
            \item Suppose from the expanded node, simulation is performed until termination. A reward of $+1$ is obtained. Update the statistics at $s_0$ accordingly.
            \item Then, repeat the question, assuming a reward of $-1$ is attained after the simulation phase.
        \end{itemize}
        \item \textbf{Solution:}
        \begin{enumerate}
            \item \textbf{Selection 1:} $s_0$ since we traverse until a node has unexplored children (i.e. $s_3$ is unexplored)
            \item \textbf{Expansion 1:} $s_3$ is automatically expanded since it is the only unexplored child of $s_0$ w/ $N(s_0, a_3) = 1$ and $\bar{X}(s_0, a_3) = 0$
            \item \textbf{Simulation 1:} Get a reward of $+1$ 
            \item \textbf{Back Propogation 1:} For this edge from $s_0$ to $s_3$, we update the statistics as follows:
            \begin{itemize}
                \item $N(s_0, a_3) = 1$
                \item $\bar{X}(s_0, a_3) = \frac{1}{1} = 1$
            \end{itemize}
            \item \textbf{Selection 2:} $s_0$ and choose the action with the highest UCB value for $s_1$, $s_2$, and $s_3$:
            \begin{itemize}
                \item $UCB(s_0, a_1) = 0.6 + \sqrt{\frac{\ln(16)}{10}} = 1.13$ 
                \item $UCB(s_0, a_2) = 0.8 + \sqrt{\frac{\ln(16)}{5}} = 1.54$ 
                \item $UCB(s_0, a_3) = 1 + \sqrt{\frac{\ln(16)}{1}} = 2.67$. Therefore, choose $s_3$ as part of the selection phase and assume it has unexplored children.
            \end{itemize}
            \item \textbf{Expansion 2:} Not enough info but assume we expand an unexplored child. 
            \item \textbf{Simulation 2:} Get a reward of $-1$
            \item \textbf{Back Propogation 2:} For this edge from $s_0$ to $s_3$, we update the statistics as follows:
            \begin{itemize}
                \item $N(s_0, a_3) = 2$
                \item $\bar{X}(s_0, a_3) = \frac{1 + (-1)}{2} = 0$
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Consider (partial) 2-player turn-taking game-tree in which 21 iterations of MCTS have already been performed: 
        \customFigure[0.5]{../Images/L12_11.png}{}
        \begin{itemize}
            \item Total reward: Numerator
            \item Total number of times action $a$ has been selected at state $s$: Denominator
        \end{itemize}
        \item \textbf{Problem:} If we use UCB to rank order state-action pairs, which of the following states will be chosen during the 22nd selection phase. 
        \item \textbf{Solution:}
        \begin{itemize}
            \item $\text{UCB}(AB) = 7/10 + \sqrt{\frac{\ln(21)}{10}} = 1.25$
            \begin{itemize}
                \item $\text{UCB}(BE) = 2/4 + \sqrt{\frac{\ln(10)}{4}} = 1.26$
                \item $\text{UCB}(BF) = 1/6 + \sqrt{\frac{\ln(10)}{6}} = 0.79$
            \end{itemize}
            \item $\text{UCB}(AC) = 5/8 + \sqrt{\frac{\ln(21)}{8}} = 1.24$
            \item $\text{UCB}(AD) = 0/3 + \sqrt{\frac{\ln(21)}{3}} = 1.01$
            \item Therefore, choose A,B,E by selecting the nodes with the highest UCB values.
        \end{itemize}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Consider (partial) 2-player turn-taking game-tree in which 9 iterations of MCTS have already been performed: 
        \customFigure[0.5]{../Images/L12_12.png}{}
        \begin{itemize}
            \item \textbf{Fix:} $CG$ has $0/2$ not $0/1$ and $CH$ has $0/1$ not $1/1$
        \end{itemize}
        \item \textbf{Problem:} Suppose path chosen during the 10th selection phase had the state sequence $\langle A,C,H \rangle$ (i.e. $H$ is the state expanded during the 10th expansion phase)
        \begin{itemize}
            \item The simulation phase lasts for $12$ transitions, after which a terminal state is reached. 
            \item The reward to the last turn-taker was $+4$.
            \item Find $q(A,\langle A,B \rangle)$, $q(A,\langle A,C \rangle)$, $q(C,\langle C,H \rangle)$
        \end{itemize}
        \item \textbf{Solution:}
        \begin{itemize}
            \item Assuming P1 starts at $A$, then P2 goes at $C$, then P1 goes at $H$, that means after 12 transitions (\textbf{even number}), P1 is the last turn-taker, therefore, P1 gets the reward of $+4$.
            \item \textbf{Backpropogation:}
            \begin{itemize}
                \item $N(C,\langle C,H \rangle) = 1$, $X(C,\langle C,H \rangle) = 4$ so $4/1$
                \item $N(A,\langle A,C \rangle) = 4$, $X(A,\langle A,C \rangle) = 2+4=6$ so $6/4$
                \item $q(A,\langle A,B \rangle) = 2/5 = 0.4$
                \item $q(A,\langle A,C \rangle) = 6/4 = 1.5$
                \item $q(C,\langle C,H \rangle) = 4/1 = 4$
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{example}