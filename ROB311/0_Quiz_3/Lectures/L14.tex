\begin{algo}
    \begin{enumerate}
        \item Selection: Traverse using an alternate policy until a node has unexplored children. 
        \customFigure[0.4]{../Images/L12_6.png}{}
        \begin{itemize}
            \item Our Agent (Upper Triangle): Uses UCB to choose the next node to explore
            \item Other Agent (Down Triangle): Can't control their actions, so this agent picks w/ their own heuristic.
            \item Square Boxes: Estimated values (i.e. $n$ and $\hat{q}$)
            \item Ends when there is at least one action that hasn't been explored yet. In this case, two actions ahven't been explored. 
            \item Can skip expansion and simulation if the most recently expanded node is a terminal state.
        \end{itemize}
        \item Expansion: Expand an unexplored child; initialize $n(a)$ and $\hat{q}(s,a)$.
        \customFigure[0.4]{../Images/L12_7.png}{}
        \begin{itemize}
            \item $\hat{q}(s,a)$ is initialized to 0 and $n(a)$ is initialized to 1 b/c we've visited this node once.
            \item Randomly pick an unexplored action unless there is only one action left.
            \item Can skip similuation if the most recently expanded node is a terminal state. 
        \end{itemize}
        \item Simulation: Traverse using the random policy until a terminal node is reached. 
        \customFigure[0.4]{../Images/L12_8.png}{}
        \begin{itemize}
            \item Using random policy to simulate the game until a terminal state is reached (i.e. reward is obtained)
        \end{itemize}
        \item Back-propogation: Get the reward and reverse; update $n(a)$ and $\hat{q}(s,a)$.
        \customFigure[0.4]{../Images/L12_9.png}{}
        \begin{itemize}
            \item Go up the path in yellow and update the values of $n(a)$ and $\hat{q}(s,a)$ for OUR agent only (i.e. the upper triangle)
        \end{itemize}
    \end{enumerate}
\end{algo}

\begin{warning}
    \begin{itemize}
        \item Works for more than 2 agents. 
        \item Don't need to know anyone else's reward function. 
        \item Has to be turn taking but can be not alternating (i.e. immediate switch between agents)
        \item Can augment simultaneous actions 
        \item Communication 
        \item Works fo rnon-zero sum games. 
    \end{itemize}
\end{warning}
\newpage

\subsection{Examples}
\subsubsection{Monte-Carlo Tree Search (MCTS) Algorithm}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Consider a simplified two-player turn-based game tree. You are currently at the root node $S_0$, which has three possible actions $a_1, a_2, a_3$. The current statistics of its children are as follows:
        \vspace{1em}
        \begin{center}
        \begin{tabular}{ccc}
        \toprule
        \textbf{Action} & $N(s_0, a)$ & $\bar{X}(s_0, a)$ \\
        \midrule
        $a_1$ & 10 & 0.6 \\
        $a_2$ & 5 & 0.8 \\
        $a_3$ & 0 & -- \\
        \bottomrule
        \end{tabular}
        \end{center}
        \vspace{1em}
        \begin{itemize}
            \item $N(s_0, a)$: Number of times action $a$ has been selected at state $s_0$
            \item $\bar{X}(s_0, a)$: Average reward obtained from action $a$ at state $s_0$
            \item $\text{UCB} = \bar{X}(s_0, a) + \sqrt{\frac{\ln(t)}{N(s_0, a)}}$ 
            \begin{itemize}
                \item $t$: Total number of actions taken at $s_0$
            \end{itemize}
        \end{itemize}
        \item \textbf{Problems:} 
        \begin{itemize}
            \item If we were to use the UCB algorithm, which nodes get selected during the selection phase? Which node gets expanded during the expansion phase?
            \item Suppose from the expanded node, simulation is performed until termination. A reward of $+1$ is obtained. Update the statistics at $s_0$ accordingly.
            \item Then, repeat the question, assuming a reward of $-1$ is attained after the simulation phase.
        \end{itemize}
        \item \textbf{Solution:}
        \begin{enumerate}
            \item \textbf{Selection 1:} $s_0$ since we traverse until a node has unexplored children (i.e. $s_3$ is unexplored)
            \item \textbf{Expansion 1:} $s_3$ is automatically expanded since it is the only unexplored child of $s_0$ w/ $N(s_0, a_3) = 1$ and $\bar{X}(s_0, a_3) = 0$
            \item \textbf{Simulation 1:} Get a reward of $+1$ 
            \item \textbf{Back Propogation 1:} For this edge from $s_0$ to $s_3$, we update the statistics as follows:
            \begin{itemize}
                \item $N(s_0, a_3) = 1$
                \item $\bar{X}(s_0, a_3) = \frac{1}{1} = 1$
            \end{itemize}
            \item \textbf{Selection 2:} $s_0$ and choose the action with the highest UCB value for $s_1$, $s_2$, and $s_3$:
            \begin{itemize}
                \item $UCB(s_0, a_1) = 0.6 + \sqrt{\frac{\ln(16)}{10}} = 1.13$ 
                \item $UCB(s_0, a_2) = 0.8 + \sqrt{\frac{\ln(16)}{5}} = 1.54$ 
                \item $UCB(s_0, a_3) = 1 + \sqrt{\frac{\ln(16)}{1}} = 2.67$. Therefore, choose $s_3$ as part of the selection phase and assume it has unexplored children.
            \end{itemize}
            \item \textbf{Expansion 2:} Not enough info but assume we expand an unexplored child. 
            \item \textbf{Simulation 2:} Get a reward of $-1$
            \item \textbf{Back Propogation 2:} For this edge from $s_0$ to $s_3$, we update the statistics as follows:
            \begin{itemize}
                \item $N(s_0, a_3) = 2$
                \item $\bar{X}(s_0, a_3) = \frac{1 + (-1)}{2} = 0$
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Consider (partial) 2-player turn-taking game-tree in which 21 iterations of MCTS have already been performed: 
        \customFigure[0.5]{../Images/L12_11.png}{}
        \begin{itemize}
            \item Total reward: Numerator
            \item Total number of times action $a$ has been selected at state $s$: Denominator
        \end{itemize}
        \item \textbf{Problem:} If we use UCB to rank order state-action pairs, which of the following states will be chosen during the 22nd selection phase. 
        \item \textbf{Solution:}
        \begin{itemize}
            \item $\text{UCB}(AB) = 7/10 + \sqrt{\frac{\ln(21)}{10}} = 1.25$
            \begin{itemize}
                \item $\text{UCB}(BE) = 2/4 + \sqrt{\frac{\ln(10)}{4}} = 1.26$
                \item $\text{UCB}(BF) = 1/6 + \sqrt{\frac{\ln(10)}{6}} = 0.79$
            \end{itemize}
            \item $\text{UCB}(AC) = 5/8 + \sqrt{\frac{\ln(21)}{8}} = 1.24$
            \item $\text{UCB}(AD) = 0/3 + \sqrt{\frac{\ln(21)}{3}} = 1.01$
            \item Therefore, choose A,B,E by selecting the nodes with the highest UCB values.
        \end{itemize}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Consider (partial) 2-player turn-taking game-tree in which 9 iterations of MCTS have already been performed: 
        \customFigure[0.5]{../Images/L12_12.png}{}
        \begin{itemize}
            \item \textbf{Fix:} $CG$ has $0/2$ not $0/1$ and $CH$ has $0/1$ not $1/1$
        \end{itemize}
        \item \textbf{Problem:} Suppose path chosen during the 10th selection phase had the state sequence $\langle A,C,H \rangle$ (i.e. $H$ is the state expanded during the 10th expansion phase)
        \begin{itemize}
            \item The simulation phase lasts for $12$ transitions, after which a terminal state is reached. 
            \item The reward to the last turn-taker was $+4$.
            \item Find $q(A,\langle A,B \rangle)$, $q(A,\langle A,C \rangle)$, $q(C,\langle C,H \rangle)$
        \end{itemize}
        \item \textbf{Solution:}
        \begin{itemize}
            \item Assuming P1 starts at $A$, then P2 goes at $C$, then P1 goes at $H$, that means after 12 transitions (\textbf{even number}), P1 is the last turn-taker, therefore, P1 gets the reward of $+4$.
            \item \textbf{Backpropogation:}
            \begin{itemize}
                \item $N(C,\langle C,H \rangle) = 1$, $X(C,\langle C,H \rangle) = 4$ so $4/1$
                \item $N(A,\langle A,C \rangle) = 4$, $X(A,\langle A,C \rangle) = 2+4=6$ so $6/4$
                \item $q(A,\langle A,B \rangle) = 2/5 = 0.4$
                \item $q(A,\langle A,C \rangle) = 6/4 = 1.5$
                \item $q(C,\langle C,H \rangle) = 4/1 = 4$
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{example}