\begin{algo}
    \begin{enumerate}
        \item Selection: Traverse using an alternate policy until a node has unexplored children. 
        \begin{equation}
            \text{UCB}(s,a) = u(s,a) + \sqrt{\frac{\log_{10}(n)}{n(s,a)}}
        \end{equation}
        \begin{itemize}
            \item \textbf{Edge:} $(s,a)$ w/ $u(s,a)/n(s,a)$
            \begin{itemize}
                \item $u(s,a)$: Average reward obtained from action $a$ at state $s$
                \item $n(s,a)$: Number of times action $a$ has been selected at state $s$
            \end{itemize}
            \item $n$: Total number of actions taken at $s$
        \end{itemize}
        \customFigure[0.4]{../Images/L12_6.png}{}
        \begin{itemize}
            \item \textbf{Icons:}
            \begin{itemize}
                \item Our Agent (Upper Triangle): Uses UCB to choose the next node to explore
                \item Other Agent (Down Triangle): Can't control their actions, so this agent picks w/ their own heuristic.
                \item Square Boxes: Estimated values (i.e. $n$ and $\hat{q}$)
            \end{itemize}
            \item \textbf{Termination:} Ends when there is at least one action that hasn't been explored yet. In this case, two actions haven't been explored. 
            \item \textbf{Skip:} Can skip expansion and simulation if the most recently expanded node is a terminal state.
        \end{itemize}
        \item Expansion: Expand an unexplored child; initialize $n(a)$ and $\hat{q}(s,a)$.
        \customFigure[0.4]{../Images/L12_7.png}{}
        \begin{itemize}
            \item \textbf{Initialize:} $\hat{q}(s,a)$ is initialized to 0 and $n(s,a)$ is initialized to 1 b/c we've visited this node once.
            \item \textbf{Termination:} Randomly pick an unexplored action unless there is only one action left.
            \item \textbf{Skip:} Can skip simulation if the most recently expanded node is a terminal state. 
        \end{itemize}
    \end{enumerate}
\end{algo}

\begin{algo}
    \begin{enumerate}
        \item Simulation: Traverse using the random policy until a terminal node is reached. 
        \customFigure[0.4]{../Images/L12_8.png}{}
        \begin{itemize}
            \item \textbf{Overview:} Using random policy to simulate the game until a terminal state is reached (i.e. reward is obtained).
        \end{itemize}
        \item Back-propogation: Get the reward and reverse; update $n(a)$ and $\hat{q}(s,a)$.
        \customFigure[0.4]{../Images/L12_9.png}{}
        \begin{itemize}
            \item \textbf{Two Player:} Go up the path in yellow and update the values of $n(s,a)$ and $\hat{q}(s,a)$ for OUR agent only (i.e. the upper triangle) except the simulation edges.
            \item \textbf{One Player:} Go up the path in yellow and update the values of $n(s,a)$ and $\hat{q}(s,a)$ for everything except the simulation edges.
            \item \textbf{Key:} Do not update the edges that were obtained from simulation due to it being a random policy. Only update the edges from selection and expansion.
        \end{itemize}
    \end{enumerate}
\end{algo}

% \begin{warning}
%     \begin{itemize}
%         \item Works for more than 2 agents. 
%         \item Don't need to know anyone else's reward function. 
%         \item Has to be turn taking but can be not alternating (i.e. immediate switch between agents)
%         \item Can augment simultaneous actions 
%         \item Communication 
%         \item Works fo rnon-zero sum games. 
%     \end{itemize}
% \end{warning}

\subsection{1 Player vs. 2 Player Turn-Taking Game Tree}
\begin{notes}
    \begin{itemize}
        \item 1 Player: 
        \begin{itemize}
            \item Don't have to worry about who gets the reward at the end for simulation. Always P1
            \item Selection: Have to calculate UCB for all edges. 
            \item Backpropogation: Keep track of $n(s,a)$ and $\hat{q}(s,a)$ for all edges. 
        \end{itemize}
        \item 2 Player: 
        \begin{itemize}
            \item Have to worry about who gets the reward at the end for simulation. 
            \begin{itemize}
                \item Even number of transitions for simulation: P1 gets the reward
                \item Odd number of transitions for simulation: P2 gets the reward
            \end{itemize}
            \item Selection: Have to calculate UCB for our agent only and given the other agent's policy. 
            \item Backpropogation: Don't keep track of $n(s,a)$ and $\hat{q}(s,a)$ for the other agent. 
        \end{itemize}
    \end{itemize}
\end{notes}
\newpage

\subsection{Examples}
\subsubsection{1 Player Turn-Taking Game Tree}
\newpage


\subsubsection{2 Player Turn-Taking Game Tree}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Consider a simplified two-player turn-based game tree. You are currently at the root node $S_0$, which has three possible actions $a_1, a_2, a_3$. The current statistics of its children are as follows:
        \vspace{1em}
        \begin{center}
        \begin{tabular}{ccc}
        \toprule
        \textbf{Action} & $N(s_0, a)$ & $\bar{X}(s_0, a)$ \\
        \midrule
        $a_1$ & 10 & 0.6 \\
        $a_2$ & 5 & 0.8 \\
        $a_3$ & 0 & -- \\
        \bottomrule
        \end{tabular}
        \end{center}
        \vspace{1em}
        \begin{itemize}
            \item $N(s_0, a)$: Number of times action $a$ has been selected at state $s_0$
            \item $\bar{X}(s_0, a)$: Average reward obtained from action $a$ at state $s_0$
            \item $\text{UCB} = \bar{X}(s_0, a) + \sqrt{\frac{\ln(t)}{N(s_0, a)}}$ 
            \begin{itemize}
                \item $t$: Total number of actions taken at $s_0$
            \end{itemize}
        \end{itemize}
        \item \textbf{Problems:} 
        \begin{itemize}
            \item If we were to use the UCB algorithm, which nodes get selected during the selection phase? Which node gets expanded during the expansion phase?
            \item Suppose from the expanded node, simulation is performed until termination. A reward of $+1$ is obtained. Update the statistics at $s_0$ accordingly.
            \item Then, repeat the question, assuming a reward of $-1$ is attained after the simulation phase.
        \end{itemize}
        \item \textbf{Solution:}
        \begin{enumerate}
            \item \textbf{Selection 1:} $s_0$ since we traverse until a node has unexplored children (i.e. $s_3$ is unexplored)
            \item \textbf{Expansion 1:} $s_3$ is automatically expanded since it is the only unexplored child of $s_0$ w/ $N(s_0, a_3) = 1$ and $\bar{X}(s_0, a_3) = 0$
            \item \textbf{Simulation 1:} Get a reward of $+1$ 
            \item \textbf{Back Propogation 1:} For this edge from $s_0$ to $s_3$, we update the statistics as follows:
            \begin{itemize}
                \item $N(s_0, a_3) = 1$
                \item $\bar{X}(s_0, a_3) = \frac{1}{1} = 1$
            \end{itemize}
            \item \textbf{Selection 2:} $s_0$ and choose the action with the highest UCB value for $s_1$, $s_2$, and $s_3$:
            \begin{itemize}
                \item $UCB(s_0, a_1) = 0.6 + \sqrt{\frac{\ln(16)}{10}} = 1.13$ 
                \item $UCB(s_0, a_2) = 0.8 + \sqrt{\frac{\ln(16)}{5}} = 1.54$ 
                \item $UCB(s_0, a_3) = 1 + \sqrt{\frac{\ln(16)}{1}} = 2.67$. Therefore, choose $s_3$ as part of the selection phase and assume it has unexplored children.
            \end{itemize}
            \item \textbf{Expansion 2:} Not enough info but assume we expand an unexplored child. 
            \item \textbf{Simulation 2:} Get a reward of $-1$
            \item \textbf{Back Propogation 2:} For this edge from $s_0$ to $s_3$, we update the statistics as follows:
            \begin{itemize}
                \item $N(s_0, a_3) = 2$
                \item $\bar{X}(s_0, a_3) = \frac{1 + (-1)}{2} = 0$
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Consider (partial) 2-player turn-taking game-tree in which 21 iterations of MCTS have already been performed: 
        \customFigure[0.5]{../Images/L12_11.png}{}
        \begin{itemize}
            \item Total reward: Numerator
            \item Total number of times action $a$ has been selected at state $s$: Denominator
        \end{itemize}
        \item \textbf{Problem:} If we use UCB to rank order state-action pairs, which of the following states will be chosen during the 22nd selection phase. 
        \item \textbf{Solution:}
        \begin{itemize}
            \item $\text{UCB}(AB) = 7/10 + \sqrt{\frac{\ln(21)}{10}} = 1.25$
            \begin{itemize}
                \item $\text{UCB}(BE) = 2/4 + \sqrt{\frac{\ln(10)}{4}} = 1.26$
                \item $\text{UCB}(BF) = 1/6 + \sqrt{\frac{\ln(10)}{6}} = 0.79$
            \end{itemize}
            \item $\text{UCB}(AC) = 5/8 + \sqrt{\frac{\ln(21)}{8}} = 1.24$
            \item $\text{UCB}(AD) = 0/3 + \sqrt{\frac{\ln(21)}{3}} = 1.01$
            \item Therefore, choose A,B,E by selecting the nodes with the highest UCB values.
        \end{itemize}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Consider (partial) 2-player turn-taking game-tree in which 9 iterations of MCTS have already been performed: 
        \customFigure[0.5]{../Images/L12_12.png}{}
        \begin{itemize}
            \item \textbf{Fix:} $CG$ has $0/2$ not $0/1$ and $CH$ has $0/1$ not $1/1$
        \end{itemize}
        \item \textbf{Problem:} Suppose path chosen during the 10th selection phase had the state sequence $\langle A,C,H \rangle$ (i.e. $H$ is the state expanded during the 10th expansion phase)
        \begin{itemize}
            \item The simulation phase lasts for $12$ transitions, after which a terminal state is reached. 
            \item The reward to the last turn-taker was $+4$.
            \item Find $q(A,\langle A,B \rangle)$, $q(A,\langle A,C \rangle)$, $q(C,\langle C,H \rangle)$
        \end{itemize}
        \item \textbf{Solution:}
        \begin{itemize}
            \item Assuming P1 starts at $A$, then P2 goes at $C$, then P1 goes at $H$, that means after 12 transitions (\textbf{even number}), P1 is the last turn-taker, therefore, P1 gets the reward of $+4$.
            \item \textbf{Backpropogation:}
            \begin{itemize}
                \item $N(C,\langle C,H \rangle) = 1$, $X(C,\langle C,H \rangle) = 4$ so $4/1$
                \item $N(A,\langle A,C \rangle) = 4$, $X(A,\langle A,C \rangle) = 2+4=6$ so $6/4$
                \item $q(A,\langle A,B \rangle) = 2/5 = 0.4$
                \item $q(A,\langle A,C \rangle) = 6/4 = 1.5$
                \item $q(C,\langle C,H \rangle) = 4/1 = 4$
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{example}