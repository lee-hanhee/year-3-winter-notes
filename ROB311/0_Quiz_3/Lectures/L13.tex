\subsection{Estimating the Optimal Quality Function}
\begin{motivation}
    The agent need not know the model of the environment. However, it must actually make moves, even when learning. 
    \vspace{1em}

    If the agent doesn't have a model, it must estimate $q^*$, $\mathcal{A}^*$, and $\pi^*$. 
\end{motivation}

\begin{definition}
    When the environment is in state $s$, the agent can take an action $a$ and: 
    \begin{itemize}
        \item \textbf{Update $\hat{q}$:} $\hat{q}(s,a; t) \leftarrow (1-\alpha) \hat{q}(s,a;t) + \alpha \left( r' + \gamma \max_{a'} \hat{q}(s',a';t+1) \right)$
        \begin{itemize}
            \item $0 \leq \alpha \leq 1$: learning rate
        \end{itemize}
        \item \textbf{Compute $\hat{\mathcal{A}}$:} $\hat{\mathcal{A}}(s;t) = \arg\max_{a'\in \mathcal{A}(s)} \hat{q}(s,a';t)$
        \item \textbf{Compute $\hat{\pi}$:} $\hat{\pi}(a' \mid s;t) = 0 \; \forall a' \notin \hat{\mathcal{A}}(s;t)$
    \end{itemize}
\end{definition}

\subsection{Exploration versus Exploitation}
\begin{motivation}
    To ensure $\hat{q}$ converges to $q^*$ and the agent's expected return is maximized, the agent must balance exploration and exploitation.    
\end{motivation}

\begin{definition}
    \begin{itemize}
        \item \textbf{Exploitation:} Choose the most promising actions based on current knowledge. 
        \begin{itemize}
            \item Use optimal policy: $\hat{\pi}(\cdot,\cdot;t)$
        \end{itemize}
        \item \textbf{Exploration:} Choose the least tried actions to improve current knowledge.
        \begin{itemize}
            \item Choose actions randomly 
        \end{itemize}
    \end{itemize}
\end{definition}

\subsubsection{Simplified Case:}
\begin{example}
    \begin{itemize}
        \item \textbf{Given:} Assume the environment is stateless, but rewards are random.
        
        \customFigure[0.5]{../Images/L11_1.png}{}
        \vspace{-1em}
        \customFigure[0.5]{../Images/L11_2.png}{}
        \begin{itemize}
            \item $\mu(a)$: expected reward for action $a$ (unknown to the agent):
            \item $0 \leq \mu(a) \leq 1 \text{ for all } a.$
        \end{itemize}
        
        \item \textbf{Best-case expected return:} (with $\gamma = 1$ under $\pi^*$) from transition $t$ is:
        
        \[
        u^*(t) := (T - t) \max_{a'} \mu(a')
        \]
        
        where in this case:
        
        \[
        \pi^*(a; t) = 0 \quad \text{if } a \not\in \arg\max_{a'} \mu(a').
        \]
        
        \item \textbf{Estimation of $\mu(\cdot)$.} Since the agent does not have a model, it must estimate $\mu(\cdot)$.
        \vspace{1em}
        
        The agent can take an action $a$ and:
        
        \begin{enumerate}
            \item \textbf{Update} $n(\cdot)$ and $\hat{\mu}(\cdot)$:
        
            \[
            n(a) \leftarrow n(a) + 1
            \]
            
            \[
            \hat{\mu}(a) \leftarrow \left(1 - \frac{1}{n(a)}\right) \hat{\mu}(a) + \frac{1}{n(a)} r'
            \]
            
            \item \textbf{Compute} $\hat{\pi}$:
            
            \[
            \hat{\pi}(a; t) = 0 \text{ for all } a \not\in \arg\max_{a'} \hat{\mu}(a').
            \]
        \end{enumerate}
        \item  \textbf{Alternate Policies} We want to compare the expected return under various policies. The expected return from transition $t$ under a policy $\rho$ is:
        
        \[
        u^{\rho}(t) := \mathbb{E}^{\pi}[G_t] = \sum_{a'} \rho(a'; t) \left(\mu(a') + u^{\rho}(t+1)\right).
        \]
    \end{itemize}
\end{example}
\newpage

\subsection{Alternate Policies}
\begin{summary}
    To ensure the agent's expected return is maximized, the agent must strike still strike a balance exploration and exploitation. 
    \vspace{1em}

    In the following cases, the expected return from transition $t$ is 
    \begin{equation*}
        u^{\text{avg}} (t) \equiv \frac{T-t}{|\mathcal{A}|} \sum_{a} \mu(a)
    \end{equation*}

    We want to choose $\rho$ so that $u^\rho > u^{\text{avg}}$. 

    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Policy} & \textbf{Function:} \\
            \midrule
            Exploitation only & Choose a random action, same for all transitions \\
            \midrule
            Exploration only & Choose a random action, different for each transition \\ 
            \midrule
            Softmax & Apply a soft-max over $\hat{u}$ \\
            & $\rho(a;t) = \left[\sum_{a'} \text{exp}\left(\frac{\hat{\mu}(a')}{\tau}\right)\right]^{-1} \text{exp}\left(\frac{\hat{\mu}(a)}{\tau}\right)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Choose a temperature value decrease with $t$. 
                \item $\tau(t) \in [0,\infty), \tau \rightarrow 0$
            \end{itemize}} \\
            \midrule
            $\epsilon$-greedy & Use $\hat{\pi}$ w/ prob. $1-\epsilon$, otherwaise take a random action \\
            & $\rho(a;t) = \epsilon \frac{1}{|\mathcal{A}|} + (1-\epsilon) \hat{\pi}(a;t)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Choose an exploration rate decrease w/ $t$. 
                \item $\epsilon(t) \in [0,1], \epsilon \rightarrow 0$
            \end{itemize}} \\
            \midrule
            Upper confidence bound & Choose the action with the highest $\text{ucb}(\cdot)$\\ 
            & $\rho(a;t) = 0$ if $a \notin \arg \max_{a'} \text{ucb}(a';t)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item Compute $\text{ucb}(\cdot)$ for each action. 
                \item $\text{ucb}(a;t) = \hat{\mu}(a) + \sqrt{\frac{\ln t}{n(a)}}$
            \end{itemize}} \\
            \bottomrule            
        \end{tabular}
    \end{center}

    \customFigure[0.5]{../Images/L11_0.png}{}
\end{summary}

\subsection{Examples}
\begin{example}
    \begin{enumerate}

        \item \textbf{Problem 1:}
    
        Consider the policy:
        \[
        \pi(a|s) = 
        \begin{cases}
        0.99 & \text{if } a = \arg\max_{a'} q(s, a') \\
        0.01 & \text{otherwise}
        \end{cases}
        \]
    
        Which of the following best describes its behaviour?
        \begin{itemize}
            \item (A) It mostly exploits
            \item (B) It mostly explores
            \item (C) It mostly explores initially, but mostly exploits in the long-run
            \item (D) It mostly exploits initially, but mostly explores in the long-run
            \item (E) None of the above
        \end{itemize}
    
        \item \textbf{Solution 1:}
    
        \textbf{Correct answer: (A)} It mostly exploits.  
        \textit{Explanation:} The policy heavily favors the action with the highest Q-value (with 99% probability), showing a strong bias toward exploitation.
    
        \item \textbf{Problem 2:}
    
        Consider the policy:
        \[
        \pi(a|s) = \frac{\exp\left(\frac{q(s,a)}{\tau}\right)}{\sum\limits_{a' \in \mathcal{A}(s)} \exp\left(\frac{q(s,a')}{\tau}\right)}
        \]
        where $\tau$ decreases each episode.
    
        Which of the following best describes its behaviour?
        \begin{itemize}
            \item (A) It mostly exploits
            \item (B) It mostly explores
            \item (C) It mostly explores initially, but mostly exploits in the long-run
            \item (D) It mostly exploits initially, but mostly explores in the long-run
            \item (E) None of the above
        \end{itemize}
    
        \item \textbf{Solution 2:}
    
        \textbf{Correct answer: (C)} It mostly explores initially, but mostly exploits in the long-run.  
        \textit{Explanation:} The softmax policy allows for exploration when $\tau$ is high. As $\tau \rightarrow 0$, it becomes increasingly greedy, converging to exploitation.
    
        \item \textbf{Problem 3:}
    
        Consider the policy:
        \[
        \pi(a|s) = 
        \begin{cases}
        1 & \text{if } a = \arg\max_{a'} f(s, a') \\
        0 & \text{otherwise}
        \end{cases}
        \]
        where
        \[
        f(s,a) = 
        \begin{cases}
        q(s,a) + C \sqrt{\frac{\log(\tau)}{N(s,a)}} & \text{if } N(s,a) > 0 \\
        \infty & \text{else}
        \end{cases}
        \]
        for some constant $C > 0$ and increasing $\tau$.
    
        Which of the following best describes its behaviour?
        \begin{itemize}
            \item (A) It mostly exploits
            \item (B) It mostly explores
            \item (C) It mostly explores initially, but mostly exploits in the long-run
            \item (D) It mostly exploits initially, but mostly explores in the long-run
            \item (E) None of the above
        \end{itemize}
    
        \item \textbf{Solution 3:}
    
        \textbf{Correct answer: (C)} It mostly explores initially, but mostly exploits in the long-run.  
        \textit{Explanation:} This is a UCB (Upper Confidence Bound) policy. The exploration bonus decreases as $N(s,a)$ increases, so the agent prioritizes exploring under-visited actions early on but exploits later.
    
        \item \textbf{Problem 4:}
    
        Consider the policy:
        \[
        \pi(a|s) = 
        \begin{cases}
        1 & \text{if } a = \arg\min_{a'} N(s, a') \\
        0 & \text{otherwise}
        \end{cases}
        \]
    
        Which of the following best describes its behaviour?
        \begin{itemize}
            \item (A) It mostly exploits
            \item (B) It mostly explores
            \item (C) It mostly explores initially, but mostly exploits in the long-run
            \item (D) It mostly exploits initially, but mostly explores in the long-run
            \item (E) None of the above
        \end{itemize}
    
        \item \textbf{Solution 4:}
    
        \textbf{Correct answer: (B)} It mostly explores.  
        \textit{Explanation:} This policy always chooses the least-visited action. It does not consider value (Q-function) and is purely driven by state-action visitation counts, thus ensuring exploration.
    
    \end{enumerate}
\end{example}