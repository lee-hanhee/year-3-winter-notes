\begin{summary}
    In a \textbf{POMDPs}, we assume that: 
    \begin{itemize}
        \item environment modelled using state space, $\mathcal{S}$
        \item single agent
        \item $S_t$ = state after transition $t$
        \item $A_t$ = action inducing transition $t$
        \item stochastic state transitions with memoryless property:
        \[
        S_T \perp S_0, A_1, \dots, A_{T-1}, S_{T-2} \mid S_{T-1}, A_T
        \]
        \item $R_t$ = reward for transition $t$, i.e., $(S_{T-1}, A_T, S_T)$
        \item $O_t$ = observation of $S_t$
        \begin{itemize}
            \item Measurement of a state (i.e. appproximation, so may not be exact)
            \item \textbf{Key:} Since actual state is unknown, so are legal actions. 
        \end{itemize}
    \end{itemize}
    \vspace{1em}

    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Name} & \textbf{Function:} \\
            \midrule
            Initial state distribution & $p_0(s) := \mathbb{P}[S_0 = s]$ \\
            \midrule
            Transition distribution & $p(s'|s,a) := \mathbb{P}[S_t = s' | A_t = a, S_{t-1} = s]$ \\
            \multicolumn{2}{p{\linewidth}}
            {
            \begin{itemize}
                \item Assume $\mathcal{A}(s) = \mathcal{A}(s') := \mathcal{A}\; \forall s, s'$ (i.e. since actual state is unknown, so are legal actions, so assume all actions are legal):
                \begin{itemize}
                    \item if $a \notin \mathcal{A}(s)$, then $p(s' | s, a) = 0$ for all $s' \neq s$
                \end{itemize}                
            \end{itemize}
            } \\
            \midrule
            Reward function & $r(s,a,s') :=$ reward for transition $(s, a, s')$ \\
            \multicolumn{2}{p{\linewidth}}
            {
            \begin{itemize}
                \item Assume $\mathcal{A}(s) = \mathcal{A}(s') := \mathcal{A}\; \forall s, s'$ (i.e. since actual state is unknown, so are legal actions, so assume all actions are legal):
                \begin{itemize}
                    \item if $a \notin \mathcal{A}(s)$, then $r(s, a, s') = 0$ for all $s'$
                \end{itemize}                
            \end{itemize}
            } \\
            \midrule
            Policy for choosing actions & $\pi_t(a | o_0, \dots, o_t) := \mathbb{P}[A_t = a | O_0 = o_0, \dots, O_t = o_t]$ \\
            \multicolumn{2}{p{\linewidth}}
            {
            \begin{itemize}
                \item Observe that policy is now time-dependent.
                \item \textbf{Special Case:} If we assume the agent cannot use past observations, $A_t \perp O_0, \dots, O_{t-1} \mid O_t,$ policy becomes time-independent,
                \[
                \pi_t(a | o_0, \dots, o_t) = \pi_0(a | o_t).
                \]
                \begin{itemize}
                    \item Only need to specify $\pi_0$.
                \end{itemize}
            \end{itemize}
            } \\
            \midrule
            Measurement model & $m(o | s) := \mathbb{P}[O_t = o | S_t = s]$ \\
            \midrule 
            Belief after $t$ observations &  $b_t(s_t | a_{1:t}, o_{0:t}) = \mathbb{P}[S_t = s_t | A_t = a_t, O_{0:t} = o_{0:t}]$ \\
            & $b_t(s_t | a_{1:t}, o_{0:t}) = m(o_t | s_t) \sum_{s_{t-1}} p(s_t | s_{t-1}, a_t) b_{t-1}(s_{t-1} | a_{1:t-1}, o_{0:t-1})$ \\
            \multicolumn{2}{p{\linewidth}}
            {
            \begin{itemize}
                \item $b_t$: Probability distribution
                \item $b_0(s_0) = \mathbb{P}[S_0 = s_0]$: Initial belief distribution
                \item Only holds for $t \geq 1$.
                \begin{itemize}
                    \item @$t$: Measurement before and after action for the belief is the same except at $t=0$ b/c of initial belief. 
                \end{itemize}
                \item For $t = 0$ (assuming uniform prior): $b_0(s_0 | o_0) = \frac{m(o_0 | s_0)}{\sum_s m(o_0 | s)}.$
            \end{itemize}
            } \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage 

\subsection{Bayesian Network}
\begin{notes}
    $S_0, O_0, A_1, R_1, S_1, O_1, A_2, R_2, S_2, O_2, \dots$ form a Bayesian network:
    \customFigure[0.5]{../Images/L10_0.png}{}
    \begin{itemize}
        \item Assuming $A_t \perp O_0, \dots, O_{t-1} \mid O_t$. WHERE DOES THIS COME INTO PLAY. 
    \end{itemize}
\end{notes}

\subsection{Belief (Probability Distribution) Over the States:}
\begin{notes} Assume actual state is the most likely state. 
    \customFigure[0.5]{../Images/L10_5.png}{}
    \begin{itemize}
        \item Usually assume uniform distribution before you observe anything. 
        \item \textbf{Flow:} Measurement $\rightarrow$ Take action $\rightarrow$ Update belief $\rightarrow$ Take action. 
    \end{itemize}
\end{notes}
\newpage

\subsection{Examples}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:}
        \begin{itemize}
            \item Now suppose Cavemen wants to feed child: 
            \begin{itemize}
                \item Cannot know satiety of child exactly. 
                \item Whether apple is edible or not must be inferred from senses. 
            \end{itemize}
            \item Possible obsevations for the apple: 
            \customFigure[0.5]{../Images/L10_1.png}{}
            \item Possible states for the child's satiety:
            \customFigure[0.5]{../Images/L10_2.png}{}
            \item Measurement distribution for the apple: 
            \customFigure[0.5]{../Images/L10_4.png}{$m(o_1 | s) = P(o_1 | s)$}
            \begin{itemize}
                \item $\sum = 1$ across the rows
                \item What is the probability of observing a certain state of the apple given the true state?
            \end{itemize}
            \item Measurement distribution for child's satiety:
            \customFigure[0.5]{../Images/L10_3.png}{$m(o_2 | s) = P(o_2 | s)$}
            \begin{itemize}
                \item $\sum = 1$ across the rows
                \item What is the probability of observing a certain state of the child given the true state?
            \end{itemize}
            \item \textbf{Key:} Assume independence between the observations of the child's satiety and the apple's edibility: $P(o | s) = P(o_1 | s) \cdot P(o_2 | s)$.
        \end{itemize}
        \item \textbf{Problem}
        \begin{itemize}
            \item Initial distribution, $b_0(s_0)$ over states is uniform. 
            \item Action sequence is $\langle a_1, a_2, a_3 \rangle = \langle \text{seed}, \text{wait}, \text{wait} \rangle$.
            \item Observation sequence is $\langle o_0, o_1, o_2, o_3 \rangle = \langle \left(\text{:(,no apple}\right), \left(\text{:(,ga)}\right), \left(\text{:(,ra)}\right), \left(\text{:|,ra)}\right) \rangle$.
            \item Find state distribution: $b_3 (s_3 \mid a_{1:3}, o_{0:3})$.
        \end{itemize}
        \customFigure[0.6]{../Images/L10_6.png}{}
        \item \textbf{Solution:}
    \end{enumerate}
\end{example}
\newpage

\begin{example}
    \begin{equation}
        b_0(s_0 \mid o_0) = \frac{m(o_0 \mid s_0)}{\sum_s m(o_0 \mid s)}
    \end{equation}
    \begin{equation}
        b_t(s_t \mid a_{1:t},o_{0:t}) = m(o_t \mid s_t) \sum_{s_{t-1}} p(s_t \mid s_{t-1}, a_t) b_{t-1}(s_{t-1} \mid a_{1:t-1}, o_{0:t-1})
    \end{equation}
    \begin{center}
        \begin{tabular}{cccccc}
            \toprule 
            s & $b_0(s)$ & $b_0(s \mid o_0)$ & $b_1(s \mid o_{0:1}, a_1)$ & $b_2(s \mid o_{0:2}, a_{1:2})$ & $b_3(s \mid o_{0:3}, a_{1:3})$ \\
            \midrule
            No meat & $1/6$ & $0.4545$ & & & \\
            \multicolumn{6}{p{\linewidth}}
            {
            \begin{itemize}
                \item $\sum_s m(\left(\text{:(,no apple}\right) \mid s) = (1.0)(0.8) + (0.2)(0.8) + (0.0)(0.8) + (0.0)(0.8) + (1.0)(0.8) + (1.0)(0.0) = 1.76$
                \item $b_0(\text{No meat} \mid \left(\text{:(,no apple}\right)) = \frac{(1.0)(0.8)}{1.76} = \frac{0.8}{1.76} = 0.4545$     
            \end{itemize}
            } \\
            \midrule
            Green apple & $1/6$ & $0.0909$ & & & \\
            \multicolumn{6}{p{\linewidth}}
            {
            \begin{itemize}
                \item $b_0(\text{Green apple} \mid o_0) = \frac{(0.2)(0.8)}{1.76} = \frac{0.16}{1.76} = 0.0909$  
            \end{itemize}
            } \\
            \midrule
            Red apple & $1/6$ & $0$ & & & \\
            \multicolumn{6}{p{\linewidth}}
            {
            \begin{itemize}
                \item $b_0(\text{Red apple} \mid o_0) = \frac{(0.0)(0.8)}{1.76} = \frac{0.0}{1.76} = 0$
            \end{itemize}
            } \\
            \midrule
            Rotten apple & $1/6$ & $0$ & & & \\
            \multicolumn{6}{p{\linewidth}}
            {
            \begin{itemize}
                \item $b_0(\text{Rotten apple} \mid o_0) = \frac{(0.0)(0.8)}{1.76} = \frac{0.0}{1.76} = 0$
            \end{itemize}
            } \\
            \midrule
            Meat & $1/6$ & $0.4545$ & & & \\
            \multicolumn{6}{p{\linewidth}}
            {
            \begin{itemize}
                \item $b_0(\text{Meat} \mid o_0) = \frac{(1.0)(0.8)}{1.76} = \frac{0.8}{1.76} = 0.4545$
            \end{itemize}
            } \\
            \midrule
            Dead & $1/6$ & $0$ & & & \\
            \multicolumn{6}{p{\linewidth}}
            {
            \begin{itemize}
                \item $b_0(\text{Dead} \mid o_0) = \frac{(1.0)(0.0)}{1.76} = \frac{0.0}{1.76} = 0$   
            \end{itemize}
            } \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{example}

