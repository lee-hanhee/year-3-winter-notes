\begin{notes}
    \begin{itemize}
        \item Transformers on large text-like datasets.
        \item Transformers on "tokens" (discretized data)
        \item Foundational models
    \end{itemize}
\end{notes}

\subsection{Transformers \& LLMs}
\begin{summary}
    % DO a table with all these subtopics 
\end{summary}

\subsubsection{Inputs: Tolenizing Text \& Embedding Layers}

\subsubsection{Outputs: Auto-Regressive Decoding of Tokens}

\subsubsection{Sizes of Text Datasets for LLMs}

\subsubsection{Text to Text Tasks}

\subsubsection{Transformers and Masking: Encoders and Decoders}

\subsubsection{Masking Language Modelling (Self-Supervised)}
\newpage

\subsection{Scaling LLMs}
\begin{motivation}
    
\end{motivation}

\subsubsection{Techniques}
\begin{summary}
    Table format
\end{summary}

\subsubsection{High-Level Impacts}
\begin{summary}

\end{summary}