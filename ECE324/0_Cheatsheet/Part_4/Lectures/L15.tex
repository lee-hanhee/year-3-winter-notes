\begin{notes}
    \begin{itemize}
        \item Transformers on large text-like datasets.
        \item Transformers on "tokens" (discretized data)
        \item Foundational models
    \end{itemize}
\end{notes}

\subsection{Transformers \& LLMs}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Concept} & \textbf{Description} \\
            \midrule
            Sizes of text datasets & Scale significantly impacts LLM's performance. \\
            \midrule
            Text to Text Tasks & Many tasks can be framed as text-to-text problems. \\
            \midrule
            Transformers \& Masking: Encoders \& Decoders & \\
            \midrule 
            Masked Language Modelling & Predicting masked tokens in a sequence. \\
            \midrule
            Inputs & Tokenizing Text \& Embedding Layers \\
            \midrule
            Outputs & Auto-Regressive Decoding of Tokens \\
            & Decoding one token at a time, using previous outputs \\
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Scaling LLMs}
\begin{motivation}
    
\end{motivation}

\subsubsection{Techniques}
\begin{summary}
    Table format
\end{summary}

\subsubsection{High-Level Impacts}
\begin{summary}

\end{summary}