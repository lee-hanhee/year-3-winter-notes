\begin{motivation}
    One of the core mechanisms inside of current LLMs. 
\end{motivation}

\begin{warning}
    Convolution takes in a single node, while attention takes in all nodes. 
\end{warning}

\subsection{Transformer}
\begin{notes}
    \customFigure[0.5]{../../Images/L14_3.png}{}
    \begin{itemize}
        \item \textbf{Transformer Layer:} 
        \begin{itemize}
            \item Attention mechanism (multi-headed)
            \item Positional encodings
        \end{itemize}
        \item With massive unsupervised datasets: 
        \begin{itemize}
            \item Masked self-supervised training
            \item Contrastive training
        \end{itemize}
    \end{itemize}
\end{notes}
\newpage

\subsubsection{Transformer Layer}
\begin{summary}
    \customFigure[0.3]{../../Images/L14_4.png}{}
    \vspace{-1em}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Component} & \textbf{Description} \\
            \midrule
            Positional encoding & Learn to map integer positions into a vectorized representation. \\
            & $\text{PE}_{(\text{pos},i)} = 
            \begin{cases}
                \sin\left(\frac{\text{pos}}{10000^{\frac{i}{d_{model}}}}\right) & \text{if } i \text{ is even} \\
                \cos\left(\frac{\text{pos}}{10000^{\frac{i-1}{d_{model}}}}\right) & \text{if } i \text{ is odd}
            \end{cases}$ \\
            \midrule
            Multi-Head Attention & Computes attention scores for each token in the sequence. \\
            \midrule
            LayerNorm & Stabilizes activations and accelerates training. \\
            \midrule
            Residual Connection & Preserve information and enable deeper networks. \\
            \midrule
            FFN/MLP & Increases the expressive power of the learned representation, often using GELU activations. \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}

\subsection{Transformers are GNNs}
\begin{summary} Transformers are a special case of GNN
    \begin{center}
        \begin{tabular}{lll}
            \toprule
            \textbf{} & \textbf{GNN} & \textbf{Transformer} \\
            \midrule
            \textbf{Connectivity (Adjacency)} & Sparse & Full \\
            \textbf{Edge Learning} & Yes & No (Implicitly) \\
            \textbf{Message Computation} & $M(n_i, n_j, e_{ij})$ & $\langle n_i, n_j \rangle$ \\
            \textbf{Communication per step} & \# Number of Neighboring nodes & $\sim$ \# Number of Heads \\
            \textbf{Data requirements} & Low & High \\
            \textbf{Computation} & Slow due to gather operations & Fast, Optimizable $\sim$ Matrix Multiplications \\
            \textbf{Training} & Straightforward & Pre-training is needed \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Attention Mechanism}
\begin{process}
    \begin{enumerate}
        \item \textbf{Inputs:} Tokens tensor, Mask
        \begin{itemize}
            \item \textbf{Tokens:} Inputs for Transformer/Attention Layers, which is a numerical representation of pieces of data.
            \item \textbf{Mask:} A binary matrix that indicates which tokens to give attention to.
        \end{itemize}
        \item \textbf{Preprocessing:} Linear maps Tokens into Queries, Keys, and Values.
        \begin{itemize}
            \item $Q = \text{Tokens} \cdot W_Q$: Represents the current token's context.
            \item $K = \text{Tokens} \cdot W_K$: Represents the context of all tokens.
            \item $V = \text{Tokens} \cdot W_V$: Represents the information to be passed on.
        \end{itemize}
        \item \textbf{Attention scores:} $\text{Scores} = \frac{QK^T}{\sqrt{d_k}} \cdot \text{Mask}$
        \begin{itemize}
            \item $\text{score}_{ij} = \frac{(q_i \cdot k_j) m_{ij}}{\sqrt{d_k}}$
        \end{itemize}
        \item \textbf{Attention Normalization:} $\text{Attention Weights} = \text{softmax}(\text{scores})$
        \begin{itemize}
            \item $\text{score}_{ij}^{\text{normalized}} = \frac{\text{exp}(\text{score}_{ij})}{\sum_{k=1}^n \text{exp}{(\text{score}_{ik})}}$
        \end{itemize}
        \item \textbf{Value update:} $\text{New Values} = \text{Attention Weights} \cdot V$
        \begin{itemize}
            \item $v_i^{\text{new}} = \sum_{j=1}^n \text{score}_{ij}^{\text{normalized}} v_j$
        \end{itemize}
        \item \textbf{Post Processing:} Apply LayerNorm, Residual connections, and a FFN.
        \item \textbf{Outputs:} Updated tokens tensor
    \end{enumerate}
    \begin{equation}
        \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}\cdot \text{Mask}}\right)V
    \end{equation}

\end{process}

\subsubsection{Attention Maps: Visualizing Where the Model Attends}
\begin{notes} Softmax bias:
    \begin{itemize}
        \item Values between 0 and 1
        \item Categorical like
        \item Attend to one token at a time
    \end{itemize}
    \customFigure[0.3]{../../Images/L14_6.png}{}
\end{notes}
\newpage

\subsubsection{Self-Attention vs. Cross-Attention}
\begin{notes}
    \begin{itemize}
        \item \textbf{Self-Attention:} Attention allows connections between the same sequence without masking. 
        \begin{equation*}
            \text{Self-Attention}(x) = \text{Attention}(Q(x), K(x), V(x))
        \end{equation*}
        \item \textbf{Cross-Attention:} Attention allows connections between different sequences.
        \begin{equation*}
            \text{Cross-Attention}(x, y) = \text{Attention}(Q(x), K(y), V(x))
        \end{equation*}
    \end{itemize}
\end{notes}

\subsubsection{Multi-Headed Attention}
\begin{notes}
    Multiple attention mechanisms in parallel, each with different linear maps.
    \begin{itemize}
        \item Ensemble-like approach. 
        \item Same compute and \# of parameters. 
        \item \textbf{Strategy:} Expand and contract tensors to new axis: number of heads. 
    \end{itemize}
    \customFigure[0.5]{../../Images/L14_8.png}{}
\end{notes}
\newpage

\subsection{Examples}
\subsubsection{Tokens}
\begin{example}
    \customFigure[0.5]{../../Images/L14_0.png}{}
\end{example}

\subsubsection{Positional Encoding}
\begin{example}
    \customFigure[0.75]{../../Images/L14_1.png}{}
\end{example}
\newpage

\subsubsection{Cross-Attention}
\begin{example}
    \customFigure[0.5]{../../Images/L14_7.png}{}
\end{example}
