\begin{motivation}

\end{motivation}

\subsection{Tokens: Inputs for Transformer/Attention Layers}
\begin{definition}
    
\end{definition}

\subsubsection{Positional Encoding}
\begin{definition}
    
\end{definition}

\subsection{Attention}
\begin{process}
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{process}

\begin{notes}
    
\end{notes}

\subsubsection{Residuals, Norms, and FFN}
\begin{notes}
    \begin{itemize}
        \item 
    \end{itemize}
\end{notes}

\subsubsection{Self-Attention vs. Cross-Attention}
\begin{notes}
    
\end{notes}

\subsubsection{Multi-Head Attention}

\subsection{Transformers}
\begin{notes}

\end{notes}

\subsubsection{Transformer Block}
\begin{notes}
    
\end{notes}

\subsubsection{Transformers are GNNs}
\begin{summary} Transformers are a special case of GNN
    % Insert table
\end{summary}

\subsection{Examples}
\subsubsection{Tokens}
\begin{example}
    
\end{example}
