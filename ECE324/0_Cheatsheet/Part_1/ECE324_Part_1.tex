\documentclass{article}
\usepackage{style}
\title{ECE324 Part 1}
\author{Hanhee Lee}
\lhead{ECE324}
\rhead{Hanhee Lee}

\begin{document}

\begin{itemize}
    \item Problem 1: Description of the Problem, Subscribe a ML/NN Based Solution
    \item Problem 2: Prescribe a strategy to optimize the NN.
    \item Problem 3: Explain step by step inference for basic algorithms (MLP, CNN, GNN, attention mechanism) in terms of numpy or basic tensor operations.
    \item Problem 4: Be able to explain why such a solution might work or fail.
\end{itemize}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
        \toprule
        \textbf{Model} & \textbf{Description} \\
        \midrule
        \textbf{Linear Regression} & Predicts a continuous target variable based on weighted input features. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When the relationship between input and output is approximately linear and interpretability is important.
        \end{itemize}} \\
        \midrule
        \textbf{Logistic Regression} & Predicts the probability of a binary outcome using a sigmoid function. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When you need a simple and interpretable classifier for binary classification tasks.
        \end{itemize}} \\
        \midrule
        \textbf{Decision Trees} & Recursively splits data based on feature thresholds to make predictions. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When interpretability and handling non-linear relationships are required, even with small datasets.
        \end{itemize}} \\
        \midrule
        \textbf{Random Forest} & Ensemble of decision trees that averages multiple predictions. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When you need a robust, high-accuracy model that mitigates overfitting and can handle missing data.
        \end{itemize}} \\
        \midrule
        \textbf{Gradient Boosting} & Ensemble method that sequentially improves weak learners to min. errors. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When optimizing for high accuracy in structured/tabular data with feature importance interpretation.
        \end{itemize}} \\
        \midrule
        \textbf{SVM} & Finds the optimal hyperplane to separate classes using kernel methods. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When working with small-to-medium datasets with complex decision boundaries and high dimensionality.
        \end{itemize}} \\
        \midrule
        \textbf{K-Nearest Neighbors} & A non-parametric model that classifies based on the majority class of the KNN. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When working with small datasets where decision boundaries are unclear and computational cost is not a concern.
        \end{itemize}} \\
        \midrule
        \textbf{Naïve Bayes} & A probabilistic classifier that assumes feature independence using Bayes’ Theorem. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When dealing with text classification (e.g., spam detection) or highly independent features.
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
        \toprule
        \textbf{Model} & \textbf{Description} \\
        \midrule
        \textbf{MLP} & Fully-connected layers \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When handling large, complex datasets, particularly in pattern recognition and function approximation.
        \end{itemize}} \\
        \midrule
        \textbf{CNN} & A deep learning model specialized in detecting spatial features in image data. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When working with computer vision tasks such as image classification and object detection.
        \end{itemize}} \\
        \midrule
        \textbf{RNN} & A deep learning model designed for sequential data with recurrent connections. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When processing time-series, speech, or natural language data where past information matters.
        \end{itemize}} \\
        \midrule
        \textbf{Transformer Models} & Advanced neural networks designed for parallelized sequence processing in NLP. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When working with NLP tasks such as text generation, sentiment analysis, and translation.
        \end{itemize}} \\
        \midrule
        \textbf{K-Means Clustering} & An unsupervised algorithm that partitions data into k clusters. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When segmenting data into groups without predefined labels, such as customer segmentation.
        \end{itemize}} \\
        \midrule
        \textbf{PCA} & A dimensionality reduction technique that projects data onto principal components. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When reducing dimensionality for visualization or improving model efficiency in high-dimensional data.
        \end{itemize}} \\
        \midrule
        \textbf{Autoencoders} & A neural network that compresses and reconstructs data to learn latent representations. \\
        \multicolumn{2}{p{\linewidth}}{
        \begin{itemize}
            \item \textbf{When to Use?} When performing anomaly detection or dimensionality reduction in unsupervised learning tasks.
        \end{itemize}} \\
        \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage


\section{ML Concepts}
\subsection{Bias-Variance Tradeoff}
\begin{definition}
    \begin{itemize}
        \item \textbf{Bias (Underfitting):} Error due to overly simplistic assumptions in the model.
        \begin{itemize}
            \item A model with high bias makes strong assumptions about the data, leading to oversimplification.
            \item This results in poor training and test performance because the model fails to capture important patterns.
        \end{itemize}
        \item \textbf{Variance (Overfitting):} Error due to the model's sensitivity to fluctuations in the training data.
        \begin{itemize}
            \item A model with high variance is too complex and captures noise in the training data, rather than just the underlying pattern.
            \item This results in excellent training performance but poor generalization to new data.
        \end{itemize}
        \item \textbf{Low Bias, High Variance:} Overly complex models (e.g., deep neural networks with excessive parameters) suffer from overfitting, performing well on training data but poorly on test data.
        \item \textbf{High Bias, Low Variance:} Overly simplistic models (e.g., linear regression) suffer from underfitting, performing poorly on both training and test data.
    \end{itemize}
\end{definition}

\subsubsection{Solution to High Variance}
\begin{definition}
    \begin{itemize}
        \item Regularization
        \item Data Augmentation
        \item More trianing data 
        \item Simpler models
        \item Ensemble methods
    \end{itemize}
\end{definition}

\subsubsection{Solution to High Bias}
\begin{definition}
    \begin{itemize}
        \item More complex models
        \item Feature engineering
        \item Hyperparameter tuning
        \item Reduce regularization
    \end{itemize}
\end{definition}
\newpage

\section{Loss Functions}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Loss Fn} & \textbf{Equation} \\ 
            \toprule
            \textbf{Mean Squared Error (MSE)} & $\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{When to Use?} Penalizes large errors.
            \end{itemize}} \\
            \midrule
            \textbf{Root Mean Squared Error (RMSE)} & $\sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{When to Use?} Error needs to be in the same units as the target.
            \end{itemize}} \\
            \midrule
            \textbf{Mean Absolute Error (MAE)} & $\frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{When to Use?} Equal weighting of all errors (robust to outliers).
            \end{itemize}} \\
            \midrule
            \textbf{Binary Cross Entropy} & $- \frac{1}{n} \sum_{i=1}^{n} y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{When to Use?} Binary classification problems.
            \end{itemize}} \\
            \midrule
            \textbf{Sum of Binary Cross Entropy} & $\frac{1}{n} \sum^{\text{labels}} \sum_n y_n \log(\hat{y}_n) + (1 - y_n) \log(1 - \hat{y}_n)$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{When to Use?} Multi-label classification problems.
            \end{itemize}} \\
            \midrule
            \textbf{Cross Entropy} & $\sum_i^{\text{classes}} y_i \log(\hat{y}_i)$ \\
            \midrule
            \textbf{AE: Reconstruction Error} & Minimize reconstruction error. \\
            & $L_{AE} = \left\| \mathbf{x} - f_{\text{dec}}(f_{\text{enc}}(\mathbf{x})) \right\|^2 = \left\| \mathbf{x} - \hat{\mathbf{x}} \right\|^2$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item If our data is binary, what loss function should you use? Binary cross entropy.
            \end{itemize}} \\
            \midrule
            \textbf{VAE: Reconstruction and KL Divergence} & Balances reconstruction and latent space regularization. \\
            &  $L_{\text{VAE}} = L_{\text{AE}} (x, \hat{x}) + \text{KL} (q_\phi (\mathbf{z} \mid \mathbf{x}) \parallel p(\mathbf{z}))$ \\
            \multicolumn{2}{p{\linewidth}}{
            \begin{itemize}
                \item For Gaussian Distribution: $KL(P \| Q) = KL\left(\mathcal{N}(\mu_1, \sigma_1^2) \| \mathcal{N}(\mu_2, \sigma_2^2)\right) = \frac{1}{2} \left[ \log\left(\frac{\sigma_2^2}{\sigma_1^2}\right) + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{\sigma_2^2} - 1 \right]$ 
            \end{itemize}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\section{Algorithms}
\begin{summary}
    \begin{center}
        \begin{tabular}{llll}
            \toprule
            \textbf{Algorithm} & \textbf{Inputs} & \textbf{Outputs} & \textbf{Equations} \\
            \toprule
            MLP & $x$ & $y$ & $y = \text{MLP}_n(x) = f_n(f_{n-1}(\dots f_1(x)))$ \\
            & & & $f_{\text{layer}}(x) = \text{act}(W \cdot x + b)$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{Overview:} Feed-Forward NN, FC NN: Vector-in, vector-out optimizable parametrized trans.
                \item Parameters: $f(x; \theta), \theta = \{W_1, \dots, W_n\} = \{w_1, \dots, w_N\}$
                \item Hyperparameters: $n$ (NN depth), hidden\_dim (NN width)
            \end{itemize}} \\
            \midrule
            PCA & $x$ & $x'$ & $x \underset{\text{Encode}}{\mapsto} z \underset{\text{Decode}}{\mapsto} x'$ \\
            & & & $\text{Encoder}(x) = Wx$, $\text{Decoder}(z) = W^{-1}z$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{Overview:} Linear transformation for dimension reduction 
            \end{itemize}} \\
            \midrule
            AE & $x$ & $x'$ & $x \underset{\text{Encode}}{\mapsto} z \underset{\text{Decode}}{\mapsto} x'$ \\
            & & & $f_{\text{enc}}(x), \; \text{Encoder}(x) = \text{Neural network}$, $f_{\text{dec}}(z), \; \text{Decoder}(z) = \text{Neural network}$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Bottleneck (latent space) forces network to learn compressed representation
                \item Encoder maps input to lower dimension, decoder reconstructs input
            \end{itemize}} \\
            \midrule
            VAE & $x$ & $x'$ & $x \underset{\text{Encode}}{\mapsto} z \underset{\text{Decode}}{\mapsto} x'$ \\
            & & & $f_{\text{enc}, \phi}(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\mathbf{z} \mid \boldsymbol{\mu}(\mathbf{x}), \sigma^2(\mathbf{x}) \mathbf{I})$, $f_{\text{dec}}(\mathbf{z})$\\ 
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item \textbf{Overview:} A probabilistic approach by learning smooth latent spaces by modelling probability distributions.
                \item $\boldsymbol{\mu}(\mathbf{x})$: Mean of the distribution, $\sigma^2(\mathbf{x})$: Variance of the distribution (independent)
                \item Encoder maps input to a distribution, decoder samples from it.
                \begin{itemize}
                    \item Encoder estimates parameters for a distribution in the latent space
                    \item Decoder samples from the distribution to generate an output sample
                \end{itemize}
                \item \textbf{Key:} Works on any data.
            \end{itemize}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{llll}
            \toprule
            \textbf{Algorithm} & \textbf{Inputs} & \textbf{Outputs} & \textbf{Equations} \\
            \toprule
            RNN & $x_t,h_{t-1}$ & $y_{t},h_{t}$ & $h_t = \text{tanh}(\text{Linear} (h_{t-1}) + \text{Linear}(x_t))$ \\ 
            & & & $y_t = \text{MLP}(h_t)$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item $x_t$: Input, $h_t$: Hidden state, $y_t$: Output
            \end{itemize}} \\
            \midrule
            GRU & $x_t,h_{t-1}$ & $y_t,h_t$ & $z_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $r_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $\tilde{h}(t) = \text{tanh}(\text{Linear}(x_t) + \text{Linear}(r_t \odot h_{t-1}))$ \\
            & & & $h_t = (1-z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item $z_t$: Update gate, $r_t$: Reset gate
                \item $h_t$: Hidden state
            \end{itemize}} \\
            \midrule
            LSTM & $x_t,h_{t-1}$ & $h_t,c_t$ & $f_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $i_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $o_t = \text{sigmoid}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $\tilde{c}_t = \text{tanh}(\text{Linear}(x_t) + \text{Linear}(h_{t-1}))$ \\
            & & & $c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$ \\
            & & & $h_t = o_t \odot \text{tanh}(c_t)$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item $f_t$: Forget gate, $i_t$: Input/Update gate, $o_t$: Output gate
                \item $h_t$: Hidden state, $c_t$: Cell state
            \end{itemize}} \\
            \midrule 
            GNN & $U_n, V_n, E_n$ & $U_{n+1}, V_{n+1}, E_{n+1}$ & $f_{U_n}, f_{V_n}, f_{E_n}$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Graph in, graph out, optimizable, parametrized transformation.
                \item $f$: Graph independent layer.
                \customFigure[0.5]{../Images/L12_14.png}{}
            \end{itemize}} \\
            \midrule 
            Graph Convolution & $U_n, V_n, E_n$ & $U_{n+1}, V_{n+1}, E_{n+1}$ & $\rho_{V_n \rightarrow V_n}, f_{U_n}, f_{V_n}, f_{E_n}$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item Add step where we pool info from neighboring nodes. 
                \customFigure[0.5]{../Images/L12_22.png}{}
                \vspace{-2em}
            \end{itemize}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\begin{summary}
    \begin{center}
        \begin{tabular}{llll}
            \toprule
            \textbf{Algorithm} & \textbf{Inputs} & \textbf{Outputs} & \textbf{Equations} \\
            \toprule
            Graph Convolution & $X, A$ & $X'$ & $GCN(X, A) = \text{act}\left(D^{-1/2} (A + I) D^{-1/2} X W \right) = \text{act}(\tilde{A} X W)$ \\
            \multicolumn{4}{p{\linewidth}}{
            \begin{itemize}
                \item \( D \) is the degree matrix
                \item \( A \) is the adjacency matrix
                \item \( X \) is the node matrix/tensor
                \item \( W \) is the learnable weights
                \item \textit{Act} is the activation function
                \item What is the aggregation fn? Sum from the matrix multiplication.
            \end{itemize}} \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{summary}
\newpage

\subsection{Geometric DL Blueprint for NNs}
\begin{summary}
    Unify various networks around symmetry. 
    \customFigure[0.5]{../Images/L9_0.png}{}
    \begin{itemize}
        \item MLP: Permutation Invariance for Neural Networks (functions unaffected by input permutation)
        \customFigure[0.5]{../Images/L9_1.png}{}
        \item Deep Sets: Permutation Invariant NNs (function of element embeddings and pooled features).
        \customFigure[0.5]{../Images/L9_2.png}{}
        \item CNN
        \customFigure[0.5]{../Images/L10_0.png}{}
        \item RNN 
        \customFigure[0.5]{../Images/L11_0.png}{}
    \end{itemize}
\end{summary}
\newpage

\section{Intro to ML}
\input{Lectures/L2.tex}
\newpage

\section{MLP}
\input{Lectures/L3.tex}
\newpage

\section{Neural Network Engineering}
\input{Lectures/L4.tex}
\newpage

\section{Hyperparameter Optimization}
\input{Lectures/L5.tex}

\end{document}