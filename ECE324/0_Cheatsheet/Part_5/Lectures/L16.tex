\begin{motivation}
    A single metric is an incomplete description of most real-world tasks. 
    \begin{enumerate}
        \item Improve models
        \item Justify models \\
        $
        \left.
        \begin{array}{l}
        \text{a. Creators} \\
        \text{b. Operators} \\
        \text{c. Executors} \\
        \text{d. Decision} \\
        \text{e. Auditors} \\
        \text{f. Data Subjects}
        \end{array}
        \right\} \text{Stakeholders of a AI System}
        $
        \item Discover Insights 
    \end{enumerate}
\end{motivation}

\subsection{What is Interpretability?}
\begin{definition}
    Interpretability is:
    \begin{enumerate}
        \item Degree to which a human can understand the cause of a decision
        \item Where a user can correctly and efficiently predict the method's results. 
        \item Science of understanding AI models from the inside out. 
    \end{enumerate}
\end{definition}

\subsection{Types of Interpretability}
\begin{summary}
    \customFigure[0.5]{../../Images/L16_0.png}{}
    \begin{itemize}
        \item \textbf{ML Interpretability:} 
        \begin{itemize}
            \item \textbf{By-design:} Interpretability built directly into the model (e.g., decision trees, linear models).
            \item \textbf{Post-hoc:} Interpretability techniques are applied after the model is trained, without altering the model.
            \begin{itemize}
                \item \textbf{Model-agnostic:} Interpretation methods that can be applied to any model.
                \begin{itemize}
                    \item \textbf{Global:} Provides an overall understanding of the model’s behavior across the entire dataset.
                    \item \textbf{Local:} Explains the model’s prediction for a specific input instance.
                \end{itemize}
                \item \textbf{Model-specific:} Interpretation methods that are tailored to specific models.
            \end{itemize}
        \end{itemize}
    \end{itemize}    
\end{summary}
\newpage

\subsection{Examples}
\subsubsection{Interpretability in LLMs}
\begin{example}
    \customFigure[0.5]{../../Images/L16_1.png}{}
    \begin{itemize}
        \item \textbf{LS:} Inputs feed into a neural network (NN), which then outputs some prediction. Interpretability is easier here.
        \item \textbf{LLM:} Interpretability is very hard for next token prediction so need to use mechanistic interpretability.
    \end{itemize}
\end{example}
