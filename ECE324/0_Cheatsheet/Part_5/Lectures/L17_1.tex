\subsection{Mechanistic Interpretability}
\begin{motivation}
    Necessary b/c AI and humans use different representations and perform different tasks. 
\end{motivation}
\begin{definition}
    Reverse engineering the algorithm of a NN.
\end{definition}

\subsubsection{Iterative Process}
\begin{notes}
    \customFigure[0.75]{../../Images/L17_6.png}{}
    \begin{itemize}
        \item \textbf{Step 1 Network:} Trained neural network.
        \item \textbf{Step 2 Decomposition:} Network is decomposed into interpretable components, such as 
        \begin{itemize}
            \item identifying groups of neurons
            \item paths that contribute to specific outputs.
        \end{itemize}
        \item \textbf{Step 3 Hypotheses:} Formulate hypotheses about the function of specific neurons or subnetworks. 
        \item \textbf{Step 4 Validation:} Hypotheses are empirically tested against the network's behavior to validate whether they accurately explain the underlying computation.
    \end{itemize}    
\end{notes}

\subsubsection{Superposition}
\begin{notes} Neural networks encode multiple abstract concepts in overlapping representations, and introduces strategies for disentangling them.
    \customFigure[0.5]{../../Images/L17_7.png}{}
    \begin{itemize}
        \item \textbf{Linear Representation Hypothesis:} Neural activations can be linearly decomposed into directions corresponding to distinct features. 
        \item \textbf{Superposition:} Multiple concepts are often encoded within the same neuron or direction in the representation space.
        \item \textbf{Sparse Dictionary Learning:} Techniques such as sparse coding or L1 regularization can help disentangle overlapping representations by promoting sparsity to assign each neuron to a specific concept.
    \end{itemize}    
\end{notes}
\newpage

\subsection{Attribution Study}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Metric} & \textbf{Description} \\
            \midrule
            \textbf{Accurate} & \\
            \midrule
            \textbf{Faithful} & \\
            \midrule
            \textbf{Consistent} & \\
            \midrule
            \textbf{Stable} & \\
        \end{tabular}
    \end{center}    
\end{summary}

\begin{summary}
\end{summary}
\newpage

\subsection{Examples}
\subsubsection{Sanity Checks for Saliency Maps}
\begin{example}
    \customFigure[0.6]{../../Images/L17_9.png}{}
    \customFigure[0.6]{../../Images/L17_10.png}{}
    \customFigure[0.6]{../../Images/L17_11.png}{Randomly initialized neural networks give okay attributions}
\end{example}
\newpage

\subsubsection{Are they really that important?}
\begin{example} Removing importance pixels, replace for mean and retrain model.
    \customFigure[0.75]{../../Images/L17_12.png}{}
\end{example}

\subsubsection{What does a Neural Network think a Banana is?}
\begin{example}
    Starting from noise, optimizing for a class label.
    \customFigure[0.75]{../../Images/L17_13.png}{}
\end{example}
\newpage

\subsubsection{Does Interpretability Make Us Better?}
\begin{notes}
    Maybe
    \begin{itemize}
        \item Machine concepts can be different than human concepts. 
        \customFigure[0.5]{../../Images/L17_8.png}{}
        \item Can align human concepts with machine concepts.
    \end{itemize}
\end{notes}

\subsubsection{Finding Concepts within LLMs}
\begin{example} Features that activate on abstract concepts. 
    \customFigure[0.75]{../../Images/L17_14.png}{}
\end{example}
