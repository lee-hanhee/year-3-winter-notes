\subsection{Does Interpretability Make Us Better?}

\subsection{Mechanistic Interpretability}
\begin{motivation}
    Necessary b/c AI and humans use different representations and perform different tasks. 
\end{motivation}
\begin{definition}
    Reverse engineering the algorithm of a NN. 
    \customFigure[0.5]{../../Images/L17_6.png}{}
    \begin{itemize}
        \item \textbf{Step 1 Network:} The process begins with a trained neural network whose structure and learned parameters are opaque and require analysis.
        \item \textbf{Step 2 Decomposition:} The network is decomposed into interpretable components, such as identifying groups of neurons or paths that contribute to specific outputs.
        \item \textbf{Step 3 Hypotheses:} Researchers formulate hypotheses about the function of specific neurons or subnetworks. These are often expressed as simplified algorithms mapping input representations to outputs.
        \item \textbf{Step 4 Validation:} The hypotheses are empirically tested against the network's behavior to validate whether they accurately explain the underlying computation.
        \item \textbf{Iterative Process:} If validation fails or is inconclusive, the process loops back to earlier stages (decomposition or hypothesis generation) to refine the understanding.
    \end{itemize}    
    \customFigure[0.5]{../../Images/L17_7.png}{}
    \begin{itemize}
        \item 
    \end{itemize}
\end{definition}

\subsection{Attribution Study}
\begin{summary}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Metric} & \textbf{Description} \\
            \midrule
            \textbf{Accurate} & \\
            \midrule
            \textbf{Faithful} & \\
            \midrule
            \textbf{Consistent} & \\
            \midrule
            \textbf{Stable} & \\
        \end{tabular}
    \end{center}    
\end{summary}

\begin{summary}
\end{summary}

\subsection{Examples}
\subsubsection{Finding Concepts within LLMs}
\begin{example}
    
\end{example}
