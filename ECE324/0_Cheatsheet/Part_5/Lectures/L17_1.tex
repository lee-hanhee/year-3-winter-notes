\subsection{Mechanistic Interpretability}
\begin{motivation}
    Necessary b/c AI and humans use different representations and perform different tasks. 
\end{motivation}
\begin{definition}
    Reverse engineering the algorithm of a NN.
\end{definition}

\subsubsection{Iterative Process}
\begin{notes}
    \customFigure[0.75]{../../Images/L17_6.png}{}
    \begin{itemize}
        \item \textbf{Step 1 Network:} Trained neural network.
        \item \textbf{Step 2 Decomposition:} Network is decomposed into interpretable components, such as 
        \begin{itemize}
            \item identifying groups of neurons
            \item paths that contribute to specific outputs.
        \end{itemize}
        \item \textbf{Step 3 Hypotheses:} Formulate hypotheses about the function of specific neurons or subnetworks. 
        \item \textbf{Step 4 Validation:} Hypotheses are empirically tested against the network's behavior to validate whether they accurately explain the underlying computation.
    \end{itemize}    
\end{notes}

\subsubsection{Superposition}
\begin{notes} Neural networks encode multiple abstract concepts in overlapping representations, and introduces strategies for disentangling them.
    \customFigure[0.5]{../../Images/L17_7.png}{}
    \begin{itemize}
        \item \textbf{Linear Representation Hypothesis:} Neural activations can be linearly decomposed into directions corresponding to distinct features. 
        \item \textbf{Superposition:} Multiple concepts are often encoded within the same neuron or direction in the representation space.
        \item \textbf{Sparse Dictionary Learning:} Techniques such as sparse coding or L1 regularization can help disentangle overlapping representations by promoting sparsity to assign each neuron to a specific concept.
    \end{itemize}    
\end{notes}
\newpage

\subsection{Attribution Study}
\begin{summary} Setting up experiments to study different qualities of attribution
    \vspace{1em}
    \begin{center}
        \begin{tabular}{ll}
            \toprule
            \textbf{Metric} & \textbf{Description} \\
            \midrule
            \textbf{Accurate} & How accurate is an attribution technique? \\
            \midrule
            \textbf{Faithful} & How faithful are attributions to a model's prediction?\\
            \midrule
            \textbf{Consistent} & How consistent are attributions for similar models?\\
            \midrule
            \textbf{Stable} & How stable are attribution to input perturbations? \\
            \bottomrule
        \end{tabular}
    \end{center}    
\end{summary}
\newpage

\subsection{Examples}
\subsubsection{Sanity Checks for Saliency Maps}
\begin{example}
    \customFigure[0.6]{../../Images/L17_9.png}{}
    \customFigure[0.6]{../../Images/L17_10.png}{}
    \customFigure[0.6]{../../Images/L17_11.png}{Randomly initialized neural networks give okay attributions}
\end{example}
\newpage

\subsubsection{Attribution Qualities in a Controlled Setting}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given}
        \customFigure[0.75]{../../Images/L17_16.png}{}
        \item \textbf{1 Attribution Accuracy:} Attribution performance for perfect models (>0.95 AUROC). Values are average for 10\% top performing models across 195 hyperparameter combinations and 7 random seeds.
        \customFigure[0.75]{../../Images/L17_17.png}{}
        \item \textbf{1/3 Attribution Accuracy \& Consistency During Training:} Prediction and attribution performance during training of a GCN for a subgraph logic task. Shaded area is variance across 10 random seeds. 
        \customFigure[0.75]{../../Images/L17_18.png}{}
        \item \textbf{1/3 Attribution Accuracy \& Consistency During Training: Performance for Untrained Networks} Attribution performance for untrained networks for a subgraph logic task. Boxplots are from 25 random seeds. 
        \customFigure[0.75]{../../Images/L17_19.png}{}
        \item \textbf{3 Attribution Consistency: Regularization} Attribution performance for networks with different L2 regularization coefficient for a subgraph logic task. Boxplots are from 10 random seeds. 
        \customFigure[0.75]{../../Images/L17_20.png}{}
        \item \textbf{2 Attribution Faithfulness via Label Noise:} Attribution performance for handicapped models: Noise is added to labels to limit predictive performance. Boxplots come from 25 random seeds.
        \customFigure[0.75]{../../Images/L17_22.png}{}
        \item \textbf{2 Attribution Faithfulness:} Attribution performance in datasets with spurious correlations. We introduce the correlated presence of another subgraph and measure attribution on the correlated and uncorrelated tasks. 
        \customFigure[0.75]{../../Images/L17_25.png}{}
        \item \textbf{4 Attribution Stability:} We perturb multiple times a graph without changing the label and look at the delta in attribution performance. 
        \customFigure[0.75]{../../Images/L17_26.png}{}
    \end{enumerate}
\end{example}
\newpage

\subsubsection{Are they really that important?}
\begin{example} Removing importance pixels, replace for mean and retrain model.
    \customFigure[0.75]{../../Images/L17_12.png}{}
\end{example}

\subsubsection{What does a Neural Network think a Banana is?}
\begin{example}
    Starting from noise, optimizing for a class label.
    \customFigure[0.75]{../../Images/L17_13.png}{}
\end{example}
\newpage

\subsubsection{Does Interpretability Make Us Better?}
\begin{example}
    Maybe
    \begin{itemize}
        \item Machine concepts can be different than human concepts. 
        \customFigure[0.5]{../../Images/L17_8.png}{}
        \item Can align human concepts with machine concepts.
    \end{itemize}
\end{example}

\subsubsection{Finding Concepts within LLMs}
\begin{example} Features that activate on abstract concepts. 
    \customFigure[0.75]{../../Images/L17_14.png}{}
\end{example}
