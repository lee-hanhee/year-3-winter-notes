\subsection{Inputs: GraphTensors}
\begin{notes}
    Graph tensor consists of:
    \begin{itemize}
        \item Node Tensor (X)
        \item Edge Tensor (E)
        \item Global Tensor (G)
        \item Adjcency (A)
        \begin{itemize}
            \item Matrix
            \item List
        \end{itemize}
    \end{itemize}
\end{notes}

\subsection{Message Passing}
\begin{algo}
    \begin{enumerate}
        \item Prepare messages (collect relevant information)
        \item Aggregate messages (pool information)
        \item Update graph (transform graph with \(f\))
        \begin{itemize}
            \item Update node features
            \item Update edge features
            \item Update global features
        \end{itemize}
        \customFigure[0.75]{../Images/L12_20.png}{}
    \end{enumerate}
\end{algo}
\newpage

\subsubsection{Propogation pattern of message passing}
\begin{notes}
    After a few layers we get $\uparrow$ more complex patterns of info. 
    \customFigure[0.75]{../Images/L12_21.png}{}
\end{notes}

\subsubsection{Adjacency Multiplication is Message Passing}
\begin{process} Repeated adjacency matrix multiplications ($A^k$) correspond to $k$-hop message passing.
    \begin{enumerate}
        \item \textbf{Initial State:}
        \begin{itemize}
            \item The graph is represented using an adjacency matrix $A$.
            \item $A^0 = I$ (identity matrix) captures only the self-loop at each node.
            \item Node 0 is initialized with a feature or signal (highlighted in red), and all others are inactive (gray).
        \end{itemize}
    
        \item \textbf{1-hop Neighbors:} Multiply the signal by $A^1$ (i.e., the original adjacency matrix).
        \begin{itemize}
            \item The signal from node 0 propagates to its direct neighbors (nodes 1 and 2).
            \item This mimics one step of message passing where only 1-hop neighbors receive the message.
        \end{itemize}
    
        \item \textbf{2-hop Neighbors:} Multiply by $A^2$ to simulate 2-hop propagation.
        \begin{itemize}
            \item Nodes that are 2 steps away from the source (e.g., nodes 3 and 4) are now activated.
            \item The adjacency matrix $A^2$ encodes the number of 2-step paths between nodes.
        \end{itemize}
    
        \item \textbf{3-hop Neighbors:} Multiply by $A^3$ to simulate 3-hop propagation.
        \begin{itemize}
            \item Information has now reached the entire graph.
            \item Each element in $A^3$ indicates how many distinct 3-step paths exist between node pairs.
        \end{itemize}
    \end{enumerate}    
\end{process}
\newpage

\begin{notes}
    \customFigure[0.75]{../../Images/L13_1.png}{}
    \customFigure[0.75]{../../Images/L13_2.png}{}
    \customFigure[0.75]{../../Images/L13_3.png}{}
    \customFigure[0.75]{../../Images/L13_4.png}{}
\end{notes}

\subsubsection{Gather + Scatter for Advanced Indexing}
\begin{notes}
    \customFigure[0.75]{../../Images/L13_5.png}{Arrows depict how values are selected (\texttt{gather}) and accumulated (\texttt{scatter}) to produce the \texttt{output}}
    \begin{itemize}    
        \item \textbf{Adjacency List Representation:}
        \begin{itemize}
            \item The graph is represented as:
            \begin{itemize}
                \item \texttt{source\_nodes = [0, 0, 1, 2, 2]}
                \item \texttt{destination\_nodes = [1, 2, 3, 0, 4]}
            \end{itemize}
            \item These define the directed edges: $(0,1), (0,2), (1,3), (2,0), (2,4)$.
        \end{itemize}
        \item \textbf{Gather Operation:} Gathers features from the source nodes using the \texttt{index} vector.
        \begin{itemize}
            \item This step retrieves features for each edge from the source node.
        \end{itemize}
        \item \textbf{Scatter Operation:} Scatters (adds) gathered features to destination nodes.
        \begin{itemize}
            \item For each destination node, incoming messages are aggregated using a reduction operation (e.g., \texttt{sum}).
        \end{itemize}
    \end{itemize}    
\end{notes}
\newpage

\subsection{Conditioning Information}
\begin{notes}
    Many ways of adding context to a specific part of a graph \\
    \customFigure[0.9]{../Images/L12_23.png}{}
    \begin{itemize}    
        \item \textbf{Context Sources:}
        \begin{itemize}
            \item \textbf{Node embedding:} Current embedding of the node being updated.
            \item \textbf{Adjacent nodes embedding:} Features from neighboring nodes.
            \item \textbf{Adjacent edges embedding:} Features from edges connected to the current node.
            \item \textbf{Global embedding:} A summary embedding of the entire graph or subgraph, providing global context.
        \end{itemize}
    
        \item \textbf{Conditioning Function \(\text{cond}(x|z)\):} Combines all relevant embeddings (\(x\)) conditioned on context (\(z\)) to form a joint representation.
    
        \item \textbf{Update Function \(f_{v_n}\):} A learnable function (e.g., MLP) that maps the conditioned representation to an updated node embedding.
    \end{itemize}    
\end{notes}

\subsection{Hyperparameters of GNNs}
\begin{notes}
    \begin{itemize}
        \item Number of layers
        \item Dimensionality of embeddings (node, edge, globals)
        \item Update functions (MLPs, Attention)
        \item Conditioning mechanisms (concat, add, linear, FiLM)
        \item Message passing (Nodes, Edges, Globals)
        \item Aggregation operations (sum, mean, max, min)
        \item Regularization
        \item Optimization
    \end{itemize}
\end{notes}

\subsection{GCN vs. GAT Node Update:}
\begin{notes}
    \textbf{GCN:}
    \[
    x_i^{n+1} = \text{act} \left( \sum_{j \in \text{neighbors}} W^n x_j^n \right)
    \]

    \textbf{GAT:}
    \[
    x_i^{n+1} = \text{act} \left( \sum_{j \in \text{neighbors}} \alpha_{ij} W^n x_j^n \right)
    \]
    \begin{itemize}
        \item $\alpha_{ij} \approx \langle Q * x_i, K * x_j \rangle / N$
    \end{itemize}
\end{notes}

\subsection{Examples}
\subsubsection{Message passing / GraphNets on a small graph}
\begin{example}
    \customFigure[0.75]{../Images/L12_24.png}{}
    \begin{itemize}
        \item \textbf{Input Graph:} 3 nodes (0, 1, 2) and 2 edges: $(0,1)$ and $(1,2)$.
        \begin{itemize}
            \item Nodes and edges are initialized with features that will be updated through message passing.
        \end{itemize}
    
        \item \textbf{Step 1 – Update Edges:} Messages are passed along edges.
        \begin{itemize}
            \item Edge attributes are updated based on the features of the connected nodes.
            \item Each edge (e.g., $0 \rightarrow 1$, $1 \rightarrow 2$) aggregates information from the connected nodes to compute new edge embeddings.
        \end{itemize}
    
        \item \textbf{Step 2 – Update Nodes:} Each node collects messages from incoming edges.
        \begin{itemize}
            \item Node features are updated by aggregating information from its neighbors
        \end{itemize}
    
        \item \textbf{Step 3 – Update Global State:} A global context vector is computed based on the set of updated node and edge embeddings.
    
        \item \textbf{Output Graph:} Graph with updated node, edge, and global features.
    \end{itemize}    
\end{example}
\newpage

\subsubsection{Graph Tensor}
\begin{example}
    \customFigure[0.75]{../../Images/L13_0.png}{}
\end{example}
\newpage

\subsubsection{Molecules Message Passing into Prediction}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:} Nodes (Atoms), Edges (Bonds), Globals (Properties)
        \customFigure[0.25]{../../Images/L12_2.png}{}
        \item \textbf{Task:} Predict the smell of the molecule. 
        \item \textbf{Message Passing:}
        \customFigure[0.75]{../../Images/L12_3.png}{}
        \item \textbf{Prediction:}
        \customFigure[0.75]{../../Images/L12_4.png}{}
    \end{enumerate}
\end{example}
\newpage

\subsubsection{GraphNets: Example on Methane Molecules}
\begin{example}
    \begin{enumerate}
        \item \textbf{Given:}
        \customFigure[0.75]{../../Images/L13_6.png}{}
        \begin{itemize}
            \item 5 nodes, each with 3-dim. 
            \item 4 edges, each with 2-dim.
            \item 1 global, with 1-dim.
        \end{itemize}
        \item \textbf{Edge Block $f_E$:}
        \customFigure[0.75]{../../Images/L13_7.png}{}
        \begin{enumerate}
            \item \textbf{Edge of Interest:} $E_i$ connects two nodes, labeled as H and C, $[1 \times 2]$.
        
            \item \textbf{Prepare Data:} 
            \begin{itemize}
                \item $E_i$: Edge, shape $[1 \times 2]$.
                \item $X_H$: Feature vector of source node (H), shape $[1 \times 3]$.
                \item $X_C$: Feature vector of destination node (C), shape $[1 \times 3]$.
                \item $U$: Global graph-level feature vector, shape $[1 \times 1]$.
            \end{itemize}

            \item \textbf{Aggregation (Sum):} These features are concatenated into a single vector of shape $[1 \times 9]$.
            
            \item \textbf{MLP Update:} The aggregated $[1 \times 9]$ vector is passed through a MLP. 

            \item \textbf{Output:} The MLP outputs an updated edge representation $E_{i,\text{new}}$ of shape $[1 \times 2]$.
        \end{enumerate}
        \newpage
        \item \textbf{Node Block $f_V$:}
        \customFigure[0.75]{../../Images/L13_8.png}{}
        \begin{enumerate}
            \item Same idea for the nodes, but note after the update, the node can change dimensions after the update. 
        \end{enumerate}
        \item \textbf{Global Block $f_U$:}
        \customFigure[0.75]{../../Images/L13_9.png}{}
        \begin{enumerate}
            \item Same idea for the nodes, but note after the update, the node can change dimensions after the update. 
        \end{enumerate}
    \end{enumerate}
\end{example}